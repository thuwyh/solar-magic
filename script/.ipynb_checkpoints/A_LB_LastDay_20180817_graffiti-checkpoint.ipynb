{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.问题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.问题类别"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一元回归问题.\n",
    "训练集样本9000条(包括82条重复样本)\n",
    "测试集样本8409条\n",
    "原始特征19个\n",
    "\n",
    "预测量为样本对应的发电量\n",
    "score = 1/(1+RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.特征分析 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "时间戳\n",
    "ID：当前记录条数； \n",
    "\n",
    "测量物理量(连续)\n",
    "板温：光伏电池板背测温度；\n",
    "现场温度：光伏电站现场温度；\n",
    "转换效率：为计算得到的平均转换效率；\n",
    "电压A：为数据采集点A处汇流箱电压值；\n",
    "电压B：为数据采集点B处汇流箱电压值；\n",
    "电压C：为数据采集点C处汇流箱电压值；\n",
    "电流A：为采集点A处汇流箱电流值；\n",
    "电流B：为采集点B处汇流箱电流值；\n",
    "电流C：为采集点C处汇流箱电流值；\n",
    "风速：为光伏电厂现场风速测量值；\n",
    "风向：为光伏电厂现场风的来向；\n",
    "\n",
    "计算物理量\n",
    "转换效率A：数据采集点A处的光伏板转换效率；\n",
    "转换效率B：数据采集点B处的光伏板转换效率；\n",
    "转换效率C：数据采集点C处的光伏板转换效率；\n",
    "功率A：为采集点A处的功率Pa，P=UI；\n",
    "功率B：为采集点B处的功率Pb，P=UI；\n",
    "功率C：为采集点C处的功率Pc，P=UI；\n",
    "平均功率：为A、B、C三点功率的平均值：(Pa+Pb+Pc)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.数据挖掘"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "详见 0803_数据挖掘.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.准备工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.原始数据准备(不进行任何清洗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/public_raw.train.csv')\n",
    "test_raw = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "train_raw['is_train']=1\n",
    "test_raw['is_train']=0\n",
    "\n",
    "df = pd.concat([train_raw, test_raw],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原始路线\n",
    "all_data = df.copy()\n",
    "bad_feature = ['P_A', 'P_B', 'P_C', 'P_avg', 'env_t', 'V_A', 'V_B', 'V_C', 'I_B', 'I_C', 'efficiency', 'efficiency_A', 'efficiency_B', 'efficiency_C']\n",
    "bad_index1 = all_data[bad_feature][\n",
    "    (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "    (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "].dropna(how='all').index\n",
    "bad_index2 = all_data[\n",
    "    ((all_data['V_A']<500)&(all_data['V_A']!=0))|\n",
    "    ((all_data['V_B']<500)&(all_data['V_B']!=0))|\n",
    "    ((all_data['V_C']<500)&(all_data['V_C']!=0))].index\n",
    "bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "\n",
    "# bad_index = all_data[bad_feature][\n",
    "#     (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "#     (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "# ].dropna(how='all').index\n",
    "\n",
    "\n",
    "\n",
    "bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 上下记录均值替代异常值\n",
    "for idx, line in bad_data.iterrows():\n",
    "    ID = line['ID']\n",
    "    col_index = line[bad_feature][ \n",
    "        (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "        (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "    ].index\n",
    "    index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "    # idx - before_offset, CV  0.8684\n",
    "    before_offset = 1\n",
    "    while (idx - before_offset)in bad_index:\n",
    "        before_offset += 1\n",
    "\n",
    "    after_offset = 1\n",
    "    while (idx + after_offset) in bad_index:\n",
    "        after_offset += 1\n",
    "    \n",
    "    replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / 2\n",
    "    all_data.loc[index, col_index] = replace_value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.增加前后有效发电量均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_data.copy()\n",
    "\n",
    "#前二后二\n",
    "next_one = []\n",
    "prev_one = []\n",
    "next_id = []\n",
    "prev_id = []\n",
    "\n",
    "second_next_one = []\n",
    "second_prev_one = []\n",
    "\n",
    "df_len = df.shape[0]\n",
    "\n",
    "i_y =df.columns.get_loc(\"y\")\n",
    "\n",
    "def get_prev_nn_index(cur_i):\n",
    "    prev_i = cur_i-1\n",
    "    while(prev_i>=0 and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i-=1\n",
    "    return prev_i\n",
    "\n",
    "def get_next_nn_index(cur_i):\n",
    "    prev_i = cur_i+1\n",
    "    while(prev_i<df_len and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i+=1\n",
    "    return prev_i\n",
    "\n",
    "for i in range(df_len):\n",
    "    f_pre_i=get_prev_nn_index(i)\n",
    "    if(f_pre_i)<0:\n",
    "        prev_one.append(np.nan)\n",
    "        prev_id.append(0)\n",
    "    else:\n",
    "        prev_one.append(df.iat[f_pre_i,i_y])\n",
    "        prev_id.append(f_pre_i)\n",
    "        \n",
    "    s_pre_i=get_prev_nn_index(f_pre_i)\n",
    "    if (s_pre_i)<0:\n",
    "        second_prev_one.append(np.nan)\n",
    "    else:\n",
    "        second_prev_one.append(df.iat[s_pre_i,i_y])\n",
    "    \n",
    "    f_next_i=get_next_nn_index(i)\n",
    "    if(f_next_i<df_len):\n",
    "        next_one.append(df.iat[f_next_i,i_y])\n",
    "        next_id.append(f_next_i)\n",
    "    else:\n",
    "        next_one.append(np.nan)\n",
    "        next_id.append(df_len)\n",
    "    \n",
    "    s_next_i=get_next_nn_index(f_next_i)\n",
    "    if(s_next_i<df_len):\n",
    "        second_next_one.append(df.iat[s_next_i,i_y])\n",
    "    else:\n",
    "        second_next_one.append(np.nan)\n",
    "        \n",
    "\n",
    "df['next_value'] = next_one\n",
    "df['prev_value'] = prev_one\n",
    "df['avg_value'] = np.nanmean([df['next_value'], df['prev_value']],axis=0)\n",
    "\n",
    "df.drop(['next_value','prev_value'],1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.增加前后功率均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg(df):\n",
    "    array = np.array(df[\"P_avg\"])\n",
    "    newarray=[]\n",
    "    num = 0\n",
    "    for i in np.arange(len(array)):\n",
    "        for j in np.arange(10):\n",
    "            if i<10:\n",
    "                num = (array[j-1]+array[j-2]+array[j-3])/3\n",
    "            if i>=10:\n",
    "                num = (array[i-1]+array[i-2]+array[i-3]+array[i-5]+array[i-6]+array[i-7]+array[i-8]+array[i-9])/9\n",
    "        newarray.append(num)\n",
    "    df[\"old_SoCalledSF_P_avg\"] = newarray\n",
    "    return df\n",
    "\n",
    "df = add_avg(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.训练集测试集数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.去除训练集的重复样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 8409)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拆分数据\n",
    "\n",
    "train_data = df[df['is_train']==1]\n",
    "test_data = df[df['is_train']==0]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备提交结果\n",
    "\n",
    "df_result = pd.DataFrame()\n",
    "df_result['ID'] = list(test_data['ID'])\n",
    "special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8409"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>board_t</th>\n",
       "      <th>env_t</th>\n",
       "      <th>light_strength</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>efficiency_A</th>\n",
       "      <th>efficiency_B</th>\n",
       "      <th>efficiency_C</th>\n",
       "      <th>V_A</th>\n",
       "      <th>V_B</th>\n",
       "      <th>...</th>\n",
       "      <th>P_A</th>\n",
       "      <th>P_B</th>\n",
       "      <th>P_C</th>\n",
       "      <th>P_avg</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>y</th>\n",
       "      <th>is_train</th>\n",
       "      <th>avg_value</th>\n",
       "      <th>old_SoCalledSF_P_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-17.4</td>\n",
       "      <td>34</td>\n",
       "      <td>80.55</td>\n",
       "      <td>106.32</td>\n",
       "      <td>16.98</td>\n",
       "      <td>118.36</td>\n",
       "      <td>729.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>...</td>\n",
       "      <td>976.86</td>\n",
       "      <td>155.98</td>\n",
       "      <td>1087.50</td>\n",
       "      <td>740.11</td>\n",
       "      <td>0.6</td>\n",
       "      <td>272</td>\n",
       "      <td>1.437752</td>\n",
       "      <td>1</td>\n",
       "      <td>1.692575</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>-18.73</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>30</td>\n",
       "      <td>99.90</td>\n",
       "      <td>139.00</td>\n",
       "      <td>21.20</td>\n",
       "      <td>139.51</td>\n",
       "      <td>728.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1128.40</td>\n",
       "      <td>172.08</td>\n",
       "      <td>1132.56</td>\n",
       "      <td>811.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>275</td>\n",
       "      <td>1.692575</td>\n",
       "      <td>1</td>\n",
       "      <td>1.706770</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>-17.54</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>41</td>\n",
       "      <td>82.48</td>\n",
       "      <td>114.86</td>\n",
       "      <td>14.91</td>\n",
       "      <td>117.66</td>\n",
       "      <td>731.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1279.25</td>\n",
       "      <td>166.06</td>\n",
       "      <td>1310.40</td>\n",
       "      <td>918.57</td>\n",
       "      <td>1.1</td>\n",
       "      <td>283</td>\n",
       "      <td>1.975787</td>\n",
       "      <td>1</td>\n",
       "      <td>2.031615</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>-15.43</td>\n",
       "      <td>-16.6</td>\n",
       "      <td>53</td>\n",
       "      <td>73.98</td>\n",
       "      <td>101.72</td>\n",
       "      <td>15.55</td>\n",
       "      <td>104.67</td>\n",
       "      <td>730.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1474.60</td>\n",
       "      <td>225.37</td>\n",
       "      <td>1517.34</td>\n",
       "      <td>1072.44</td>\n",
       "      <td>0.9</td>\n",
       "      <td>280</td>\n",
       "      <td>2.370656</td>\n",
       "      <td>1</td>\n",
       "      <td>2.253939</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>-14.60</td>\n",
       "      <td>-16.3</td>\n",
       "      <td>65</td>\n",
       "      <td>64.62</td>\n",
       "      <td>86.86</td>\n",
       "      <td>13.09</td>\n",
       "      <td>93.92</td>\n",
       "      <td>727.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1548.51</td>\n",
       "      <td>233.28</td>\n",
       "      <td>1674.40</td>\n",
       "      <td>1152.06</td>\n",
       "      <td>1.1</td>\n",
       "      <td>280</td>\n",
       "      <td>2.532091</td>\n",
       "      <td>1</td>\n",
       "      <td>2.575187</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  board_t  env_t  light_strength  efficiency  efficiency_A  efficiency_B  \\\n",
       "2  10   -19.14  -17.4              34       80.55        106.32         16.98   \n",
       "3  11   -18.73  -17.3              30       99.90        139.00         21.20   \n",
       "4  12   -17.54  -17.0              41       82.48        114.86         14.91   \n",
       "6  14   -15.43  -16.6              53       73.98        101.72         15.55   \n",
       "7  15   -14.60  -16.3              65       64.62         86.86         13.09   \n",
       "\n",
       "   efficiency_C    V_A    V_B          ...               P_A     P_B      P_C  \\\n",
       "2        118.36  729.0  709.0          ...            976.86  155.98  1087.50   \n",
       "3        139.51  728.0  717.0          ...           1128.40  172.08  1132.56   \n",
       "4        117.66  731.0  722.0          ...           1279.25  166.06  1310.40   \n",
       "6        104.67  730.0  727.0          ...           1474.60  225.37  1517.34   \n",
       "7         93.92  727.0  729.0          ...           1548.51  233.28  1674.40   \n",
       "\n",
       "     P_avg  wind_speed  wind_direction         y  is_train  avg_value  \\\n",
       "2   740.11         0.6             272  1.437752         1   1.692575   \n",
       "3   811.01         0.8             275  1.692575         1   1.706770   \n",
       "4   918.57         1.1             283  1.975787         1   2.031615   \n",
       "6  1072.44         0.9             280  2.370656         1   2.253939   \n",
       "7  1152.06         1.1             280  2.532091         1   2.575187   \n",
       "\n",
       "   old_SoCalledSF_P_avg  \n",
       "2           1172.806667  \n",
       "3           1172.806667  \n",
       "4           1172.806667  \n",
       "6           1172.806667  \n",
       "7           1172.806667  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重前训练集条数:9000\n",
      "去重后训练集条数:8918\n"
     ]
    }
   ],
   "source": [
    "print('去重前训练集条数:' +str(train_data.shape[0]))\n",
    "train_data = train_data.drop_duplicates(train_data.columns.drop(['ID','avg_value','old_SoCalledSF_P_avg']), keep='first')\n",
    "print('去重后训练集条数:' +str(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.使训练集样本分布更合理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_train_test_data(train_data, test_data, poly=False, select=False):\n",
    "    Y = train_data['y']\n",
    "    X = train_data.drop(['y','ID','is_train'], axis=1)\n",
    "    test_data = test_data.drop(['y','ID','is_train'], axis=1)\n",
    "    \n",
    "    polynm = None\n",
    "    if poly:\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        polynm = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "        X = polynm.fit_transform(X)\n",
    "        test_data = polynm.transform(test_data)\n",
    "        \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    sm = None\n",
    "    if select:\n",
    "        from sklearn.feature_selection import SelectFromModel\n",
    "        sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "        X_train = sm.fit_transform(X_train, Y_train)\n",
    "        X_val = sm.transform(X_val)\n",
    "        test_data = sm.transform(test_data)\n",
    "    \n",
    "    train_X = np.concatenate([X_train, X_val])\n",
    "    train_Y = np.concatenate([Y_train, Y_val])\n",
    "    test_X = test_data\n",
    "        \n",
    "    return train_X, train_Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=True)\n",
    "# X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data)\n",
    "\n",
    "# train_X = np.concatenate([X_train, X_test])\n",
    "# train_Y = np.concatenate([y_train, y_test])\n",
    "\n",
    "\n",
    "\n",
    "# test_X = sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X = improve_train_test_data(train_data, test_data, poly=True, select=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义打分函数,  SCORE = 1/(1+RMSE)\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# def cal_score(mse):\n",
    "#     return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# 定义交叉验证函数  \n",
    "def cross_validate(models, X, Y, cv=5):\n",
    "    model_name, mse_avg, score_avg = [], [], []\n",
    "    for i, model in enumerate(models):\n",
    "        #获取模型名\n",
    "        name = str(i + 1) + '.' + str(model) \n",
    "#         print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        print(name)\n",
    "#         model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        model_name.append(name.split('(')[0])\n",
    "        #计算metric\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "#         folds=strat_k_fold.split(X,Y)\n",
    "        #apply shuffling to cross_val_score\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=0)\n",
    "        nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "#         nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "        avg_mse = np.average(-nmse)\n",
    "        mse_avg.append(avg_mse)\n",
    "        #计算分数\n",
    "        scores = cal_score(-nmse)\n",
    "        avg_score = np.average(scores)    \n",
    "        score_avg.append(avg_score)\n",
    "        print('MSE:', -nmse)\n",
    "        print('Score:', scores)\n",
    "        print('Average MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "    res['Avg MSE'] = mse_avg\n",
    "    res['Avg Score'] = score_avg\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基学习器\n",
    "\n",
    "xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=321, n_jobs=8)\n",
    "xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=456, n_jobs=8)\n",
    "xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=789, n_jobs=8)\n",
    "# n_estimators=1000  max_depth=5  'sqrt'  GradientBoostingRegressor 最佳参数 ,learning_rate=0.08\n",
    "gbdt1 = GradientBoostingRegressor(n_estimators=800, max_depth=4, max_features='log2', random_state=123,learning_rate=0.08)\n",
    "gbdt2 = GradientBoostingRegressor(n_estimators=900, max_depth=4, max_features='log2', random_state=456,learning_rate=0.08)\n",
    "gbdt3 = GradientBoostingRegressor(n_estimators=1000, max_depth=5, max_features='log2', random_state=789,learning_rate=0.08)\n",
    "# n_estimators=700, max_features='auto', random_state=2, n_jobs=8,max_depth=10\n",
    "forest1 = RandomForestRegressor(n_estimators=800, max_features='sqrt', random_state=7, n_jobs=8)\n",
    "forest2 = RandomForestRegressor(n_estimators=900, max_features='log2', random_state=9, n_jobs=8)\n",
    "forest3 = RandomForestRegressor(n_estimators=900, max_features='sqrt', random_state=11, n_jobs=8) \n",
    "\n",
    "lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) \n",
    "lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=7, n_jobs=8)\n",
    "lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=9, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = [\n",
    "    xgbt1, gbdt1, forest1, lgb1,\n",
    "    xgbt2, gbdt2, forest2, lgb2,\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]\n",
    "\n",
    "regrs_light = [\n",
    "    lgb3, xgbt3, gbdt3, forest3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    # Train_X: 原始训练集输入矩阵, Train_Y: 原始训练集输出矩阵, Test_X: 原始测试集输入矩阵\n",
    "    def fit_predict(self, Train_X, Train_Y, Test_X):\n",
    "        Train_X = np.array(Train_X)\n",
    "        Train_Y = np.array(Train_Y)\n",
    "        Test_X = np.array(Test_X)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=2018).split(Train_X, Train_Y))       \n",
    "        \n",
    "        # 以基学习器预测结果为特征的 stacker训练数据 与 stacker预测数据\n",
    "        # 原始训练集预测结果容器\n",
    "        S_train = np.zeros((Train_X.shape[0], len(self.base_models)))\n",
    "        # 原始测试集预测结果容器\n",
    "        S_predict = np.zeros((Test_X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for n_model, regr in enumerate(self.base_models):\n",
    "            print(n_model + 1, 'Base model:', str(regr).split('(')[0])\n",
    "            S_predict_i = np.zeros((Test_X.shape[0], self.n_splits))\n",
    "            \n",
    "            for n_fold, (train_idx, test_idx) in enumerate(folds):\n",
    "                # 将X分为训练集与测试集\n",
    "                X_train_fold, Y_train_fold, X_test_fold, Y_test_fold = Train_X[train_idx], Train_Y[train_idx], Train_X[test_idx], Train_Y[test_idx]\n",
    "                print ('Fit fold', (n_fold+1), '...')\n",
    "                regr.fit(X_train_fold, Y_train_fold)\n",
    "                Y_pred = regr.predict(X_test_fold)\n",
    "                # 每折训练得到的模型根据原始训练集中的测试折的输入矩阵预测\n",
    "                S_train[test_idx, n_model] = Y_pred\n",
    "                # 每折训练得到的模型根据原始测试集输入矩阵预测\n",
    "                S_predict_i[:, n_fold] = regr.predict(Test_X)\n",
    "            \n",
    "            S_predict[:, n_model] = S_predict_i.mean(axis=1)\n",
    "\n",
    "        nmse_score = cross_val_score(self.stacker, S_train, Train_Y, cv=5, scoring='neg_mean_squared_error')\n",
    "        print('CV MSE:', -nmse_score)\n",
    "        print('Stacker AVG MSE:', -nmse_score.mean(), 'Stacker AVG Score:', np.mean(np.divide(1, 1 + np.sqrt(-nmse_score))))\n",
    "\n",
    "        self.stacker.fit(S_train, Train_Y)\n",
    "        res = self.stacker.predict(S_predict)\n",
    "        return res, S_train, S_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs_light)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "# stacker = Stacker(5, stacking_model, regrs)\n",
    "# pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result['score'] = pred_stack\n",
    "\n",
    "# index = df_result[df_result['ID'].isin(special_missing_ID)].index\n",
    "# df_result.loc[index, 'score'] = 0.379993053\n",
    "\n",
    "# df_result.to_csv('../result/081701_08789.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
