{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.问题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.问题类别"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一元回归问题.\n",
    "训练集样本9000条(包括82条重复样本)\n",
    "测试集样本8409条\n",
    "原始特征19个\n",
    "\n",
    "预测量为样本对应的发电量\n",
    "score = 1/(1+RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.特征分析 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "时间戳\n",
    "ID：当前记录条数； \n",
    "\n",
    "测量物理量(连续)\n",
    "板温：光伏电池板背测温度；\n",
    "现场温度：光伏电站现场温度；\n",
    "转换效率：为计算得到的平均转换效率；\n",
    "电压A：为数据采集点A处汇流箱电压值；\n",
    "电压B：为数据采集点B处汇流箱电压值；\n",
    "电压C：为数据采集点C处汇流箱电压值；\n",
    "电流A：为采集点A处汇流箱电流值；\n",
    "电流B：为采集点B处汇流箱电流值；\n",
    "电流C：为采集点C处汇流箱电流值；\n",
    "风速：为光伏电厂现场风速测量值；\n",
    "风向：为光伏电厂现场风的来向；\n",
    "\n",
    "计算物理量\n",
    "转换效率A：数据采集点A处的光伏板转换效率；\n",
    "转换效率B：数据采集点B处的光伏板转换效率；\n",
    "转换效率C：数据采集点C处的光伏板转换效率；\n",
    "功率A：为采集点A处的功率Pa，P=UI；\n",
    "功率B：为采集点B处的功率Pb，P=UI；\n",
    "功率C：为采集点C处的功率Pc，P=UI；\n",
    "平均功率：为A、B、C三点功率的平均值：(Pa+Pb+Pc)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.数据挖掘"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "详见 0803_数据挖掘.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.建立基线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.准备工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.原始数据准备(不进行任何清洗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/public_raw.train.csv')\n",
    "test_raw = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "train_raw['is_train']=1\n",
    "test_raw['is_train']=0\n",
    "\n",
    "df = pd.concat([train_raw, test_raw],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.特征工程(完全使用原始特征)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1.准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n"
     ]
    }
   ],
   "source": [
    "# CV时不方便shuffle，因此准备训练集和测试集时shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 准备训练集和测试集\n",
    "train = df[df['is_train']==1]\n",
    "test = df[df['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义打分函数,  SCORE = 1/(1+RMSE)\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# def cal_score(mse):\n",
    "#     return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# 定义交叉验证函数  \n",
    "def cross_validate(models, X, Y, cv=5):\n",
    "    model_name, mse_avg, score_avg = [], [], []\n",
    "    for i, model in enumerate(models):\n",
    "        #获取模型名\n",
    "        name = str(i + 1) + '.' + str(model) \n",
    "#         print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        print(name)\n",
    "#         model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        model_name.append(name.split('(')[0])\n",
    "        #计算metric\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "#         folds=strat_k_fold.split(X,Y)\n",
    "        #apply shuffling to cross_val_score\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=0)\n",
    "        nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "#         nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "        avg_mse = np.average(-nmse)\n",
    "        mse_avg.append(avg_mse)\n",
    "        #计算分数\n",
    "        scores = cal_score(-nmse)\n",
    "        avg_score = np.average(scores)    \n",
    "        score_avg.append(avg_score)\n",
    "        print('MSE:', -nmse)\n",
    "        print('Score:', scores)\n",
    "        print('Average MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "    res['Avg MSE'] = mse_avg\n",
    "    res['Avg Score'] = score_avg\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2.Cross Validation（LightGBM/XGBoost/RF/GBM）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基学习器\n",
    "\n",
    "xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=321, n_jobs=8)\n",
    "xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=456, n_jobs=8)\n",
    "xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=789, n_jobs=8)\n",
    "# n_estimators=1000  max_depth=5  'sqrt'  GradientBoostingRegressor 最佳参数 ,learning_rate=0.08\n",
    "gbdt1 = GradientBoostingRegressor(n_estimators=800, max_depth=4, max_features='log2', random_state=123,learning_rate=0.08)\n",
    "gbdt2 = GradientBoostingRegressor(n_estimators=900, max_depth=4, max_features='log2', random_state=456,learning_rate=0.08)\n",
    "gbdt3 = GradientBoostingRegressor(n_estimators=1000, max_depth=5, max_features='log2', random_state=789,learning_rate=0.08)\n",
    "# n_estimators=700, max_features='auto', random_state=2, n_jobs=8,max_depth=10\n",
    "forest1 = RandomForestRegressor(n_estimators=800, max_features='sqrt', random_state=7, n_jobs=8)\n",
    "forest2 = RandomForestRegressor(n_estimators=900, max_features='log2', random_state=9, n_jobs=8)\n",
    "forest3 = RandomForestRegressor(n_estimators=900, max_features='sqrt', random_state=11, n_jobs=8) \n",
    "\n",
    "lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) \n",
    "lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=7, n_jobs=8)\n",
    "lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=9, n_jobs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = [\n",
    "    xgbt1, gbdt1, forest1, lgb1,\n",
    "    xgbt2, gbdt2, forest2, lgb2,\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]\n",
    "\n",
    "regrs_light = [\n",
    "    lgb3, xgbt3, gbdt3, forest3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.01779459 0.02274581 0.01631306 0.06987453 0.0616137 ]\n",
      "Score: [0.88230389 0.86894777 0.88674289 0.7909278  0.80114014]\n",
      "Average MSE: 0.03766833784406778  - Score: 0.8460124974576955 \n",
      "\n",
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "MSE: [0.01630178 0.0116917  0.01464583 0.0609594  0.06830057]\n",
      "Score: [0.88677762 0.90242271 0.89204478 0.80198921 0.79280528]\n",
      "Average MSE: 0.034379855460550286  - Score: 0.8552079174446255 \n",
      "\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "MSE: [0.01954347 0.02003072 0.01258313 0.0434202  0.06188575]\n",
      "Score: [0.87734847 0.87601733 0.89913944 0.82755758 0.80078896]\n",
      "Average MSE: 0.03149265344349904  - Score: 0.8561703548441431 \n",
      "\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=8,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "MSE: [0.02420318 0.03660947 0.01679753 0.06592874 0.06651812]\n",
      "Score: [0.86537103 0.83939374 0.88526498 0.79569313 0.79496875]\n",
      "Average MSE: 0.04201140754193676  - Score: 0.8361383257406418 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.037668</td>\n",
       "      <td>0.846012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.034380</td>\n",
       "      <td>0.855208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.031493</td>\n",
       "      <td>0.856170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.042011</td>\n",
       "      <td>0.836138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model   Avg MSE  Avg Score\n",
       "0              1.LGBMRegressor  0.037668   0.846012\n",
       "1               2.XGBRegressor  0.034380   0.855208\n",
       "2  3.GradientBoostingRegressor  0.031493   0.856170\n",
       "3      4.RandomForestRegressor  0.042011   0.836138"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3.使用LightGBM/XGBoost/RF/GBM多种模型训练后融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    # Train_X: 原始训练集输入矩阵, Train_Y: 原始训练集输出矩阵, Test_X: 原始测试集输入矩阵\n",
    "    def fit_predict(self, Train_X, Train_Y, Test_X):\n",
    "        Train_X = np.array(Train_X)\n",
    "        Train_Y = np.array(Train_Y)\n",
    "        Test_X = np.array(Test_X)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=2018).split(Train_X, Train_Y))       \n",
    "        \n",
    "        # 以基学习器预测结果为特征的 stacker训练数据 与 stacker预测数据\n",
    "        # 原始训练集预测结果容器\n",
    "        S_train = np.zeros((Train_X.shape[0], len(self.base_models)))\n",
    "        # 原始测试集预测结果容器\n",
    "        S_predict = np.zeros((Test_X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for n_model, regr in enumerate(self.base_models):\n",
    "            print(n_model + 1, 'Base model:', str(regr).split('(')[0])\n",
    "            S_predict_i = np.zeros((Test_X.shape[0], self.n_splits))\n",
    "            \n",
    "            for n_fold, (train_idx, test_idx) in enumerate(folds):\n",
    "                # 将X分为训练集与测试集\n",
    "                X_train_fold, Y_train_fold, X_test_fold, Y_test_fold = Train_X[train_idx], Train_Y[train_idx], Train_X[test_idx], Train_Y[test_idx]\n",
    "                print ('Fit fold', (n_fold+1), '...')\n",
    "                regr.fit(X_train_fold, Y_train_fold)\n",
    "                Y_pred = regr.predict(X_test_fold)\n",
    "                # 每折训练得到的模型根据原始训练集中的测试折的输入矩阵预测\n",
    "                S_train[test_idx, n_model] = Y_pred\n",
    "                # 每折训练得到的模型根据原始测试集输入矩阵预测\n",
    "                S_predict_i[:, n_fold] = regr.predict(Test_X)\n",
    "            \n",
    "            S_predict[:, n_model] = S_predict_i.mean(axis=1)\n",
    "\n",
    "        nmse_score = cross_val_score(self.stacker, S_train, Train_Y, cv=5, scoring='neg_mean_squared_error')\n",
    "        print('CV MSE:', -nmse_score)\n",
    "        print('Stacker AVG MSE:', -nmse_score.mean(), 'Stacker AVG Score:', np.mean(np.divide(1, 1 + np.sqrt(-nmse_score))))\n",
    "\n",
    "        self.stacker.fit(S_train, Train_Y)\n",
    "        res = self.stacker.predict(S_predict)\n",
    "        return res, S_train, S_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "2 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "3 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "4 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "CV MSE: [0.01488159 0.01912186 0.07429619 0.01533122 0.01309632]\n",
      "Stacker AVG MSE: 0.027345434603574963 Stacker AVG Score: 0.8685469979970518\n"
     ]
    }
   ],
   "source": [
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs_light)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local CV: 0.7909689115097466\n",
    "\n",
    "ans['y_n_shuffle'] = pred_stack\n",
    "ans['y_n_shuffle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.695812\n",
       "std         3.459761\n",
       "min        -0.131349\n",
       "25%         2.505766\n",
       "50%         5.704603\n",
       "75%         8.887224\n",
       "max        12.348426\n",
       "Name: y_shuffle, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local CV: 0.8697213790417854\n",
    "\n",
    "ans['y_shuffle'] = pred_stack\n",
    "ans['y_shuffle'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.增加数据清洗工序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n",
      "去重后训练集数量:8918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# CV时不方便shuffle，因此准备训练集和测试集时shuffle\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 准备训练集和测试集\n",
    "train = df[df['is_train']==1]\n",
    "test = df[df['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "# 对训练集进行去重\n",
    "train.drop_duplicates(train.columns.drop('ID'), keep='first', inplace=True)\n",
    "print('去重后训练集数量:'+str(len(train)))\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train','I_B','I_C'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train','I_B','I_C'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.01248851 0.05849184 0.01669115 0.03821168 0.03093328]\n",
      "Score: [0.89948119 0.80524967 0.88558722 0.83648543 0.85042794]\n",
      "Average MSE: 0.03136329128162621  - Score: 0.8554462916949553 \n",
      "\n",
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "MSE: [0.01641912 0.0590451  0.01870848 0.04441967 0.03124461]\n",
      "Score: [0.88641707 0.80451042 0.8796785  0.82592768 0.84978991]\n",
      "Average MSE: 0.03396739665467438  - Score: 0.8492647178528626 \n",
      "\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "MSE: [0.0110443  0.0581514  0.0133631  0.03279386 0.02491824]\n",
      "Score: [0.90490213 0.80570697 0.89637954 0.84667498 0.8636659 ]\n",
      "Average MSE: 0.02805418229901856  - Score: 0.8634659053766114 \n",
      "\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=8,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "MSE: [0.01182279 0.06174776 0.02137348 0.04233212 0.03132838]\n",
      "Score: [0.90193067 0.80096695 0.87245058 0.82936089 0.84961894]\n",
      "Average MSE: 0.03372090754653591  - Score: 0.8508656067265761 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.031363</td>\n",
       "      <td>0.855446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.849265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.028054</td>\n",
       "      <td>0.863466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>0.850866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model   Avg MSE  Avg Score\n",
       "0              1.LGBMRegressor  0.031363   0.855446\n",
       "1               2.XGBRegressor  0.033967   0.849265\n",
       "2  3.GradientBoostingRegressor  0.028054   0.863466\n",
       "3      4.RandomForestRegressor  0.033721   0.850866"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "2 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "3 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "4 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "CV MSE: [0.05783203 0.01160472 0.03428923 0.01559797 0.02176324]\n",
      "Stacker AVG MSE: 0.028217437959844903 Stacker AVG Score: 0.8626126000810512\n"
     ]
    }
   ],
   "source": [
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs_light)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.696612\n",
       "std         3.457480\n",
       "min        -0.117571\n",
       "25%         2.510122\n",
       "50%         5.704925\n",
       "75%         8.886383\n",
       "max        12.132167\n",
       "Name: y_drop_duplicate, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 models stack, shuffle, local CV: 0.8710764504503057\n",
    "\n",
    "ans['y_drop_duplicate'] = pred_stack\n",
    "ans['y_drop_duplicate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 models stack, shuffle train data, local CV: 0.8626126000810512"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "仅仅去重，效果并未提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.异常值处理:增加异常修正值特征(异常通过离群程度定义)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mask_two = [-i for i in range(1,2)]+[i for i in range(1,2)]\n",
    "rolling_mask_four = [-i for i in range(1,3)]+[i for i in range(1,3)]\n",
    "rolling_mask_six = [-i for i in range(1,4)]+[i for i in range(1,4)]\n",
    "rolling_mask_eight = [-i for i in range(1,5)]+[i for i in range(1,5)]\n",
    "rolling_mask_ten = [-i for i in range(1,6)]+[i for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重前训练集条数:9000\n",
      "去重前测试集条数:8409\n"
     ]
    }
   ],
   "source": [
    "# 由于训练集中存在重复数据:1)连续重复数据 2)电学测量值为0的重复数据\n",
    "\n",
    "# 对于训练集,重复项全部去除\n",
    "train_raw = pd.read_csv('../data/public_raw.train.csv')\n",
    "print('去重前训练集条数:' +str(train_raw.shape[0]))\n",
    "# train_raw.drop_duplicates(train_raw.columns.drop('ID'), keep='first', inplace=True)\n",
    "# print('去重后训练集条数:' +str(train_raw.shape[0]))\n",
    "\n",
    "# 对于测试集,仅仅通过ID去除电学测量值为0的重复数据\n",
    "test_raw = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "# 在去重前准备完整测试集ID\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_raw['ID']\n",
    "\n",
    "# 电学测量量全为0的异常点个数\n",
    "# zero_sample_ID = test_raw[test_raw[(test_raw == 0) | (test_raw == 0.)].count(axis=1) > 13]['ID'].tolist()\n",
    "# print('测试集中电学测量量全为0的异常点个数为: '+str(len(zero_sample_ID)))\n",
    "\n",
    "# 根据ID全部去除\n",
    "print('去重前测试集条数:' +str(test_raw.shape[0]))\n",
    "# test_raw = test_raw[~test_raw['ID'].isin(zero_sample_ID)].reset_index(drop=True)\n",
    "# print('去重后测试集条数:' +str(test_raw.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_raw['is_train']=1\n",
    "test_raw['is_train']=0\n",
    "\n",
    "df = pd.concat([train_raw, test_raw],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "Int64Index([  527,   529,   531,   673,   859,  1373,  1561,  1722,  2071,\n",
      "             3108,  3253,  3255,  3303,  3305,  3311,  3316,  3318,  3429,\n",
      "             4671,  4672,  5083, 16413],\n",
      "           dtype='int64')\n",
      "Int64Index([  527,   529,   531,  2071,  1561, 16413,   673,  3108,  3253,\n",
      "             3255,  1722,  4671,  4672,  5083,   859,  1373,  3429,  3303,\n",
      "             3305,  3311,  3316,  3318],\n",
      "           dtype='int64')\n",
      "ID :591.0\n",
      "6.68 is abnormal as value of  I_A\n",
      "Has been replaced by 0.9400000000000001\n",
      "ID :593.0\n",
      "6.68 is abnormal as value of  I_A\n",
      "Has been replaced by 1.56\n",
      "ID :595.0\n",
      "6.77 is abnormal as value of  I_A\n",
      "Has been replaced by 0.9650000000000001\n",
      "ID :737.0\n",
      "6.53 is abnormal as value of  I_A\n",
      "Has been replaced by 0.345\n",
      "ID :948.0\n",
      "6.87 is abnormal as value of  I_A\n",
      "Has been replaced by 0.33\n",
      "ID :1519.0\n",
      "7.04 is abnormal as value of  I_A\n",
      "Has been replaced by 0.77\n",
      "ID :1717.0\n",
      "6.81 is abnormal as value of  I_A\n",
      "Has been replaced by 0.33999999999999997\n",
      "ID :1894.0\n",
      "7.1 is abnormal as value of  I_A\n",
      "Has been replaced by 0.745\n",
      "ID :2271.0\n",
      "7.22 is abnormal as value of  I_A\n",
      "Has been replaced by 1.105\n",
      "ID :3393.0\n",
      "6.64 is abnormal as value of  I_A\n",
      "Has been replaced by 0.335\n",
      "ID :3538.0\n",
      "6.56 is abnormal as value of  I_A\n",
      "Has been replaced by 0.8999999999999999\n",
      "ID :3540.0\n",
      "6.5 is abnormal as value of  I_A\n",
      "Has been replaced by 0.88\n",
      "ID :3597.0\n",
      "6.93 is abnormal as value of  I_A\n",
      "Has been replaced by 0.315\n",
      "ID :3599.0\n",
      "6.97 is abnormal as value of  I_A\n",
      "Has been replaced by 0.32\n",
      "ID :3605.0\n",
      "7.01 is abnormal as value of  I_A\n",
      "Has been replaced by 0.325\n",
      "ID :3610.0\n",
      "6.94 is abnormal as value of  I_A\n",
      "Has been replaced by 0.33\n",
      "ID :3612.0\n",
      "7.13 is abnormal as value of  I_A\n",
      "Has been replaced by 0.335\n",
      "ID :3723.0\n",
      "7.21 is abnormal as value of  I_A\n",
      "Has been replaced by 0.355\n",
      "ID :5067.0\n",
      "0.78 is abnormal as value of  I_A\n",
      "Has been replaced by 0.0\n",
      "ID :5068.0\n",
      "1.02 is abnormal as value of  I_A\n",
      "Has been replaced by 0.39\n",
      "ID :5521.0\n",
      "1.66 is abnormal as value of  I_A\n",
      "Has been replaced by 0.355\n",
      "ID :16879.0\n",
      "6.87 is abnormal as value of  I_A\n",
      "Has been replaced by 1.755\n",
      "Int64Index([14, 859, 981, 1065, 1237, 1419, 3303, 3305, 3309, 3311, 3313], dtype='int64')\n",
      "Int64Index([   14,   499,   673,   859,   981,  1065,  1237,  1373,  1419,\n",
      "             1561,  1722,  2071,  3108,  3253,  3254,  3255,  3303,  3305,\n",
      "             3309,  3311,  3313,  3318,  3429,  4068,  4671,  4672,  5083,\n",
      "             7281, 16376, 16404, 16412, 16582],\n",
      "           dtype='int64')\n",
      "Int64Index([ 1419,    14, 16404,  2071,  1561, 16412,   673,  3108,  1065,\n",
      "             3253,  3254,  3255,  1722,  4671,  4672, 16582,  1237,   981,\n",
      "              859,  5083,  1373,  4068,  3429,  3303,  3305,  3309,  3311,\n",
      "             3313,  7281,   499,  3318, 16376],\n",
      "           dtype='int64')\n",
      "ID :22.0\n",
      "645.39 is abnormal as value of  I_B\n",
      "Has been replaced by 2.84\n",
      "ID :543.0\n",
      "6.78 is abnormal as value of  I_B\n",
      "Has been replaced by 1.67\n",
      "ID :737.0\n",
      "6.45 is abnormal as value of  I_B\n",
      "Has been replaced by 0.35\n",
      "ID :948.0\n",
      "638.94 is abnormal as value of  I_B\n",
      "Has been replaced by 0.3\n",
      "ID :1070.0\n",
      "653.71 is abnormal as value of  I_B\n",
      "Has been replaced by 5.32\n",
      "ID :1173.0\n",
      "649.75 is abnormal as value of  I_B\n",
      "Has been replaced by 6.275\n",
      "ID :1362.0\n",
      "652.02 is abnormal as value of  I_B\n",
      "Has been replaced by 5.35\n",
      "ID :1519.0\n",
      "7.1 is abnormal as value of  I_B\n",
      "Has been replaced by 0.37\n",
      "ID :1565.0\n",
      "653.23 is abnormal as value of  I_B\n",
      "Has been replaced by 5.12\n",
      "ID :1717.0\n",
      "6.83 is abnormal as value of  I_B\n",
      "Has been replaced by 0.36\n",
      "ID :1894.0\n",
      "7.13 is abnormal as value of  I_B\n",
      "Has been replaced by 0.365\n",
      "ID :2271.0\n",
      "7.28 is abnormal as value of  I_B\n",
      "Has been replaced by 0.36\n",
      "ID :3393.0\n",
      "6.65 is abnormal as value of  I_B\n",
      "Has been replaced by 0.365\n",
      "ID :3538.0\n",
      "6.62 is abnormal as value of  I_B\n",
      "Has been replaced by 0.36\n",
      "ID :3539.0\n",
      "6.62 is abnormal as value of  I_B\n",
      "Has been replaced by 0.355\n",
      "ID :3540.0\n",
      "6.56 is abnormal as value of  I_B\n",
      "Has been replaced by 3.48\n",
      "ID :3597.0\n",
      "643.45 is abnormal as value of  I_B\n",
      "Has been replaced by 0.335\n",
      "ID :3599.0\n",
      "643.58 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3603.0\n",
      "643.75 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3605.0\n",
      "643.83 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3607.0\n",
      "643.94 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3612.0\n",
      "7.05 is abnormal as value of  I_B\n",
      "Has been replaced by 1.15\n",
      "ID :3723.0\n",
      "7.06 is abnormal as value of  I_B\n",
      "Has been replaced by 0.35\n",
      "ID :4401.0\n",
      "1.77 is abnormal as value of  I_B\n",
      "Has been replaced by 0.38\n",
      "ID :5067.0\n",
      "0.78 is abnormal as value of  I_B\n",
      "Has been replaced by 0.0\n",
      "ID :5068.0\n",
      "0.97 is abnormal as value of  I_B\n",
      "Has been replaced by 0.39\n",
      "ID :5521.0\n",
      "6.72 is abnormal as value of  I_B\n",
      "Has been replaced by 0.375\n",
      "ID :7742.0\n",
      "1.38 is abnormal as value of  I_B\n",
      "Has been replaced by 0.37\n",
      "ID :16842.0\n",
      "8.86 is abnormal as value of  I_B\n",
      "Has been replaced by 3.25\n",
      "ID :16870.0\n",
      "7.49 is abnormal as value of  I_B\n",
      "Has been replaced by 1.77\n",
      "ID :16878.0\n",
      "5.44 is abnormal as value of  I_B\n",
      "Has been replaced by 2.015\n",
      "ID :17048.0\n",
      "8.14 is abnormal as value of  I_B\n",
      "Has been replaced by 2.05\n",
      "Int64Index([  527,   528,   529,   530,  1067,  1373,  1722,  1950,  2036,\n",
      "             2071,  3255,  3316,  3317,  3318,  5083, 15971],\n",
      "           dtype='int64')\n",
      "Int64Index([  527,   528,   529,   530,   673,   859,  1067,  1373,  1561,\n",
      "             1722,  1950,  2036,  2071,  3108,  3255,  3303,  3305,  3309,\n",
      "             3311,  3316,  3317,  3318,  3429,  4068,  4671,  4672,  5083,\n",
      "            10456, 15971, 16376, 16399, 16404, 16413],\n",
      "           dtype='int64')\n",
      "Int64Index([  527,   528,   529,   530, 16399, 16404,  2071,  1561, 16413,\n",
      "             1950,   673,  3108,  1067,  3255,  1722,  4671,  4672, 10456,\n",
      "              859,  5083,  1373, 15971,  4068,  3429,  3303,  3305,  3309,\n",
      "             3311,  2036,  3316,  3317,  3318, 16376],\n",
      "           dtype='int64')\n",
      "ID :591.0\n",
      "640.77 is abnormal as value of  I_C\n",
      "Has been replaced by 3.54\n",
      "ID :592.0\n",
      "640.77 is abnormal as value of  I_C\n",
      "Has been replaced by 3.56\n",
      "ID :593.0\n",
      "640.85 is abnormal as value of  I_C\n",
      "Has been replaced by 323.775\n",
      "ID :594.0\n",
      "640.85 is abnormal as value of  I_C\n",
      "Has been replaced by 323.815\n",
      "ID :737.0\n",
      "6.52 is abnormal as value of  I_C\n",
      "Has been replaced by 0.345\n",
      "ID :948.0\n",
      "2.37 is abnormal as value of  I_C\n",
      "Has been replaced by 0.575\n",
      "ID :1175.0\n",
      "649.96 is abnormal as value of  I_C\n",
      "Has been replaced by 4.21\n",
      "ID :1519.0\n",
      "639.04 is abnormal as value of  I_C\n",
      "Has been replaced by 0.745\n",
      "ID :1717.0\n",
      "6.82 is abnormal as value of  I_C\n",
      "Has been replaced by 0.375\n",
      "ID :1894.0\n",
      "635.6 is abnormal as value of  I_C\n",
      "Has been replaced by 0.7949999999999999\n",
      "ID :2137.0\n",
      "652.04 is abnormal as value of  I_C\n",
      "Has been replaced by 5.66\n",
      "ID :2223.0\n",
      "649.69 is abnormal as value of  I_C\n",
      "Has been replaced by 3.01\n",
      "ID :2271.0\n",
      "639.06 is abnormal as value of  I_C\n",
      "Has been replaced by 1.12\n",
      "ID :3393.0\n",
      "6.52 is abnormal as value of  I_C\n",
      "Has been replaced by 0.375\n",
      "ID :3540.0\n",
      "647.62 is abnormal as value of  I_C\n",
      "Has been replaced by 3.4699999999999998\n",
      "ID :3597.0\n",
      "2.7 is abnormal as value of  I_C\n",
      "Has been replaced by 0.24000000000000002\n",
      "ID :3599.0\n",
      "3.35 is abnormal as value of  I_C\n",
      "Has been replaced by 0.25\n",
      "ID :3603.0\n",
      "3.16 is abnormal as value of  I_C\n",
      "Has been replaced by 0.48\n",
      "ID :3605.0\n",
      "3.36 is abnormal as value of  I_C\n",
      "Has been replaced by 0.7\n",
      "ID :3610.0\n",
      "644.15 is abnormal as value of  I_C\n",
      "Has been replaced by 0.20500000000000002\n",
      "ID :3611.0\n",
      "644.15 is abnormal as value of  I_C\n",
      "Has been replaced by 0.37\n",
      "ID :3612.0\n",
      "644.16 is abnormal as value of  I_C\n",
      "Has been replaced by 322.265\n",
      "ID :3723.0\n",
      "7.08 is abnormal as value of  I_C\n",
      "Has been replaced by 0.39\n",
      "ID :4401.0\n",
      "3.31 is abnormal as value of  I_C\n",
      "Has been replaced by 0.405\n",
      "ID :5067.0\n",
      "0.7 is abnormal as value of  I_C\n",
      "Has been replaced by 0.0\n",
      "ID :5068.0\n",
      "0.87 is abnormal as value of  I_C\n",
      "Has been replaced by 0.35\n",
      "ID :5521.0\n",
      "641.85 is abnormal as value of  I_C\n",
      "Has been replaced by 0.36\n",
      "ID :10920.0\n",
      "0.74 is abnormal as value of  I_C\n",
      "Has been replaced by 0.415\n",
      "ID :16437.0\n",
      "32.91 is abnormal as value of  I_C\n",
      "Has been replaced by 7.82\n",
      "ID :16842.0\n",
      "8.39 is abnormal as value of  I_C\n",
      "Has been replaced by 3.0549999999999997\n",
      "ID :16865.0\n",
      "7.99 is abnormal as value of  I_C\n",
      "Has been replaced by 2.3\n",
      "ID :16870.0\n",
      "8.05 is abnormal as value of  I_C\n",
      "Has been replaced by 2.36\n",
      "ID :16879.0\n",
      "6.72 is abnormal as value of  I_C\n",
      "Has been replaced by 3.9899999999999998\n",
      "Int64Index([    0,    14,   171,   174,   175,   176,   343,   498,   499,\n",
      "              512,\n",
      "            ...\n",
      "            15882, 16074, 16281, 16459, 16667, 16876, 17061, 17075, 17277,\n",
      "            17408],\n",
      "           dtype='int64', length=157)\n",
      "Int64Index([14, 859, 981, 1065, 1237, 1419, 2740, 3303, 3305, 3309, 3311, 3313,\n",
      "            4671, 4672],\n",
      "           dtype='int64')\n",
      "Int64Index([    0,   512,  1024,  6656,  7684, 17408,  1543,  2567, 15882,\n",
      "               14,\n",
      "            ...\n",
      "             5072, 13777,   981,  5083, 16876,  1520, 14833,   498,   499,\n",
      "             2036],\n",
      "           dtype='int64', length=159)\n",
      "ID :1.0\n",
      "0.0 is abnormal as value of  V_A\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([-1], dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-d2b30765cf89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ID :'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is abnormal as value of '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mreplace_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbefore_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mafter_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_cor'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplace_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Has been replaced by '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1025\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m                 \u001b[1;31m# This is an elided recursive call to iloc/loc/etc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'not applicable'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1899\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot index with multidimensional key'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1901\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1903\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\python36\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                 raise KeyError(\n\u001b[0;32m   1205\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1206\u001b[1;33m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([-1], dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# #计算偏差率的辅助列\n",
    "# # for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "# for c in ['I_A','I_B','I_C']:\n",
    "#     df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "#     df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']\n",
    "#     df[c+'_cor'] = df[c]\n",
    "    \n",
    "#     #out of range\n",
    "#     oor_index = df[df[c]>20].index\n",
    "#     print(oor_index)\n",
    "    \n",
    "#     outlier_index = df[df[c+'_exception_ratio']>1.6].index\n",
    "#     print(outlier_index)\n",
    "    \n",
    "#     ab_index = pd.Int64Index(set(list(oor_index)+list(outlier_index)))\n",
    "#     print(ab_index)\n",
    "    \n",
    "#     ab_data = df.loc[ab_index].sort_values(by='ID', ascending=True)\n",
    "    \n",
    "#     # 上下记录均值替代异常值\n",
    "#     for idx, line in ab_data.iterrows():\n",
    "#         ID = line['ID']\n",
    "#         value = line[c]\n",
    "        \n",
    "#         index = df[df['ID'] == ID].index\n",
    "            \n",
    "#         before_offset = 1\n",
    "#         while (idx + before_offset)in ab_index:\n",
    "#             before_offset += 1\n",
    "\n",
    "#         after_offset = 1\n",
    "#         while (idx + after_offset) in ab_index:\n",
    "#             after_offset += 1\n",
    "    \n",
    "#         print('ID :' + str(ID))\n",
    "#         print(value, 'is abnormal as value of ',c)\n",
    "#         replace_value = (df.loc[index - before_offset, c].values + df.loc[index + after_offset, c].values) / 2\n",
    "#         df.loc[index, c+'_cor'] = replace_value[0]\n",
    "#         print('Has been replaced by '+str(replace_value[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# for c in ['V_A','V_B','V_C']:\n",
    "#     df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "#     df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']\n",
    "#     df[c+'_cor'] = df[c]\n",
    "    \n",
    "#     #out of range\n",
    "#     oor_index = df[(df[c]>800)|(df[c]<500)].index\n",
    "#     print(oor_index)\n",
    "    \n",
    "#     outlier_index = df[df[c+'_exception_ratio']>1.6].index\n",
    "#     print(outlier_index)\n",
    "    \n",
    "#     ab_index = pd.Int64Index(set(list(oor_index)+list(outlier_index)))\n",
    "#     print(ab_index)\n",
    "    \n",
    "#     ab_data = df.loc[ab_index].sort_values(by='ID', ascending=True)\n",
    "    \n",
    "#     # 上下记录均值替代异常值\n",
    "#     for idx, line in ab_data.iterrows():\n",
    "#         ID = line['ID']\n",
    "#         value = line[c]\n",
    "        \n",
    "#         index = df[df['ID'] == ID].index\n",
    "            \n",
    "#         before_offset = 1\n",
    "#         while (idx + before_offset)in ab_index:\n",
    "#             before_offset += 1\n",
    "\n",
    "#         after_offset = 1\n",
    "#         while (idx + after_offset) in ab_index:\n",
    "#             after_offset += 1\n",
    "    \n",
    "#         print('ID :' + str(ID))\n",
    "#         print(value, 'is abnormal as value of ',c)\n",
    "#         replace_value = (df.loc[index - before_offset, c].values + df.loc[index + after_offset, c].values) / 2\n",
    "#         df.loc[index, c+'_cor'] = replace_value[0]\n",
    "#         print('Has been replaced by '+str(replace_value[0]))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "# df['P_A_cor']=df['I_A_cor']*df['V_A_cor']\n",
    "# df['P_B_cor']=df['I_B_cor']*df['V_B_cor']\n",
    "# df['P_C_cor']=df['I_C_cor']*df['V_C_cor']\n",
    "# df['P_avg_cor']=1/3*(df['P_A_cor']+df['P_B_cor']+df['P_C_cor'])\n",
    "\n",
    "\n",
    "# # df.drop(columns=['I_A','I_B','I_C','V_A','V_B','V_C','P_A','P_B','P_C','P_avg'],axis=1,inplace=True)\n",
    "# # df.drop(columns=['I_A_avg_sequence','I_A_exception_ratio','I_B_avg_sequence','I_B_exception_ratio','I_C_avg_sequence','I_C_exception_ratio','V_B_avg_sequence','V_B_exception_ratio','V_A_avg_sequence','V_A_exception_ratio','V_C_avg_sequence','V_C_exception_ratio',],axis=1,inplace=True)\n",
    "# # 0.85661\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#原始路线\n",
    "all_data = df.copy()\n",
    "bad_feature = ['ID','P_A', 'P_B', 'P_C', 'P_avg', 'env_t', 'V_A', 'V_B', 'V_C', 'I_B', 'I_C', 'efficiency', 'efficiency_A', 'efficiency_B', 'efficiency_C']\n",
    "bad_index1 = all_data[bad_feature][\n",
    "    (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "    (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "].dropna(how='all').index\n",
    "bad_index2 = all_data[\n",
    "    ((all_data['V_A']<500)&(all_data['V_A']!=0))|\n",
    "    ((all_data['V_B']<500)&(all_data['V_B']!=0))|\n",
    "    ((all_data['V_C']<500)&(all_data['V_C']!=0))].index\n",
    "bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "\n",
    "# bad_index = all_data[bad_feature][\n",
    "#     (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "#     (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "# ].dropna(how='all').index\n",
    "\n",
    "\n",
    "\n",
    "bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 上下记录均值替代异常值\n",
    "for idx, line in bad_data.iterrows():\n",
    "    ID = line['ID']\n",
    "    col_index = line[bad_feature][ \n",
    "        (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "        (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "    ].index\n",
    "    index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "    # idx - before_offset, CV  0.8684\n",
    "    before_offset = 1\n",
    "    while (idx + before_offset)in bad_index:\n",
    "        before_offset += 1\n",
    "\n",
    "    after_offset = 1\n",
    "    while (idx + after_offset) in bad_index:\n",
    "        after_offset += 1\n",
    "    \n",
    "    replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / 2\n",
    "    all_data.loc[index, col_index] = replace_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n",
      "去重后训练集条数:8918\n"
     ]
    }
   ],
   "source": [
    "# CV时不方便shuffle，因此准备训练集和测试集时shuffle\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = all_data.copy()\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 准备训练集和测试集\n",
    "train = df[df['is_train']==1]\n",
    "test = df[df['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "# 训练集中电学测量值为0的条目在构建完特征后失去价值，通过去重去掉，避免人为改变权重影响训练\n",
    "train = train.drop_duplicates(train.columns.drop(['ID']), keep='first')\n",
    "print('去重后训练集条数:' +str(train.shape[0]))\n",
    "\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.01685016 0.01555956 0.01799888 0.04183777 0.05964575]\n",
      "Score: [0.88510601 0.88909599 0.88170987 0.83019048 0.80371328]\n",
      "Average MSE: 0.03037842546310161  - Score: 0.8579631268893145 \n",
      "\n",
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "MSE: [0.01586283 0.02608264 0.01747887 0.04406287 0.05994445]\n",
      "Score: [0.88814069 0.86095476 0.88323018 0.82650669 0.80331896]\n",
      "Average MSE: 0.03268633263493488  - Score: 0.8524302547673 \n",
      "\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "MSE: [0.0131751  0.01411839 0.01653039 0.04360461 0.06127044]\n",
      "Score: [0.89703571 0.89379815 0.88607664 0.82725496 0.80158478]\n",
      "Average MSE: 0.029739786299245445  - Score: 0.8611500474605125 \n",
      "\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=8,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "MSE: [0.01533401 0.01715566 0.01900615 0.03756984 0.06810185]\n",
      "Score: [0.88981386 0.88418924 0.87884059 0.83764062 0.79304449]\n",
      "Average MSE: 0.03143349932273756  - Score: 0.8567057588229501 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.030378</td>\n",
       "      <td>0.857963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.032686</td>\n",
       "      <td>0.852430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.029740</td>\n",
       "      <td>0.861150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.031433</td>\n",
       "      <td>0.856706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model   Avg MSE  Avg Score\n",
       "0              1.LGBMRegressor  0.030378   0.857963\n",
       "1               2.XGBRegressor  0.032686   0.852430\n",
       "2  3.GradientBoostingRegressor  0.029740   0.861150\n",
       "3      4.RandomForestRegressor  0.031433   0.856706"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "2 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "3 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "4 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "CV MSE: [0.03820138 0.01411471 0.01500888 0.01062797 0.08564431]\n",
      "Stacker AVG MSE: 0.03271945123206558 Stacker AVG Score: 0.860264287075261\n"
     ]
    }
   ],
   "source": [
    "# 训练集未shuflle时\n",
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs_light)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "2 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "3 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "4 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "CV MSE: [0.02937148 0.03176862 0.01071031 0.03045894 0.0620819 ]\n",
      "Stacker AVG MSE: 0.032878251749318445 Stacker AVG Score: 0.8521156276701483\n"
     ]
    }
   ],
   "source": [
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs_light)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.增加特征工程工序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.增加前后有效发电量均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前二后二\n",
    "next_one = []\n",
    "prev_one = []\n",
    "next_id = []\n",
    "prev_id = []\n",
    "\n",
    "second_next_one = []\n",
    "second_prev_one = []\n",
    "\n",
    "df_len = df.shape[0]\n",
    "\n",
    "i_y =df.columns.get_loc(\"y\")\n",
    "\n",
    "def get_prev_nn_index(cur_i):\n",
    "    prev_i = cur_i-1\n",
    "    while(prev_i>=0 and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i-=1\n",
    "    return prev_i\n",
    "\n",
    "def get_next_nn_index(cur_i):\n",
    "    prev_i = cur_i+1\n",
    "    while(prev_i<df_len and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i+=1\n",
    "    return prev_i\n",
    "\n",
    "for i in range(df_len):\n",
    "    f_pre_i=get_prev_nn_index(i)\n",
    "    if(f_pre_i)<0:\n",
    "        prev_one.append(np.nan)\n",
    "        prev_id.append(0)\n",
    "    else:\n",
    "        prev_one.append(df.iat[f_pre_i,i_y])\n",
    "        prev_id.append(f_pre_i)\n",
    "        \n",
    "    s_pre_i=get_prev_nn_index(f_pre_i)\n",
    "    if (s_pre_i)<0:\n",
    "        second_prev_one.append(np.nan)\n",
    "    else:\n",
    "        second_prev_one.append(df.iat[s_pre_i,i_y])\n",
    "    \n",
    "    f_next_i=get_next_nn_index(i)\n",
    "    if(f_next_i<df_len):\n",
    "        next_one.append(df.iat[f_next_i,i_y])\n",
    "        next_id.append(f_next_i)\n",
    "    else:\n",
    "        next_one.append(np.nan)\n",
    "        next_id.append(df_len)\n",
    "    \n",
    "    s_next_i=get_next_nn_index(f_next_i)\n",
    "    if(s_next_i<df_len):\n",
    "        second_next_one.append(df.iat[s_next_i,i_y])\n",
    "    else:\n",
    "        second_next_one.append(np.nan)\n",
    "        \n",
    "\n",
    "df['next_value'] = next_one\n",
    "df['prev_value'] = prev_one\n",
    "df['avg_value'] = np.nanmean([df['next_value'], df['prev_value']],axis=0)\n",
    "\n",
    "df.drop(['next_value','prev_value'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg(df):\n",
    "    array = np.array(df[\"P_avg\"])\n",
    "    newarray=[]\n",
    "    num = 0\n",
    "    for i in np.arange(len(array)):\n",
    "        for j in np.arange(10):\n",
    "            if i<10:\n",
    "                num = (array[j-1]+array[j-2]+array[j-3])/3\n",
    "            if i>=10:\n",
    "                num = (array[i-1]+array[i-2]+array[i-3]+array[i-5]+array[i-6]+array[i-7]+array[i-8]+array[i-9])/9\n",
    "        newarray.append(num)\n",
    "    df[\"old_SoCalledSF_P_avg\"] = newarray\n",
    "    return df\n",
    "\n",
    "df = add_avg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n"
     ]
    }
   ],
   "source": [
    "# CV时不方便shuffle，因此准备训练集和测试集时shuffle\n",
    "# train_test_set\n",
    "tt = df.copy()\n",
    "\n",
    "tt = tt.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 准备训练集和测试集\n",
    "train = tt[tt['is_train']==1]\n",
    "test = tt[tt['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.01821326 0.01621795 0.08931631 0.01187811 0.02710777]\n",
      "Score: [0.88109104 0.88703619 0.7699069  0.90172402 0.8586312 ]\n",
      "Average MSE: 0.032546681038947124  - Score: 0.8596778691461715 \n",
      "\n",
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "MSE: [0.01836692 0.01909558 0.08252651 0.0107367  0.02709432]\n",
      "Score: [0.88065022 0.87859043 0.77683522 0.90611057 0.85866131]\n",
      "Average MSE: 0.03156400853565384  - Score: 0.860169551268342 \n",
      "\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "MSE: [0.01817265 0.01507861 0.0858109  0.01053054 0.03044015]\n",
      "Score: [0.8812079  0.89063453 0.7734341  0.90693205 0.85144713]\n",
      "Average MSE: 0.03200657060603993  - Score: 0.8607311426191785 \n",
      "\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=8,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "MSE: [0.02113726 0.01930237 0.10382996 0.01640501 0.02881017]\n",
      "Score: [0.87306766 0.87801479 0.75630004 0.88646036 0.85489404]\n",
      "Average MSE: 0.037896955568401614  - Score: 0.8497473777571033 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.859678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.031564</td>\n",
       "      <td>0.860170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.032007</td>\n",
       "      <td>0.860731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.849747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model   Avg MSE  Avg Score\n",
       "0              1.LGBMRegressor  0.032547   0.859678\n",
       "1               2.XGBRegressor  0.031564   0.860170\n",
       "2  3.GradientBoostingRegressor  0.032007   0.860731\n",
       "3      4.RandomForestRegressor  0.037897   0.849747"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "原始数据去重是否对提高精度有利？\n",
    "没有新增有价值特征时可以没有好处，甚至降低。\n",
    "有新增特征时可能有好处，减少了重复度很高样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前二后二\n",
    "next_one = []\n",
    "prev_one = []\n",
    "next_id = []\n",
    "prev_id = []\n",
    "\n",
    "second_next_one = []\n",
    "second_prev_one = []\n",
    "\n",
    "df_len = all_data.shape[0]\n",
    "\n",
    "i_y =all_data.columns.get_loc(\"y\")\n",
    "\n",
    "def get_prev_nn_index(cur_i):\n",
    "    prev_i = cur_i-1\n",
    "    while(prev_i>=0 and pd.isnull(all_data.iat[prev_i,i_y])):\n",
    "        prev_i-=1\n",
    "    return prev_i\n",
    "\n",
    "def get_next_nn_index(cur_i):\n",
    "    prev_i = cur_i+1\n",
    "    while(prev_i<df_len and pd.isnull(all_data.iat[prev_i,i_y])):\n",
    "        prev_i+=1\n",
    "    return prev_i\n",
    "\n",
    "for i in range(df_len):\n",
    "    f_pre_i=get_prev_nn_index(i)\n",
    "    if(f_pre_i)<0:\n",
    "        prev_one.append(np.nan)\n",
    "        prev_id.append(0)\n",
    "    else:\n",
    "        prev_one.append(all_data.iat[f_pre_i,i_y])\n",
    "        prev_id.append(f_pre_i)\n",
    "        \n",
    "    s_pre_i=get_prev_nn_index(f_pre_i)\n",
    "    if (s_pre_i)<0:\n",
    "        second_prev_one.append(np.nan)\n",
    "    else:\n",
    "        second_prev_one.append(all_data.iat[s_pre_i,i_y])\n",
    "    \n",
    "    f_next_i=get_next_nn_index(i)\n",
    "    if(f_next_i<df_len):\n",
    "        next_one.append(all_data.iat[f_next_i,i_y])\n",
    "        next_id.append(f_next_i)\n",
    "    else:\n",
    "        next_one.append(np.nan)\n",
    "        next_id.append(df_len)\n",
    "    \n",
    "    s_next_i=get_next_nn_index(f_next_i)\n",
    "    if(s_next_i<df_len):\n",
    "        second_next_one.append(all_data.iat[s_next_i,i_y])\n",
    "    else:\n",
    "        second_next_one.append(np.nan)\n",
    "        \n",
    "\n",
    "all_data['next_value'] = next_one\n",
    "all_data['prev_value'] = prev_one\n",
    "all_data['avg_value'] = np.nanmean([all_data['next_value'], all_data['prev_value']],axis=0)\n",
    "\n",
    "all_data.drop(['next_value','prev_value'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg(df):\n",
    "    array = np.array(df[\"P_avg\"])\n",
    "    newarray=[]\n",
    "    num = 0\n",
    "    for i in np.arange(len(array)):\n",
    "        for j in np.arange(10):\n",
    "            if i<10:\n",
    "                num = (array[j-1]+array[j-2]+array[j-3])/3\n",
    "            if i>=10:\n",
    "                num = (array[i-1]+array[i-2]+array[i-3]+array[i-5]+array[i-6]+array[i-7]+array[i-8]+array[i-9])/9\n",
    "        newarray.append(num)\n",
    "    df[\"old_SoCalledSF_P_avg\"] = newarray\n",
    "    return df\n",
    "\n",
    "all_data = add_avg(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>board_t</th>\n",
       "      <th>env_t</th>\n",
       "      <th>light_strength</th>\n",
       "      <th>efficiency</th>\n",
       "      <th>efficiency_A</th>\n",
       "      <th>efficiency_B</th>\n",
       "      <th>efficiency_C</th>\n",
       "      <th>V_A</th>\n",
       "      <th>V_B</th>\n",
       "      <th>...</th>\n",
       "      <th>P_A</th>\n",
       "      <th>P_B</th>\n",
       "      <th>P_C</th>\n",
       "      <th>P_avg</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>y</th>\n",
       "      <th>is_train</th>\n",
       "      <th>avg_value</th>\n",
       "      <th>old_SoCalledSF_P_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.437752</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>-19.33</td>\n",
       "      <td>-17.5</td>\n",
       "      <td>13</td>\n",
       "      <td>198.32</td>\n",
       "      <td>259.11</td>\n",
       "      <td>42.17</td>\n",
       "      <td>293.66</td>\n",
       "      <td>722.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>...</td>\n",
       "      <td>909.72</td>\n",
       "      <td>148.05</td>\n",
       "      <td>1031.03</td>\n",
       "      <td>696.27</td>\n",
       "      <td>0.3</td>\n",
       "      <td>273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.437752</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-17.4</td>\n",
       "      <td>34</td>\n",
       "      <td>80.55</td>\n",
       "      <td>106.32</td>\n",
       "      <td>16.98</td>\n",
       "      <td>118.36</td>\n",
       "      <td>729.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>...</td>\n",
       "      <td>976.86</td>\n",
       "      <td>155.98</td>\n",
       "      <td>1087.50</td>\n",
       "      <td>740.11</td>\n",
       "      <td>0.6</td>\n",
       "      <td>272</td>\n",
       "      <td>1.437752</td>\n",
       "      <td>1</td>\n",
       "      <td>1.692575</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>-18.73</td>\n",
       "      <td>-17.3</td>\n",
       "      <td>30</td>\n",
       "      <td>99.90</td>\n",
       "      <td>139.00</td>\n",
       "      <td>21.20</td>\n",
       "      <td>139.51</td>\n",
       "      <td>728.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1128.40</td>\n",
       "      <td>172.08</td>\n",
       "      <td>1132.56</td>\n",
       "      <td>811.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>275</td>\n",
       "      <td>1.692575</td>\n",
       "      <td>1</td>\n",
       "      <td>1.706770</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>-17.54</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>41</td>\n",
       "      <td>82.48</td>\n",
       "      <td>114.86</td>\n",
       "      <td>14.91</td>\n",
       "      <td>117.66</td>\n",
       "      <td>731.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1279.25</td>\n",
       "      <td>166.06</td>\n",
       "      <td>1310.40</td>\n",
       "      <td>918.57</td>\n",
       "      <td>1.1</td>\n",
       "      <td>283</td>\n",
       "      <td>1.975787</td>\n",
       "      <td>1</td>\n",
       "      <td>2.031615</td>\n",
       "      <td>1172.806667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  board_t  env_t  light_strength  efficiency  efficiency_A  efficiency_B  \\\n",
       "0   1     0.01    0.1               1        0.00          0.00          0.00   \n",
       "1   9   -19.33  -17.5              13      198.32        259.11         42.17   \n",
       "2  10   -19.14  -17.4              34       80.55        106.32         16.98   \n",
       "3  11   -18.73  -17.3              30       99.90        139.00         21.20   \n",
       "4  12   -17.54  -17.0              41       82.48        114.86         14.91   \n",
       "\n",
       "   efficiency_C    V_A    V_B          ...               P_A     P_B      P_C  \\\n",
       "0          0.00    0.0    0.0          ...              0.00    0.00     0.00   \n",
       "1        293.66  722.0  705.0          ...            909.72  148.05  1031.03   \n",
       "2        118.36  729.0  709.0          ...            976.86  155.98  1087.50   \n",
       "3        139.51  728.0  717.0          ...           1128.40  172.08  1132.56   \n",
       "4        117.66  731.0  722.0          ...           1279.25  166.06  1310.40   \n",
       "\n",
       "    P_avg  wind_speed  wind_direction         y  is_train  avg_value  \\\n",
       "0    0.00         0.1               1       NaN         0   1.437752   \n",
       "1  696.27         0.3             273       NaN         0   1.437752   \n",
       "2  740.11         0.6             272  1.437752         1   1.692575   \n",
       "3  811.01         0.8             275  1.692575         1   1.706770   \n",
       "4  918.57         1.1             283  1.975787         1   2.031615   \n",
       "\n",
       "   old_SoCalledSF_P_avg  \n",
       "0           1172.806667  \n",
       "1           1172.806667  \n",
       "2           1172.806667  \n",
       "3           1172.806667  \n",
       "4           1172.806667  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n",
      "去重后训练集条数:8918\n"
     ]
    }
   ],
   "source": [
    "# CV时不方便shuffle，因此准备训练集和测试集时shuffle\n",
    "# train_test_set\n",
    "tt = all_data.copy()\n",
    "\n",
    "tt = tt.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 准备训练集和测试集\n",
    "train = tt[tt['is_train']==1]\n",
    "test = tt[tt['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "train = train.drop_duplicates(train.columns.drop(['ID','avg_value','old_SoCalledSF_P_avg']), keep='first')\n",
    "print('去重后训练集条数:' +str(train.shape[0]))\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.01574406 0.01823915 0.01963925 0.07508936 0.01916174]\n",
      "Score: [0.88851349 0.8810166  0.87708519 0.78491433 0.87840586]\n",
      "Average MSE: 0.02957470992734625  - Score: 0.861987095604432 \n",
      "\n",
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=8, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "MSE: [0.01804452 0.01971915 0.01857635 0.074494   0.01992666]\n",
      "Score: [0.88157777 0.87686617 0.8800531  0.78558551 0.8762999 ]\n",
      "Average MSE: 0.030152135740583046  - Score: 0.8600764890430537 \n",
      "\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "MSE: [0.01256651 0.01428644 0.0211678  0.06847375 0.01575117]\n",
      "Score: [0.89919936 0.89323525 0.87298764 0.7925972  0.88849109]\n",
      "Average MSE: 0.02644913593266889  - Score: 0.8693021105285126 \n",
      "\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=8,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "MSE: [0.01542958 0.01828679 0.02382694 0.07019406 0.01877007]\n",
      "Score: [0.8895089  0.88087982 0.86628104 0.79055032 0.87950446]\n",
      "Average MSE: 0.029301486430150042  - Score: 0.8613449085735047 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.029575</td>\n",
       "      <td>0.861987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>0.860076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.026449</td>\n",
       "      <td>0.869302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.029301</td>\n",
       "      <td>0.861345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model   Avg MSE  Avg Score\n",
       "0              1.LGBMRegressor  0.029575   0.861987\n",
       "1               2.XGBRegressor  0.030152   0.860076\n",
       "2  3.GradientBoostingRegressor  0.026449   0.869302\n",
       "3      4.RandomForestRegressor  0.029301   0.861345"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
