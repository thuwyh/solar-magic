{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.问题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.问题类别"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一元回归问题.\n",
    "训练集样本9000条(包括82条重复样本)\n",
    "测试集样本8409条\n",
    "原始特征19个\n",
    "\n",
    "预测量为样本对应的发电量\n",
    "score = 1/(1+RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.特征分析 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "时间戳\n",
    "ID：当前记录条数； \n",
    "\n",
    "测量物理量(连续)\n",
    "板温：光伏电池板背测温度；\n",
    "现场温度：光伏电站现场温度；\n",
    "转换效率：为计算得到的平均转换效率；\n",
    "电压A：为数据采集点A处汇流箱电压值；\n",
    "电压B：为数据采集点B处汇流箱电压值；\n",
    "电压C：为数据采集点C处汇流箱电压值；\n",
    "电流A：为采集点A处汇流箱电流值；\n",
    "电流B：为采集点B处汇流箱电流值；\n",
    "电流C：为采集点C处汇流箱电流值；\n",
    "风速：为光伏电厂现场风速测量值；\n",
    "风向：为光伏电厂现场风的来向；\n",
    "\n",
    "计算物理量\n",
    "转换效率A：数据采集点A处的光伏板转换效率；\n",
    "转换效率B：数据采集点B处的光伏板转换效率；\n",
    "转换效率C：数据采集点C处的光伏板转换效率；\n",
    "功率A：为采集点A处的功率Pa，P=UI；\n",
    "功率B：为采集点B处的功率Pb，P=UI；\n",
    "功率C：为采集点C处的功率Pc，P=UI；\n",
    "平均功率：为A、B、C三点功率的平均值：(Pa+Pb+Pc)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.数据挖掘"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "详见 0803_数据挖掘.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.建立基线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.准备工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.数据准备(不进行清洗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/public_raw.train.csv')\n",
    "\n",
    "test_raw = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "train_raw['is_train']=1\n",
    "test_raw['is_train']=0\n",
    "\n",
    "df = pd.concat([train_raw, test_raw],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.特征工程(完全使用原始特征)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数量:9000\n",
      "测试集数量:8409\n"
     ]
    }
   ],
   "source": [
    "# 准备训练集和测试集\n",
    "train = df[df['is_train']==1]\n",
    "test = df[df['is_train']==0]\n",
    "print('训练集数量:'+str(len(train)))\n",
    "print('测试集数量:'+str(len(test)))\n",
    "\n",
    "# 准备训练集合输入矩阵和输出矩阵\n",
    "train_X = train.drop(['y','is_train','I_B','I_C'],axis=1)\n",
    "train_Y = train['y']\n",
    "\n",
    "# 准备测试集合输入矩阵\n",
    "test_X = test.drop(['y','is_train','I_B','I_C'],axis=1)\n",
    "\n",
    "# 准备测试集合输出矩阵容器\n",
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义打分函数,  SCORE = 1/(1+RMSE)\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# def cal_score(mse):\n",
    "#     return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "\n",
    "# 定义交叉验证函数  \n",
    "def cross_validate(models, X, Y, cv=5):\n",
    "    model_name, mse_avg, score_avg = [], [], []\n",
    "    for i, model in enumerate(models):\n",
    "        #获取模型名\n",
    "        name = str(i + 1) + '.' + str(model) \n",
    "#         print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        print(name)\n",
    "#         model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        model_name.append(name.split('(')[0])\n",
    "        #计算metric\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "#         folds=strat_k_fold.split(X,Y)\n",
    "        nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "        avg_mse = np.average(-nmse)\n",
    "        mse_avg.append(avg_mse)\n",
    "        #计算分数\n",
    "        scores = cal_score(-nmse)\n",
    "        avg_score = np.average(scores)    \n",
    "        score_avg.append(avg_score)\n",
    "        print('MSE:', -nmse)\n",
    "        print('Score:', scores)\n",
    "        print('Average MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "    res['Avg MSE'] = mse_avg\n",
    "    res['Avg Score'] = score_avg\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1.使用LightGBM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.1 使用LGBMRegressor和cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb2 = LGBMRegressor(n_estimators=800, max_depth=5, random_state=5, n_jobs=8, learning_rate=0.08, verbose_eval=500) \n",
    "# lgb3 = LGBMRegressor(n_estimators=700, max_depth=5, random_state=5, n_jobs=8) \n",
    "# lgb4 = LGBMRegressor(n_estimators=1000, max_depth=5, random_state=5, n_jobs=8) \n",
    "# lgb5 = LGBMRegressor(n_estimators=1100, max_depth=5, random_state=5, n_jobs=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=5, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=900,\n",
      "       n_jobs=8, num_leaves=31, objective=None, random_state=5,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "MSE: [0.41915703 0.03485974 0.02421466 0.02486614 0.08277516]\n",
      "Score: [0.6070085  0.84266753 0.86534339 0.86378908 0.77657434]\n",
      "Average MSE: 0.11717454717995457  - Score: 0.7910765690795137 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg MSE</th>\n",
       "      <th>Avg Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.117175</td>\n",
       "      <td>0.791077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model   Avg MSE  Avg Score\n",
       "0  1.LGBMRegressor  0.117175   0.791077"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(models=[lgb1], X=train_X, Y=train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.2 使用LGBMRegressor和自定义eval函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8409, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=pd.DataFrame()\n",
    "ans['ID']=test_X['ID']\n",
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_predict: \n",
      "[1.46104199 1.66531591 2.08794416 ... 9.6431778  9.43246941 0.39249984]\n",
      "local cv: 0.8494442887294936\n"
     ]
    }
   ],
   "source": [
    "# import lightgbm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "X = train_X.copy()\n",
    "Y = train_Y.copy()\n",
    "T = test_X.copy()\n",
    "\n",
    "folds = list(KFold(n_splits=5, shuffle=True, random_state=2018).split(X, Y))\n",
    "\n",
    "Y_predict = np.zeros(X.shape[0])\n",
    "T_predict = []\n",
    "# val_preds.append(np.zeros(train.shape[0]))\n",
    "for n_fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    X_train, Y_train, X_test= X.iloc[train_idx], Y.iloc[train_idx], X.iloc[test_idx]\n",
    "    lgb1.fit(X_train,Y_train)\n",
    "    #预测每折中作为验证集的训练集\n",
    "    Y_predict[test_idx] = lgb1.predict(X_test)\n",
    "    #以该折训练出的模型预测测试集\n",
    "    T_predict.append(lgb1.predict(T))\n",
    "#     print('T_predict: \\n'+str(T_predict))\n",
    "\n",
    "print('Y_predict: ')\n",
    "print(Y_predict)\n",
    "#     T_pred.append(Y_predict)\n",
    "# print('T_pred: ')\n",
    "# print(T_pred)\n",
    "\n",
    "\n",
    "print('local cv:',1/(1+np.sqrt(mean_squared_error(Y,Y_predict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict_5: \n",
      "[0.39995031 1.32710263 2.18466183 ... 9.85877277 9.90556858 9.09118279]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.697065\n",
       "std         3.460793\n",
       "min        -0.182216\n",
       "25%         2.504986\n",
       "50%         5.691466\n",
       "75%         8.887027\n",
       "max        12.134582\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local cv: 0.84944\n",
    "y_predict_5 = np.mean(T_predict, axis=0)\n",
    "print('y_predict_5: \\n'+str(y_predict_5))\n",
    "\n",
    "ans['y']=y_predict_5\n",
    "ans['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict_10: \n",
      "[0.39687394 1.26673675 2.16500899 ... 9.83409458 9.94489728 9.07060563]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.696876\n",
       "std         3.460119\n",
       "min        -0.329737\n",
       "25%         2.502208\n",
       "50%         5.704800\n",
       "75%         8.889595\n",
       "max        12.244228\n",
       "Name: y_10, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local cv: 0.84993\n",
    "y_predict_10 = np.mean(T_predict, axis=0)\n",
    "print('y_predict_10: \\n'+str(y_predict_10))\n",
    "\n",
    "ans['y_10']=y_predict_10\n",
    "ans['y_10'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.3 使用自定义带有early stopping参数的LGBM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0576157\tvalid's score: 0.80643\n",
      "Early stopping, best iteration is:\n",
      "[686]\tvalid's l2: 0.0573527\tvalid's score: 0.806787\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0200062\tvalid's score: 0.876084\n",
      "[1000]\tvalid's l2: 0.0179172\tvalid's score: 0.881947\n",
      "[1500]\tvalid's l2: 0.0174487\tvalid's score: 0.883319\n",
      "[2000]\tvalid's l2: 0.0173969\tvalid's score: 0.883472\n",
      "Early stopping, best iteration is:\n",
      "[1904]\tvalid's l2: 0.0173704\tvalid's score: 0.883551\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[345]\tvalid's l2: 0.0133098\tvalid's score: 0.896565\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0501147\tvalid's score: 0.817085\n",
      "[1000]\tvalid's l2: 0.046937\tvalid's score: 0.821929\n",
      "[1500]\tvalid's l2: 0.0455346\tvalid's score: 0.824138\n",
      "[2000]\tvalid's l2: 0.0449449\tvalid's score: 0.825081\n",
      "[2500]\tvalid's l2: 0.0447448\tvalid's score: 0.825403\n",
      "Early stopping, best iteration is:\n",
      "[2678]\tvalid's l2: 0.044713\tvalid's score: 0.825454\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0158303\tvalid's score: 0.888243\n",
      "Early stopping, best iteration is:\n",
      "[440]\tvalid's l2: 0.0157162\tvalid's score: 0.888601\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0627387\tvalid's score: 0.799695\n",
      "[1000]\tvalid's l2: 0.0608132\tvalid's score: 0.80218\n",
      "[1500]\tvalid's l2: 0.0602023\tvalid's score: 0.80298\n",
      "[2000]\tvalid's l2: 0.0600194\tvalid's score: 0.80322\n",
      "Early stopping, best iteration is:\n",
      "[2241]\tvalid's l2: 0.0599807\tvalid's score: 0.803271\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0170556\tvalid's score: 0.884488\n",
      "[1000]\tvalid's l2: 0.0158586\tvalid's score: 0.888154\n",
      "[1500]\tvalid's l2: 0.0154361\tvalid's score: 0.889488\n",
      "Early stopping, best iteration is:\n",
      "[1744]\tvalid's l2: 0.0153891\tvalid's score: 0.889638\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid's l2: 0.023993\tvalid's score: 0.865878\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0361899\tvalid's score: 0.840169\n",
      "[1000]\tvalid's l2: 0.0337439\tvalid's score: 0.844812\n",
      "[1500]\tvalid's l2: 0.0329059\tvalid's score: 0.846453\n",
      "Early stopping, best iteration is:\n",
      "[1475]\tvalid's l2: 0.0328819\tvalid's score: 0.846501\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.017625\tvalid's score: 0.8828\n",
      "[1000]\tvalid's l2: 0.0158373\tvalid's score: 0.888221\n",
      "[1500]\tvalid's l2: 0.0154683\tvalid's score: 0.889386\n",
      "[2000]\tvalid's l2: 0.0153123\tvalid's score: 0.889883\n",
      "Early stopping, best iteration is:\n",
      "[2328]\tvalid's l2: 0.0152674\tvalid's score: 0.890027\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0343757\tvalid's score: 0.843592\n",
      "[1000]\tvalid's l2: 0.0324743\tvalid's score: 0.847309\n",
      "Early stopping, best iteration is:\n",
      "[1024]\tvalid's l2: 0.0324287\tvalid's score: 0.8474\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0572247\tvalid's score: 0.806961\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid's l2: 0.0562216\tvalid's score: 0.808335\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0138379\tvalid's score: 0.894747\n",
      "[1000]\tvalid's l2: 0.0133681\tvalid's score: 0.896362\n",
      "Early stopping, best iteration is:\n",
      "[1222]\tvalid's l2: 0.0132407\tvalid's score: 0.896806\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[216]\tvalid's l2: 0.0116449\tvalid's score: 0.902599\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.033913\tvalid's score: 0.844484\n",
      "[1000]\tvalid's l2: 0.0309767\tvalid's score: 0.850339\n",
      "[1500]\tvalid's l2: 0.0301157\tvalid's score: 0.852124\n",
      "[2000]\tvalid's l2: 0.0299468\tvalid's score: 0.852478\n",
      "Early stopping, best iteration is:\n",
      "[2120]\tvalid's l2: 0.0299228\tvalid's score: 0.852528\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[235]\tvalid's l2: 0.0211961\tvalid's score: 0.872913\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's l2: 0.0545607\tvalid's score: 0.810647\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0378154\tvalid's score: 0.837197\n",
      "[1000]\tvalid's l2: 0.0358758\tvalid's score: 0.840754\n",
      "[1500]\tvalid's l2: 0.0353857\tvalid's score: 0.841672\n",
      "[2000]\tvalid's l2: 0.0351644\tvalid's score: 0.84209\n",
      "Early stopping, best iteration is:\n",
      "[2088]\tvalid's l2: 0.0351076\tvalid's score: 0.842197\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0349258\tvalid's score: 0.842542\n",
      "[1000]\tvalid's l2: 0.0329226\tvalid's score: 0.846421\n",
      "[1500]\tvalid's l2: 0.0319903\tvalid's score: 0.848278\n",
      "[2000]\tvalid's l2: 0.031759\tvalid's score: 0.848745\n",
      "Early stopping, best iteration is:\n",
      "[1983]\tvalid's l2: 0.0317473\tvalid's score: 0.848768\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0212909\tvalid's score: 0.872666\n",
      "[1000]\tvalid's l2: 0.0206687\tvalid's score: 0.874305\n",
      "Early stopping, best iteration is:\n",
      "[1333]\tvalid's l2: 0.0205123\tvalid's score: 0.874722\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0161039\tvalid's score: 0.887389\n",
      "[1000]\tvalid's l2: 0.0156793\tvalid's score: 0.888717\n",
      "Early stopping, best iteration is:\n",
      "[921]\tvalid's l2: 0.0156513\tvalid's score: 0.888806\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.102202\tvalid's score: 0.757754\n",
      "[1000]\tvalid's l2: 0.0921361\tvalid's score: 0.767142\n",
      "[1500]\tvalid's l2: 0.0886271\tvalid's score: 0.770592\n",
      "[2000]\tvalid's l2: 0.0870936\tvalid's score: 0.772131\n",
      "[2500]\tvalid's l2: 0.08616\tvalid's score: 0.773078\n",
      "[3000]\tvalid's l2: 0.0857863\tvalid's score: 0.773459\n",
      "[3500]\tvalid's l2: 0.0856833\tvalid's score: 0.773564\n",
      "[4000]\tvalid's l2: 0.0856526\tvalid's score: 0.773596\n",
      "Early stopping, best iteration is:\n",
      "[3988]\tvalid's l2: 0.0856488\tvalid's score: 0.7736\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's l2: 0.0218196\tvalid's score: 0.871297\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid's l2: 0.0183798\tvalid's score: 0.880613\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[282]\tvalid's l2: 0.0226348\tvalid's score: 0.869226\n",
      "local cv: 0.8571001518429565\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def my_val(preds, train_data):\n",
    "    label = train_data.get_label()\n",
    "    return 'score', 1/(1+np.sqrt(mean_squared_error(preds, label))), True\n",
    "def my_obj(preds, train_data):\n",
    "    labels = train_deata.get_label()\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.08,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "# 同样的条件下，此参数设置达到local cv: 0.8575832021441638\n",
    "\n",
    "\n",
    "test_predicts = []\n",
    "val_preds = []\n",
    "\n",
    "for idx, seed in enumerate([1,2,3,4,5]):\n",
    "    kf = KFold(5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    val_preds.append(np.zeros(train.shape[0]))\n",
    "    for n_fold, (tra_idx, val_idx) in enumerate(kf.split(train)):\n",
    "        tra = train.iloc[tra_idx]\n",
    "        val = train.iloc[val_idx]\n",
    "        tst = test.copy()\n",
    "\n",
    "        predictor = [c for c in tra.columns.tolist() if c not in['y','is_train','I_B','I_C']]\n",
    "\n",
    "        train_set = lightgbm.Dataset(\n",
    "            tra[predictor],\n",
    "            tra['y']\n",
    "        )\n",
    "\n",
    "        validation_set = lightgbm.Dataset(\n",
    "            val[predictor],\n",
    "            val['y']\n",
    "        )\n",
    "\n",
    "        model = lightgbm.train(params, train_set, num_boost_round=5000,\n",
    "                              valid_sets= [validation_set],\n",
    "                              valid_names=['valid'],\n",
    "                              early_stopping_rounds=100,\n",
    "                               feval=my_val,\n",
    "                              verbose_eval=500)\n",
    "\n",
    "        val_preds[idx][val_idx] = model.predict(val[predictor])\n",
    "        test_predicts.append(model.predict(tst[predictor]))\n",
    "\n",
    "print('local cv:',1/(1+np.sqrt(mean_squared_error(train['y'],np.mean(val_preds,axis=0)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict_earlystopping: \n",
      "[0.42516574 1.32970174 2.1882634  ... 9.90467328 9.87744583 9.13143841]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.696583\n",
       "std         3.458596\n",
       "min        -0.137579\n",
       "25%         2.515775\n",
       "50%         5.715023\n",
       "75%         8.888176\n",
       "max        12.085619\n",
       "Name: y_predict_earlystopping, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# local cv: 0.857100\n",
    "y_predict_earlystopping = np.mean(test_predicts, axis=0)\n",
    "print('y_predict_earlystopping: \\n'+str(y_predict_earlystopping))\n",
    "\n",
    "ans['y_predict_earlystopping']=y_predict_earlystopping\n",
    "ans['y_predict_earlystopping'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2.使用LightGBM/XGBoost/RF/GBM多种模型训练后融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基学习器\n",
    "\n",
    "xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=321, n_jobs=8)\n",
    "xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=456, n_jobs=8)\n",
    "xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=789, n_jobs=8)\n",
    "# n_estimators=1000  max_depth=5  'sqrt'  GradientBoostingRegressor 最佳参数 ,learning_rate=0.08\n",
    "gbdt1 = GradientBoostingRegressor(n_estimators=800, max_depth=4, max_features='log2', random_state=123,learning_rate=0.08)\n",
    "gbdt2 = GradientBoostingRegressor(n_estimators=900, max_depth=4, max_features='log2', random_state=456,learning_rate=0.08)\n",
    "gbdt3 = GradientBoostingRegressor(n_estimators=1000, max_depth=5, max_features='log2', random_state=789,learning_rate=0.08)\n",
    "# n_estimators=700, max_features='auto', random_state=2, n_jobs=8,max_depth=10\n",
    "forest1 = RandomForestRegressor(n_estimators=800, max_features='sqrt', random_state=7, n_jobs=8)\n",
    "forest2 = RandomForestRegressor(n_estimators=900, max_features='log2', random_state=9, n_jobs=8)\n",
    "forest3 = RandomForestRegressor(n_estimators=900, max_features='sqrt', random_state=11, n_jobs=8) \n",
    "\n",
    "lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) \n",
    "lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=7, n_jobs=8)\n",
    "lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=9, n_jobs=8)\n",
    "\n",
    "regrs = [\n",
    "    xgbt1, gbdt1, forest1, lgb1,\n",
    "    xgbt2, gbdt2, forest2, lgb2,\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    # Train_X: 原始训练集输入矩阵, Train_Y: 原始训练集输出矩阵, Test_X: 原始测试集输入矩阵\n",
    "    def fit_predict(self, Train_X, Train_Y, Test_X):\n",
    "        Train_X = np.array(Train_X)\n",
    "        Train_Y = np.array(Train_Y)\n",
    "        Test_X = np.array(Test_X)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=False, random_state=2018).split(Train_X, Train_Y))       \n",
    "        \n",
    "        # 以基学习器预测结果为特征的 stacker训练数据 与 stacker预测数据\n",
    "        # 原始训练集预测结果容器\n",
    "        S_train = np.zeros((Train_X.shape[0], len(self.base_models)))\n",
    "        # 原始测试集预测结果容器\n",
    "        S_predict = np.zeros((Test_X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for n_model, regr in enumerate(self.base_models):\n",
    "            print(n_model + 1, 'Base model:', str(regr).split('(')[0])\n",
    "            S_predict_i = np.zeros((Test_X.shape[0], self.n_splits))\n",
    "            \n",
    "            for n_fold, (train_idx, test_idx) in enumerate(folds):\n",
    "                # 将X分为训练集与测试集\n",
    "                X_train_fold, Y_train_fold, X_test_fold, Y_test_fold = Train_X[train_idx], Train_Y[train_idx], Train_X[test_idx], Train_Y[test_idx]\n",
    "                print ('Fit fold', (n_fold+1), '...')\n",
    "                regr.fit(X_train_fold, Y_train_fold)\n",
    "                Y_pred = regr.predict(X_test_fold)\n",
    "                # 每折训练得到的模型根据原始训练集中的测试折的输入矩阵预测\n",
    "                S_train[test_idx, n_model] = Y_pred\n",
    "                # 每折训练得到的模型根据原始测试集输入矩阵预测\n",
    "                S_predict_i[:, n_fold] = regr.predict(Test_X)\n",
    "            \n",
    "            S_predict[:, n_model] = S_predict_i.mean(axis=1)\n",
    "\n",
    "        nmse_score = cross_val_score(self.stacker, S_train, Train_Y, cv=5, scoring='neg_mean_squared_error')\n",
    "        print('CV MSE:', -nmse_score)\n",
    "        print('Stacker AVG MSE:', -nmse_score.mean(), 'Stacker AVG Score:', np.mean(np.divide(1, 1 + np.sqrt(-nmse_score))))\n",
    "\n",
    "        self.stacker.fit(S_train, Train_Y)\n",
    "        res = self.stacker.predict(S_predict)\n",
    "        return res, S_train, S_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "2 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "3 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "4 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "5 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "6 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "7 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "8 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "9 Base model: XGBRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "10 Base model: GradientBoostingRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "11 Base model: RandomForestRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "12 Base model: LGBMRegressor\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "CV MSE: [0.56507773 0.02244865 0.02713265 0.02094513 0.07759294]\n",
      "Stacker AVG MSE: 0.1426394184025874 Stacker AVG Score: 0.7909689115097418\n"
     ]
    }
   ],
   "source": [
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local CV: 0.7909689115097466\n",
    "\n",
    "ans['y_n_shuffle'] = pred_stack\n",
    "ans['y_n_shuffle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        5.695812\n",
       "std         3.459761\n",
       "min        -0.131349\n",
       "25%         2.505766\n",
       "50%         5.704603\n",
       "75%         8.887224\n",
       "max        12.348426\n",
       "Name: y_shuffle, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#local CV: 0.8697213790417854\n",
    "\n",
    "ans['y_shuffle'] = pred_stack\n",
    "ans['y_shuffle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
