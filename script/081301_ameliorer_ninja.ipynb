{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mask_two = [-i for i in range(1,2)]+[i for i in range(1,2)]\n",
    "rolling_mask_four = [-i for i in range(1,3)]+[i for i in range(1,3)]\n",
    "rolling_mask_six = [-i for i in range(1,4)]+[i for i in range(1,4)]\n",
    "rolling_mask_eight = [-i for i in range(1,5)]+[i for i in range(1,5)]\n",
    "rolling_mask_ten = [-i for i in range(1,6)]+[i for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "# def drop_all_outlier(df):\n",
    "#     df.drop_duplicates(df.columns.drop('ID'), keep='first', inplace=True)\n",
    "#     df.drop(df[(df.V_A > 800) | (df.V_A < 500)].index,inplace=True)\n",
    "#     df.drop(df[(df.V_B > 800) | (df.V_B < 500)].index,inplace=True)\n",
    "#     df.drop(df[(df.V_C > 800) | (df.V_C < 500)].index,inplace=True)\n",
    "#     df.drop(df[(df.env_t > 30) | (df.env_t < -30)].index,inplace=True)\n",
    "# #     df.drop(df[(df.转换效率A > 100)].index,inplace=True)\n",
    "# #     df.drop(df[(df.转换效率B > 100)].index,inplace=True)\n",
    "# #     df.drop(df[(df.转换效率C > 100)].index,inplace=True)\n",
    "# #     df.drop(df[(df.wind_direction > 360)].index,inplace=True)\n",
    "#     df.drop(df[(df.wind_speed > 20)].index,inplace=True)\n",
    "#     return df\n",
    "\n",
    "# 生成数据\n",
    "def generate_train_data(train_data, test_data, poly=False, select=False):\n",
    "    y = train_data['y']\n",
    "    X = train_data.drop(['y','ID','is_train'], axis=1)\n",
    "    sub_data = test_data.drop(['y','ID','is_train'], axis=1)\n",
    "    \n",
    "    polynm = None\n",
    "    if poly:\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        polynm = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "        X = polynm.fit_transform(X)\n",
    "        sub_data = polynm.transform(sub_data)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    sm = None\n",
    "    if select:\n",
    "        from sklearn.feature_selection import SelectFromModel\n",
    "        sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "        X_train = sm.fit_transform(X_train, y_train)\n",
    "        X_test = sm.transform(X_test)\n",
    "        sub_data = sm.transform(sub_data)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test, sub_data, sm, polynm\n",
    "\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "#  定义交叉验证函数  \n",
    "def cross_validation_test(models, train_X_data, train_y_data, cv=5):\n",
    "    model_name, mse_avg, score_avg = [], [], []\n",
    "    for i, model in enumerate(models):\n",
    "        print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        nmse = cross_val_score(model, train_X_data[i], train_y_data[i], cv=cv, scoring='neg_mean_squared_error')\n",
    "        avg_mse = np.average(-nmse)\n",
    "        scores = cal_score(-nmse)\n",
    "        avg_score = np.average(scores)\n",
    "        mse_avg.append(avg_mse)\n",
    "        score_avg.append(avg_score)\n",
    "        print('MSE:', -nmse)\n",
    "        print('Score:', scores)\n",
    "        print('Average XGB - MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "    res['Avg MSE'] = mse_avg\n",
    "    res['Avg Score'] = score_avg\n",
    "    return res\n",
    "\n",
    "# def add_newid(df):\n",
    "#     ID = df[\"ID\"]\n",
    "#     df[\"new_id\"]=(np.mod(ID,205))\n",
    "#     return df\n",
    "# def add_avg(df):\n",
    "#     array = np.array(df[\"P_avg\"])\n",
    "#     newarray=[]\n",
    "#     num = 0\n",
    "#     for i in np.arange(len(array)):\n",
    "#         for j in np.arange(10):\n",
    "#             if i<10:\n",
    "#                 num = (array[j-1]+array[j-2]+array[j-3])/3\n",
    "#             if i>=10:\n",
    "#                 num = (array[i-1]+array[i-2]+array[i-3]+array[i-5]+array[i-6]+array[i-7]+array[i-8]+array[i-9])/9\n",
    "#         newarray.append(num)\n",
    "#     df[\"old_SoCalledSF_P_avg\"] = newarray\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/public_raw.train.csv')\n",
    "test = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "train['is_train']=1\n",
    "test['is_train']=0\n",
    "\n",
    "df = pd.concat([train, test],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# train_data.rename(index=str, columns=rep_cols, inplace=True)\n",
    "# test_data.rename(index=str, columns=rep_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗原因一：人工发现训练集和测试集合中均有很多样本，测量值均为零，发电量为0.379993053。\n",
    "#于是有两个方向，方向一：将此类样本是为异常样本，认为不存在学习价值，使用最粗暴的方式，从训练集中剔除这些样本。在测试集中人工赋值0.379993053。\n",
    "#方向二：认为这些样本不是异常样本，存在学习价值。同时将测试集合中此类样本增加到训练样本中，让分布更逼近真实分布。\n",
    "\n",
    "#首先尝试方向一：\n",
    "# special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗原因二：电压、电流、温度、风速这些连续测量值中存在明显的异常值，违背物理常识\n",
    "#如何定义异常？\n",
    "#情况一：在一个合理的时间段内，一个时刻的测量值与其他时刻测量值的均值的差异很大，差异如何刻画？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-bb2a10d1fd26>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-bb2a10d1fd26>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    bad_index1 = all_data[bad _feature][\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#原始路线\n",
    "all_data = df.copy()\n",
    "bad_feature = ['ID','P_A', 'P_B', 'P_C', 'P_avg', 'env_t', 'V_A', 'V_B', 'V_C', 'I_B', 'I_C', 'efficiency', 'efficiency_A', 'efficiency_B', 'efficiency_C']\n",
    "bad_index1 = all_data[bad _feature][\n",
    "    (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "    (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "].dropna(how='all').index\n",
    "bad_index2 = all_data[\n",
    "    ((all_data['V_A']<500)&(all_data['V_A']!=0))|\n",
    "    ((all_data['V_B']<500)&(all_data['V_B']!=0))|\n",
    "    ((all_data['V_C']<500)&(all_data['V_C']!=0))].index\n",
    "bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "# bad_index = all_data[bad_feature][\n",
    "#     (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "#     (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "# ].dropna(how='all').index\n",
    "\n",
    "\n",
    "\n",
    "bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 上下记录均值替代异常值\n",
    "for idx, line in bad_data.iterrows():\n",
    "    ID = line['ID']\n",
    "    col_index = line[bad_feature][ \n",
    "        (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "        (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "    ].index\n",
    "    index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "    before_offset = 1\n",
    "    while (idx + before_offset)in bad_index:\n",
    "        before_offset += 1\n",
    "\n",
    "    after_offset = 1\n",
    "    while (idx + after_offset) in bad_index:\n",
    "        after_offset += 1\n",
    "    \n",
    "    replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / 2\n",
    "    all_data.loc[index, col_index] = replace_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前二后二\n",
    "next_one = []\n",
    "prev_one = []\n",
    "next_id = []\n",
    "prev_id = []\n",
    "\n",
    "second_next_one = []\n",
    "second_prev_one = []\n",
    "\n",
    "df_len = df.shape[0]\n",
    "\n",
    "i_y =df.columns.get_loc(\"y\")\n",
    "\n",
    "def get_prev_nn_index(cur_i):\n",
    "    prev_i = cur_i-1\n",
    "    while(prev_i>=0 and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i-=1\n",
    "    return prev_i\n",
    "\n",
    "def get_next_nn_index(cur_i):\n",
    "    prev_i = cur_i+1\n",
    "    while(prev_i<df_len and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i+=1\n",
    "    return prev_i\n",
    "\n",
    "for i in range(df_len):\n",
    "    f_pre_i=get_prev_nn_index(i)\n",
    "    if(f_pre_i)<0:\n",
    "        prev_one.append(np.nan)\n",
    "        prev_id.append(0)\n",
    "    else:\n",
    "        prev_one.append(df.iat[f_pre_i,i_y])\n",
    "        prev_id.append(f_pre_i)\n",
    "        \n",
    "    s_pre_i=get_prev_nn_index(f_pre_i)\n",
    "    if (s_pre_i)<0:\n",
    "        second_prev_one.append(np.nan)\n",
    "    else:\n",
    "        second_prev_one.append(df.iat[s_pre_i,i_y])\n",
    "    \n",
    "    f_next_i=get_next_nn_index(i)\n",
    "    if(f_next_i<df_len):\n",
    "        next_one.append(df.iat[f_next_i,i_y])\n",
    "        next_id.append(f_next_i)\n",
    "    else:\n",
    "        next_one.append(np.nan)\n",
    "        next_id.append(df_len)\n",
    "    \n",
    "    s_next_i=get_next_nn_index(f_next_i)\n",
    "    if(s_next_i<df_len):\n",
    "        second_next_one.append(df.iat[s_next_i,i_y])\n",
    "    else:\n",
    "        second_next_one.append(np.nan)\n",
    "        \n",
    "\n",
    "df['next_value'] = next_one\n",
    "df['prev_value'] = prev_one\n",
    "df['avg_value'] = np.nanmean([df['next_value'], df['prev_value']],axis=0)\n",
    "\n",
    "df.drop(['next_value','prev_value'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#拆分数据\n",
    "\n",
    "train_data = all_data[all_data['is_train']==1]\n",
    "test_data = all_data[all_data['is_train']==0]\n",
    "len(train_data), len(test_data)\n",
    "\n",
    "train_data = train_data.drop_duplicates(train_data.columns.drop('ID'), keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['ID'] = list(test_data['ID'])\n",
    "special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=True)\n",
    "# clean_X_train, clean_X_test, clean_y_train, clean_y_test, clean_sub_data, sm1, polynm1 = generate_train_data(cleaned_train_data, cleaned_test_data, poly=True, select=True)\n",
    "\n",
    "all_X_train = np.concatenate([X_train, X_test])\n",
    "all_y_train = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试验休了，时长为前后各4个时刻，偏差率为1.6，以此作为初值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算偏差率的辅助列\n",
    "for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "    df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "    df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标记包含异常值的记录\n",
    "df['is_abnormal']=0\n",
    "\n",
    "for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "    df.loc[df[c+'_exception_ratio'] > 10 , 'is_abnormal'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "    \n",
    "#     df.loc[df[c+'_exception_ratio'] > 1.6 , 'is_abnormal'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>board_t</th>\n",
       "      <th>light_strength</th>\n",
       "      <th>I_A</th>\n",
       "      <th>I_A_exception_ratio</th>\n",
       "      <th>I_B</th>\n",
       "      <th>I_C</th>\n",
       "      <th>V_A</th>\n",
       "      <th>V_B</th>\n",
       "      <th>V_C</th>\n",
       "      <th>P_avg</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22</td>\n",
       "      <td>-10.77</td>\n",
       "      <td>100</td>\n",
       "      <td>7.19</td>\n",
       "      <td>1.520596</td>\n",
       "      <td>645.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>65382</td>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>158302.85</td>\n",
       "      <td>5.440741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1173</td>\n",
       "      <td>-5.83</td>\n",
       "      <td>233</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.277967</td>\n",
       "      <td>649.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>65408</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>158484.77</td>\n",
       "      <td>7.753474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1286</td>\n",
       "      <td>0.44</td>\n",
       "      <td>100</td>\n",
       "      <td>7.18</td>\n",
       "      <td>1.397329</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.18</td>\n",
       "      <td>310</td>\n",
       "      <td>307</td>\n",
       "      <td>65438</td>\n",
       "      <td>158089.59</td>\n",
       "      <td>4.694385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1362</td>\n",
       "      <td>-3.70</td>\n",
       "      <td>233</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.316718</td>\n",
       "      <td>652.02</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65386</td>\n",
       "      <td>23</td>\n",
       "      <td>244</td>\n",
       "      <td>157793.07</td>\n",
       "      <td>7.806384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>1451</td>\n",
       "      <td>2.56</td>\n",
       "      <td>262</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.258220</td>\n",
       "      <td>6.82</td>\n",
       "      <td>3.12</td>\n",
       "      <td>560</td>\n",
       "      <td>65454</td>\n",
       "      <td>27</td>\n",
       "      <td>150103.64</td>\n",
       "      <td>7.996870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>1519</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9</td>\n",
       "      <td>7.04</td>\n",
       "      <td>6.673025</td>\n",
       "      <td>7.10</td>\n",
       "      <td>639.04</td>\n",
       "      <td>77</td>\n",
       "      <td>65387</td>\n",
       "      <td>14</td>\n",
       "      <td>157912.11</td>\n",
       "      <td>0.839478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1666</td>\n",
       "      <td>0.00</td>\n",
       "      <td>112</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.506156</td>\n",
       "      <td>6.80</td>\n",
       "      <td>6.81</td>\n",
       "      <td>293</td>\n",
       "      <td>255</td>\n",
       "      <td>65460</td>\n",
       "      <td>149843.17</td>\n",
       "      <td>4.521973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>1894</td>\n",
       "      <td>0.24</td>\n",
       "      <td>30</td>\n",
       "      <td>7.10</td>\n",
       "      <td>8.726027</td>\n",
       "      <td>7.13</td>\n",
       "      <td>635.60</td>\n",
       "      <td>74</td>\n",
       "      <td>65350</td>\n",
       "      <td>22</td>\n",
       "      <td>160151.37</td>\n",
       "      <td>0.780917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>2414</td>\n",
       "      <td>0.20</td>\n",
       "      <td>115</td>\n",
       "      <td>7.02</td>\n",
       "      <td>1.529730</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.99</td>\n",
       "      <td>292</td>\n",
       "      <td>296</td>\n",
       "      <td>65470</td>\n",
       "      <td>153919.05</td>\n",
       "      <td>4.299538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>2797</td>\n",
       "      <td>0.92</td>\n",
       "      <td>94</td>\n",
       "      <td>6.86</td>\n",
       "      <td>1.380911</td>\n",
       "      <td>6.86</td>\n",
       "      <td>3.60</td>\n",
       "      <td>305</td>\n",
       "      <td>65512</td>\n",
       "      <td>17</td>\n",
       "      <td>150521.94</td>\n",
       "      <td>4.221675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>2875</td>\n",
       "      <td>2.65</td>\n",
       "      <td>260</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.257537</td>\n",
       "      <td>6.82</td>\n",
       "      <td>1.99</td>\n",
       "      <td>555</td>\n",
       "      <td>65455</td>\n",
       "      <td>23</td>\n",
       "      <td>150079.84</td>\n",
       "      <td>7.794517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>2986</td>\n",
       "      <td>1.56</td>\n",
       "      <td>101</td>\n",
       "      <td>6.89</td>\n",
       "      <td>1.456328</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.21</td>\n",
       "      <td>65515</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>150472.35</td>\n",
       "      <td>4.196051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>3540</td>\n",
       "      <td>0.33</td>\n",
       "      <td>36</td>\n",
       "      <td>6.50</td>\n",
       "      <td>4.231388</td>\n",
       "      <td>6.56</td>\n",
       "      <td>647.62</td>\n",
       "      <td>36</td>\n",
       "      <td>65463</td>\n",
       "      <td>18</td>\n",
       "      <td>147109.48</td>\n",
       "      <td>0.209395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>3723</td>\n",
       "      <td>0.12</td>\n",
       "      <td>91</td>\n",
       "      <td>7.21</td>\n",
       "      <td>20.051095</td>\n",
       "      <td>7.06</td>\n",
       "      <td>7.08</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>65440</td>\n",
       "      <td>154630.97</td>\n",
       "      <td>0.460215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5083</th>\n",
       "      <td>5521</td>\n",
       "      <td>0.31</td>\n",
       "      <td>25</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.659649</td>\n",
       "      <td>6.72</td>\n",
       "      <td>641.85</td>\n",
       "      <td>36</td>\n",
       "      <td>65410</td>\n",
       "      <td>0</td>\n",
       "      <td>146538.32</td>\n",
       "      <td>0.426272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  board_t  light_strength   I_A  I_A_exception_ratio     I_B  \\\n",
       "14      22   -10.77             100  7.19             1.520596  645.39   \n",
       "1065  1173    -5.83             233  7.04             0.277967  649.75   \n",
       "1178  1286     0.44             100  7.18             1.397329    7.16   \n",
       "1237  1362    -3.70             233  7.00             0.316718  652.02   \n",
       "1326  1451     2.56             262  6.84             0.258220    6.82   \n",
       "1373  1519     0.25               9  7.04             6.673025    7.10   \n",
       "1520  1666     0.00             112  6.87             1.506156    6.80   \n",
       "1722  1894     0.24              30  7.10             8.726027    7.13   \n",
       "2214  2414     0.20             115  7.02             1.529730    7.00   \n",
       "2567  2797     0.92              94  6.86             1.380911    6.86   \n",
       "2629  2875     2.65             260  6.83             0.257537    6.82   \n",
       "2740  2986     1.56             101  6.89             1.456328    1.66   \n",
       "3255  3540     0.33              36  6.50             4.231388    6.56   \n",
       "3429  3723     0.12              91  7.21            20.051095    7.06   \n",
       "5083  5521     0.31              25  1.66             3.659649    6.72   \n",
       "\n",
       "         I_C    V_A    V_B    V_C      P_avg         y  \n",
       "14      2.75  65382      7    107  158302.85  5.440741  \n",
       "1065    2.75  65408     22    250  158484.77  7.753474  \n",
       "1178    7.18    310    307  65438  158089.59  4.694385  \n",
       "1237    2.79  65386     23    244  157793.07  7.806384  \n",
       "1326    3.12    560  65454     27  150103.64  7.996870  \n",
       "1373  639.04     77  65387     14  157912.11  0.839478  \n",
       "1520    6.81    293    255  65460  149843.17  4.521973  \n",
       "1722  635.60     74  65350     22  160151.37  0.780917  \n",
       "2214    6.99    292    296  65470  153919.05  4.299538  \n",
       "2567    3.60    305  65512     17  150521.94  4.221675  \n",
       "2629    1.99    555  65455     23  150079.84  7.794517  \n",
       "2740    0.21  65515      0     89  150472.35  4.196051  \n",
       "3255  647.62     36  65463     18  147109.48  0.209395  \n",
       "3429    7.08     39     42  65440  154630.97  0.460215  \n",
       "5083  641.85     36  65410      0  146538.32  0.426272  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练集中异常记录30条\n",
    "df[df['is_train']==1][df['is_abnormal']==1][['ID','board_t','light_strength','I_A','I_A_exception_ratio','I_B','I_C','V_A','V_B','V_C','P_avg','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集中异常记录34条\n",
    "df[df['is_train']==0][df['is_abnormal']==1][['ID','board_t','light_strength','I_A','I_B','I_C','V_A','V_B','V_C','P_avg','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#电学测量值全为0的测试集记录,ID存在special_missing_ID\n",
    "special_missing_ID=df[df[(df == 0) | (df == 0.)].count(axis=1) > 13][df['is_train']==0]['ID']\n",
    "special_missing_ID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#异常值由滚动平均值替代（前后各4个点的平均值）\n",
    "static=1\n",
    "for idx, line in df.iterrows():\n",
    "    for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "        if line[c+'_exception_ratio']>1.6:\n",
    "            print(str(line[c]) + ' is abnormal as value of ' + c)\n",
    "            print('Mark for abnormal records: ' + str(line['is_abnormal']))\n",
    "            line.loc[c] = line[c+'_avg_sequence']\n",
    "            print('Has been replaced by '+str(line[c+'_avg_sequence'])) \n",
    "            static += 1\n",
    "            print(static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除辅助行\n",
    "def drop_cols(df):\n",
    "    col_list=[] \n",
    "    for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "        col_list.append(c+'_exception_ratio')\n",
    "        col_list.append(c+'_avg_sequence')\n",
    "    df.drop(col_list,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_cols(df)\n",
    "df.drop('is_abnormal',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(df.columns.drop('ID'), keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#二次项特征\n",
    "\n",
    "# 生成数据\n",
    "def generate_train_data(train_data, test_data, poly=False, select=False):\n",
    "    y = train_data['y']\n",
    "    X = train_data.drop(['y','ID','is_train'], axis=1)\n",
    "    sub_data = test_data.drop(['y','ID','is_train'], axis=1)\n",
    "    \n",
    "    polynm = None\n",
    "    if poly:\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        polynm = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "        X = polynm.fit_transform(X)\n",
    "        sub_data = polynm.transform(sub_data)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    sm = None\n",
    "    if select:\n",
    "        from sklearn.feature_selection import SelectFromModel\n",
    "        sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "        X_train = sm.fit_transform(X_train, y_train)\n",
    "        X_test = sm.transform(X_test)\n",
    "        sub_data = sm.transform(sub_data)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test, sub_data, sm, polynm\n",
    "\n",
    "#train和test拆分\n",
    "train_data = df[df['is_train']==1]\n",
    "test_data = df[df['is_train']==0]\n",
    "\n",
    "# cleaned_train_data = train_data[train_data['is_abnormal']==0]\n",
    "# cleaned_test_data = test_data[test_data['is_abnormal']==0]\n",
    "\n",
    "\n",
    "# train_data = add_avg(train_data)\n",
    "# test_data = add_avg(test_data)\n",
    "# cleaned_train_data = add_avg(cleaned_train_data)\n",
    "# cleaned_test_data = add_avg(cleaned_test_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=True)\n",
    "# clean_X_train, clean_X_test, clean_y_train, clean_y_test, clean_sub_data, sm1, polynm1 = generate_train_data(cleaned_train_data, cleaned_test_data, poly=True, select=True)\n",
    "\n",
    "# clean_X = np.concatenate([clean_X_train, clean_X_test])\n",
    "# clean_y = np.concatenate([clean_y_train, clean_y_test])\n",
    "\n",
    "all_X_train = np.concatenate([X_train, X_test])\n",
    "all_y_train = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_all_outlier(df):\n",
    "#     for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "#         df.drop(df[(df[c+'_exception_ratio'] > 1.6)].index,inplace=True)\n",
    "#     df.drop(df[(df.env_t > 30) | (df.env_t < -30)].index,inplace=True)\n",
    "#     df.drop(df[(df.wind_speed > 20)].index,inplace=True)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train和test拆分\n",
    "# train_data = df[df['is_train']==1]\n",
    "# test_data = df[df['is_train']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #准备测试结果\n",
    "# df_result = pd.DataFrame()\n",
    "# df_result['ID'] = list(test_data['ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train_data = train_data.copy()\n",
    "# cleaned_train_data = drop_all_outlier(cleaned_train_data)\n",
    "\n",
    "# cleaned_sub_data = test_data.copy()\n",
    "# cleaned_sub_data = drop_all_outlier(cleaned_sub_data)\n",
    "# cleaned_sub_data_ID = cleaned_sub_data['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data  = pd.concat([train_data, test_data], axis=0).sort_values(by='ID').reset_index().drop(['index'], axis=1)\n",
    "bad_feature = ['ID','P_A', 'P_B', 'P_C', 'P_avg', 'env_t', 'V_A', 'V_B', 'V_C', 'I_B', 'I_C', 'efficiency', 'efficiency_A', 'efficiency_B', 'efficiency_C']\n",
    "bad_index1 = all_data[bad_feature][\n",
    "    (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "    (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "].dropna(how='all').index\n",
    "bad_index2 = all_data[\n",
    "    ((all_data['V_A']<500)&(all_data['V_A']!=0))|\n",
    "    ((all_data['V_B']<500)&(all_data['V_B']!=0))|\n",
    "    ((all_data['V_C']<500)&(all_data['V_C']!=0))].index\n",
    "bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "# all_data.loc[np.concatenate([bad_index -1,bad_index,bad_index+1])].sort_values(by='ID', ascending=True)\n",
    "\n",
    "\n",
    "nn_bad_data = all_data.loc[np.concatenate([bad_index - 1, bad_index, bad_index + 1])].sort_values(by='ID', ascending=True).drop_duplicates()\n",
    "bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上下记录均值替代异常值\n",
    "for idx, line in bad_data.iterrows():\n",
    "    ID = line['ID']\n",
    "    col_index = line[bad_feature][ \n",
    "        (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "        (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "    ].index\n",
    "    index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "    before_offset = 1\n",
    "    while (idx + before_offset)in bad_index:\n",
    "        before_offset += 1\n",
    "\n",
    "    after_offset = 1\n",
    "    while (idx + after_offset) in bad_index:\n",
    "        after_offset += 1\n",
    "    \n",
    "    replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / 2\n",
    "    all_data.loc[index, col_index] = replace_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #异常值被前后各4个点的平均值替代\n",
    "# for idx, line in df.iterrows():\n",
    "#     for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "#         if line[c+'_exception_ratio']>1.6:\n",
    "#             line.loc[c] = line[c+'_avg_sequence']\n",
    "#             print(str(line[c]) + 'is replaced by '+str(line[c+'_avg_sequence'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #拆分数据\n",
    "# train_data = df[df['is_train']==1].reset_index().drop(['index'], axis=1)\n",
    "# test_data = df[df['is_train']==0].drop(['y'], axis=1).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #拆分数据\n",
    "# train_data = all_data.drop(all_data[all_data['ID'].isin(df_result['ID'])].index).reset_index().drop(['index'], axis=1)\n",
    "# test_data = all_data[all_data['ID'].isin(df_result['ID'])].drop(['y'], axis=1).reset_index().drop(['index'], axis=1)\n",
    "# len(train_data), len(test_data)\n",
    "# # 去除重复值\n",
    "# train_data = train_data.drop_duplicates(train_data.columns.drop('ID'), keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = add_avg(train_data)\n",
    "# test_data = add_avg(test_data)\n",
    "# cleaned_train_data = add_avg(cleaned_train_data)\n",
    "# cleaned_sub_data = add_avg(cleaned_sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_sub_data = cleaned_sub_data.drop(['y'], axis=1).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_cols(df):\n",
    "#     col_list=[] \n",
    "#     for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "#         col_list.append(c+'_exception_ratio')\n",
    "#         col_list.append(c+'_avg_sequence')\n",
    "#     df.drop(col_list,axis=1,inplace=True)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=drop_cols(train_data)\n",
    "# test_data = drop_cols(test_data)\n",
    "# cleaned_train_data = drop_cols(cleaned_train_data)\n",
    "# cleaned_sub_data = drop_cols(cleaned_sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=True)\n",
    "\n",
    "# clean_X_train, clean_X_test, clean_y_train, clean_y_test, clean_sub_data, _, _ = generate_train_data(cleaned_train_data, cleaned_sub_data, poly=False, select=False)\n",
    "\n",
    "# clean_X = np.concatenate([clean_X_train, clean_X_test])\n",
    "# clean_y = np.concatenate([clean_y_train, clean_y_test])\n",
    "# clean_X = polynm.transform(clean_X)\n",
    "# clean_X = sm.transform(clean_X)\n",
    "\n",
    "# clean_sub_data = polynm.transform(clean_sub_data)\n",
    "# clean_sub_data = sm.transform(clean_sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=321, n_jobs=8)\n",
    "# xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=456, n_jobs=8)\n",
    "xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=789, n_jobs=8)\n",
    "# n_estimators=1000  max_depth=5  'sqrt'  GradientBoostingRegressor 最佳参数 ,learning_rate=0.08\n",
    "# gbdt1 = GradientBoostingRegressor(n_estimators=800, max_depth=4, max_features='log2', random_state=123,learning_rate=0.08)\n",
    "# gbdt2 = GradientBoostingRegressor(n_estimators=900, max_depth=4, max_features='log2', random_state=456,learning_rate=0.08)\n",
    "gbdt3 = GradientBoostingRegressor(n_estimators=1000, max_depth=5, max_features='log2', random_state=789,learning_rate=0.08)\n",
    "# n_estimators=700, max_features='auto', random_state=2, n_jobs=8,max_depth=10\n",
    "# forest1 = RandomForestRegressor(n_estimators=800, max_features='sqrt', random_state=7, n_jobs=8)\n",
    "# forest2 = RandomForestRegressor(n_estimators=900, max_features='log2', random_state=9, n_jobs=8)\n",
    "forest3 = RandomForestRegressor(n_estimators=900, max_features='sqrt', random_state=11, n_jobs=8) \n",
    "\n",
    "# lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) \n",
    "# lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=7, n_jobs=8)\n",
    "lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=9, n_jobs=8)\n",
    "\n",
    "# xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=2, n_jobs=8)\n",
    "# xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=3, n_jobs=8)\n",
    "# xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=4, n_jobs=8)\n",
    "\n",
    "# gbdt1 = GradientBoostingRegressor(n_estimators=500, max_depth=3, max_features='sqrt', random_state=2)\n",
    "# gbdt2 = GradientBoostingRegressor(n_estimators=400, max_depth=3, max_features='sqrt', random_state=3)\n",
    "# gbdt3 = GradientBoostingRegressor(n_estimators=500, max_depth=4, max_features='log2', random_state=4)\n",
    "\n",
    "# forest1 = RandomForestRegressor(n_estimators=300, max_features='sqrt', random_state=2, n_jobs=8)\n",
    "# forest2 = RandomForestRegressor(n_estimators=300, max_features='log2', random_state=3, n_jobs=8)\n",
    "# forest3 = RandomForestRegressor(n_estimators=600, max_features='sqrt', random_state=4, n_jobs=8) \n",
    "\n",
    "# lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=2, n_jobs=8) \n",
    "# lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=3, n_jobs=8)\n",
    "# lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=4, n_jobs=8)\n",
    "\n",
    "# cross_validation_test(\n",
    "#     models=[    \n",
    "#         xgbt1, xgbt2, xgbt3,\n",
    "#         gbdt1, gbdt2, gbdt3,\n",
    "#         forest1, forest2, forest3,\n",
    "#         lgb1, lgb2, lgb3\n",
    "#     ],\n",
    "#     train_X_data=[\n",
    "#         all_X_train, all_X_train, all_X_train, all_X_train,\n",
    "#         all_X_train, all_X_train, all_X_train, all_X_train,\n",
    "#         all_X_train, all_X_train, all_X_train, all_X_train\n",
    "#     ],\n",
    "#     train_y_data=[\n",
    "#         all_y_train, all_y_train, all_y_train, all_y_train,\n",
    "#         all_y_train, all_y_train, all_y_train, all_y_train,\n",
    "#         all_y_train, all_y_train, all_y_train, all_y_train\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = [\n",
    "    xgbt1, gbdt1, forest1, lgb1,\n",
    "    xgbt2, gbdt2, forest2, lgb2,\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = [\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    # X: 原始训练集, y: 原始训练集真实值, predict_data: 原始待预测数据\n",
    "    def fit_predict(self, X, y, predict_data):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(predict_data)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=False, random_state=2018).split(X, y))\n",
    "        \n",
    "        # 以基学习器预测结果为特征的 stacker的训练数据 与 stacker预测数据\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_predict = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, regr in enumerate(self.base_models):\n",
    "            print(i + 1, 'Base model:', str(regr).split('(')[0])\n",
    "            S_predict_i = np.zeros((T.shape[0], self.n_splits))\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                # 将X分为训练集与测试集\n",
    "                X_train, y_train, X_test, y_test = X[train_idx], y[train_idx], X[test_idx], y[test_idx]\n",
    "                print ('Fit fold', (j+1), '...')\n",
    "                regr.fit(X_train, y_train)\n",
    "                y_pred = regr.predict(X_test)                \n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_predict_i[:, j] = regr.predict(T)\n",
    "            \n",
    "            S_predict[:, i] = S_predict_i.mean(axis=1)\n",
    "\n",
    "        nmse_score = cross_val_score(self.stacker, S_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        print('CV MSE:', -nmse_score)\n",
    "        print('Stacker AVG MSE:', -nmse_score.mean(), 'Stacker AVG Score:', np.mean(np.divide(1, 1 + np.sqrt(-nmse_score))))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict(S_predict)\n",
    "        return res, S_train, S_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stacking_mode1 = Ridge(alpha=0.008, copy_X=True, fit_intercept=False, solver='auto', random_state=2)# stacki \n",
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(5, stacking_model, regrs)\n",
    "pred_stack, S_train_data, S_predict_data = stacker.fit_predict(all_X_train, all_y_train, sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result['score'] = pred_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_result[df_result['ID'].isin(special_missing_ID)].index\n",
    "df_result.loc[index, 'score'] = 0.379993053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_index = df_result[df_result['ID'].isin(cleaned_sub_data_ID)].index\n",
    "# df_result.loc[c_index, 'score'] = pred_clean_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../result/081502_08751.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
