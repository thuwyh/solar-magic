{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.问题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.问题类别"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一元回归问题.\n",
    "训练集样本9000条(包括82条重复样本)\n",
    "测试集样本8409条\n",
    "原始特征19个\n",
    "\n",
    "预测量为样本对应的发电量\n",
    "score = 1/(1+RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.特征分析 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "时间戳\n",
    "ID：当前记录条数； \n",
    "\n",
    "测量物理量(连续)\n",
    "板温：光伏电池板背测温度；\n",
    "现场温度：光伏电站现场温度；\n",
    "转换效率：为计算得到的平均转换效率；\n",
    "电压A：为数据采集点A处汇流箱电压值；\n",
    "电压B：为数据采集点B处汇流箱电压值；\n",
    "电压C：为数据采集点C处汇流箱电压值；\n",
    "电流A：为采集点A处汇流箱电流值；\n",
    "电流B：为采集点B处汇流箱电流值；\n",
    "电流C：为采集点C处汇流箱电流值；\n",
    "风速：为光伏电厂现场风速测量值；\n",
    "风向：为光伏电厂现场风的来向；\n",
    "\n",
    "计算物理量\n",
    "转换效率A：数据采集点A处的光伏板转换效率；\n",
    "转换效率B：数据采集点B处的光伏板转换效率；\n",
    "转换效率C：数据采集点C处的光伏板转换效率；\n",
    "功率A：为采集点A处的功率Pa，P=UI；\n",
    "功率B：为采集点B处的功率Pb，P=UI；\n",
    "功率C：为采集点C处的功率Pc，P=UI；\n",
    "平均功率：为A、B、C三点功率的平均值：(Pa+Pb+Pc)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.数据挖掘"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "详见 0803_数据挖掘.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.准备工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.原始数据准备(不进行任何清洗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../data/public_raw.train.csv')\n",
    "test_raw = pd.read_csv('../data/public_raw.test.csv')\n",
    "\n",
    "train_raw['is_train']=1\n",
    "test_raw['is_train']=0\n",
    "\n",
    "df = pd.concat([train_raw, test_raw],sort=False)\n",
    "\n",
    "rep_cols = {'ID':'ID', \n",
    " '板温':'board_t', \n",
    " '现场温度':'env_t', \n",
    " '光照强度':'light_strength', \n",
    " '转换效率':'efficiency', \n",
    " '转换效率A':'efficiency_A', \n",
    " '转换效率B':'efficiency_B', \n",
    " '转换效率C':'efficiency_C', \n",
    " '电压A':'V_A',\n",
    " '电压B':'V_B', \n",
    " '电压C':'V_C', \n",
    " '电流A':'I_A', \n",
    " '电流B':'I_B', \n",
    " '电流C':'I_C', \n",
    " '功率A':'P_A', \n",
    " '功率B':'P_B', \n",
    " '功率C':'P_C', \n",
    " '平均功率':'P_avg', \n",
    " '风速':'wind_speed',\n",
    " '风向':'wind_direction', \n",
    " '发电量':'y'\n",
    "}\n",
    "\n",
    "df.rename(index=str, columns=rep_cols, inplace=True)\n",
    "\n",
    "df.sort_values(by=['ID'],ascending=True, inplace=True)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #原始路线\n",
    "# all_data = df.copy()\n",
    "# bad_feature = ['env_t', 'V_A', 'V_B', 'V_C', 'I_B', 'I_C', 'efficiency', 'efficiency_A', 'efficiency_B', 'efficiency_C']\n",
    "# bad_index1 = all_data[bad_feature][\n",
    "#     (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "#     (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "# ].dropna(how='all').index\n",
    "# bad_index2 = all_data[\n",
    "#     ((all_data['V_A']<500)&(all_data['V_A']!=0))|\n",
    "#     ((all_data['V_B']<500)&(all_data['V_B']!=0))|\n",
    "#     ((all_data['V_C']<500)&(all_data['V_C']!=0))].index\n",
    "# bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "\n",
    "# # bad_index = all_data[bad_feature][\n",
    "# #     (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "# #     (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "# # ].dropna(how='all').index\n",
    "\n",
    "\n",
    "\n",
    "# bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 上下记录均值替代异常值\n",
    "# for idx, line in bad_data.iterrows():\n",
    "#     ID = line['ID']\n",
    "#     col_index = line[bad_feature][ \n",
    "#         (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "#         (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "#     ].index\n",
    "#     index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "#     # idx - before_offset, CV  0.8684\n",
    "#     before_offset = 1\n",
    "#     while (idx + before_offset)in bad_index:\n",
    "#         before_offset += 1\n",
    "\n",
    "#     after_offset = 1\n",
    "#     while (idx + after_offset) in bad_index:\n",
    "#         after_offset += 1\n",
    "    \n",
    "#     replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / 2\n",
    "#     all_data.loc[index, col_index] = replace_value[0]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mask_two = [-i for i in range(1,2)]+[i for i in range(1,2)]\n",
    "rolling_mask_four = [-i for i in range(1,3)]+[i for i in range(1,3)]\n",
    "rolling_mask_six = [-i for i in range(1,4)]+[i for i in range(1,4)]\n",
    "rolling_mask_eight = [-i for i in range(1,5)]+[i for i in range(1,5)]\n",
    "rolling_mask_ten = [-i for i in range(1,6)]+[i for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "Int64Index([], dtype='int64')\n",
      "Empty DataFrame\n",
      "Columns: [ID, V_A, V_B, V_C, I_A, I_B, I_C, y]\n",
      "Index: []\n",
      "Int64Index([14, 859, 981, 1065, 1237, 1419, 3303, 3305, 3309, 3311, 3313], dtype='int64')\n",
      "Int64Index([3303, 1065, 3305, 1419, 3309, 14, 3311, 3313, 1237, 981, 859], dtype='int64')\n",
      "        ID    V_A  V_B  V_C   I_A     I_B   I_C         y\n",
      "14      22  65382    7  107  7.19  645.39  2.75  5.440741\n",
      "859    948  65394    4   14  6.87  638.94  2.37       NaN\n",
      "981   1070  65477   41  692  2.62  653.71  3.15       NaN\n",
      "1065  1173  65408   22  250  7.04  649.75  2.75  7.753474\n",
      "1237  1362  65386   23  244  7.00  652.02  2.79  7.806384\n",
      "1419  1565  65420    8  260  6.93  653.23  0.83       NaN\n",
      "3303  3597  65419   10   63  6.93  643.45  2.70       NaN\n",
      "3305  3599  65419    5   64  6.97  643.58  3.35  0.268685\n",
      "3309  3603  65420    3   67  7.01  643.75  3.16  0.286363\n",
      "3311  3605  65420   11   67  7.01  643.83  3.36  0.278587\n",
      "3313  3607  65420   15   68  7.09  643.94  0.03  0.286621\n",
      "ID :22.0\n",
      "645.39 is abnormal as value of  I_B\n",
      "Has been replaced by 2.84\n",
      "ID :948.0\n",
      "638.94 is abnormal as value of  I_B\n",
      "Has been replaced by 0.3\n",
      "ID :1070.0\n",
      "653.71 is abnormal as value of  I_B\n",
      "Has been replaced by 5.32\n",
      "ID :1173.0\n",
      "649.75 is abnormal as value of  I_B\n",
      "Has been replaced by 6.275\n",
      "ID :1362.0\n",
      "652.02 is abnormal as value of  I_B\n",
      "Has been replaced by 5.35\n",
      "ID :1565.0\n",
      "653.23 is abnormal as value of  I_B\n",
      "Has been replaced by 5.12\n",
      "ID :3597.0\n",
      "643.45 is abnormal as value of  I_B\n",
      "Has been replaced by 0.335\n",
      "ID :3599.0\n",
      "643.58 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3603.0\n",
      "643.75 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3605.0\n",
      "643.83 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "ID :3607.0\n",
      "643.94 is abnormal as value of  I_B\n",
      "Has been replaced by 0.34\n",
      "Int64Index([  527,   528,   529,   530,  1067,  1373,  1722,  1950,  2036,\n",
      "             2071,  3255,  3316,  3317,  3318,  5083, 15971],\n",
      "           dtype='int64')\n",
      "Int64Index([15971,  1067,   527,   528,   529,   530,  2036,  3316,  3317,\n",
      "             2071,  3318,  3255,  1722,  5083,  1373,  1950],\n",
      "           dtype='int64')\n",
      "          ID  V_A    V_B  V_C   I_A   I_B     I_C         y\n",
      "527      591   36  65402    0  6.68  6.72  640.77       NaN\n",
      "528      592   36  65402    0  1.56  6.72  640.77  0.426489\n",
      "529      593   37  65403    3  6.68  6.72  640.85       NaN\n",
      "530      594   37  65403    3  1.56  6.72  640.85       NaN\n",
      "1067    1175  553  65406   27  7.07  7.07  649.96       NaN\n",
      "1373    1519   77  65387   14  7.04  7.10  639.04  0.839478\n",
      "1722    1894   74  65350   22  7.10  7.13  635.60  0.780917\n",
      "1950    2137  563  65428   30  6.95  6.97  652.04       NaN\n",
      "2036    2223  297  65446   18  7.06  7.06  649.69       NaN\n",
      "2071    2271  114  65353   14  7.22  7.28  639.06       NaN\n",
      "3255    3540   36  65463   18  6.50  6.56  647.62  0.209395\n",
      "3316    3610   36  65423    7  6.94  7.06  644.15       NaN\n",
      "3317    3611  710  65423    7  0.33  1.94  644.15       NaN\n",
      "3318    3612   37  65422    5  7.13  7.05  644.16  0.192079\n",
      "5083    5521   36  65410    0  1.66  6.72  641.85  0.426272\n",
      "15971  16437  599    146   26  7.96  8.47   32.91       NaN\n",
      "ID :591.0\n",
      "640.77 is abnormal as value of  I_C\n",
      "Has been replaced by 3.56\n",
      "ID :592.0\n",
      "640.77 is abnormal as value of  I_C\n",
      "Has been replaced by 3.56\n",
      "ID :593.0\n",
      "640.85 is abnormal as value of  I_C\n",
      "Has been replaced by 3.56\n",
      "ID :594.0\n",
      "640.85 is abnormal as value of  I_C\n",
      "Has been replaced by 3.56\n",
      "ID :1175.0\n",
      "649.96 is abnormal as value of  I_C\n",
      "Has been replaced by 4.21\n",
      "ID :1519.0\n",
      "639.04 is abnormal as value of  I_C\n",
      "Has been replaced by 0.745\n",
      "ID :1894.0\n",
      "635.6 is abnormal as value of  I_C\n",
      "Has been replaced by 0.7949999999999999\n",
      "ID :2137.0\n",
      "652.04 is abnormal as value of  I_C\n",
      "Has been replaced by 5.66\n",
      "ID :2223.0\n",
      "649.69 is abnormal as value of  I_C\n",
      "Has been replaced by 3.01\n",
      "ID :2271.0\n",
      "639.06 is abnormal as value of  I_C\n",
      "Has been replaced by 1.12\n",
      "ID :3540.0\n",
      "647.62 is abnormal as value of  I_C\n",
      "Has been replaced by 3.4699999999999998\n",
      "ID :3610.0\n",
      "644.15 is abnormal as value of  I_C\n",
      "Has been replaced by 0.37\n",
      "ID :3611.0\n",
      "644.15 is abnormal as value of  I_C\n",
      "Has been replaced by 0.37\n",
      "ID :3612.0\n",
      "644.16 is abnormal as value of  I_C\n",
      "Has been replaced by 0.37\n",
      "ID :5521.0\n",
      "641.85 is abnormal as value of  I_C\n",
      "Has been replaced by 0.36\n",
      "ID :16437.0\n",
      "32.91 is abnormal as value of  I_C\n",
      "Has been replaced by 7.82\n",
      "outlier_index :\n",
      " Int64Index([673, 859, 1373, 1561, 1722, 2071, 3108, 3253, 3255, 3303, 3318,\n",
      "            3429, 4672],\n",
      "           dtype='int64')\n",
      "Int64Index([4672, 673, 3255, 3108, 3429, 3303, 3253, 3318, 2071, 1561, 1722,\n",
      "            859, 1373],\n",
      "           dtype='int64')\n",
      "        ID    V_A    V_B    V_C   I_A    I_B    I_C         y\n",
      "673    737     37     37  65514  6.53  6.450  6.520       NaN\n",
      "859    948  65394      4     14  6.87  0.300  2.370       NaN\n",
      "1373  1519     77  65387     14  7.04  7.100  0.745  0.839478\n",
      "1561  1717     36     40  65396  6.81  6.830  6.820       NaN\n",
      "1722  1894     74  65350     22  7.10  7.130  0.795  0.780917\n",
      "2071  2271    114  65353     14  7.22  7.280  1.120       NaN\n",
      "3108  3393     34     38  65470  6.64  6.650  6.520       NaN\n",
      "3253  3538     36     37  65463  6.56  6.620  6.600  0.344703\n",
      "3255  3540     36  65463     18  6.50  6.560  3.470  0.209395\n",
      "3303  3597  65419     10     63  6.93  0.335  2.700       NaN\n",
      "3318  3612     37  65422      5  7.13  7.050  0.370  0.192079\n",
      "3429  3723     39     42  65440  7.21  7.060  7.080  0.460215\n",
      "4672  5068    692    708    724  1.02  0.970  0.870       NaN\n",
      "ID :737.0\n",
      "6.53 is abnormal as value of  I_A\n",
      "Has been replaced by 0.345\n",
      "ID :948.0\n",
      "6.87 is abnormal as value of  I_A\n",
      "Has been replaced by 0.33\n",
      "ID :1519.0\n",
      "7.04 is abnormal as value of  I_A\n",
      "Has been replaced by 0.77\n",
      "ID :1717.0\n",
      "6.81 is abnormal as value of  I_A\n",
      "Has been replaced by 0.33999999999999997\n",
      "ID :1894.0\n",
      "7.1 is abnormal as value of  I_A\n",
      "Has been replaced by 0.745\n",
      "ID :2271.0\n",
      "7.22 is abnormal as value of  I_A\n",
      "Has been replaced by 1.105\n",
      "ID :3393.0\n",
      "6.64 is abnormal as value of  I_A\n",
      "Has been replaced by 0.335\n",
      "ID :3538.0\n",
      "6.56 is abnormal as value of  I_A\n",
      "Has been replaced by 0.8999999999999999\n",
      "ID :3540.0\n",
      "6.5 is abnormal as value of  I_A\n",
      "Has been replaced by 0.88\n",
      "ID :3597.0\n",
      "6.93 is abnormal as value of  I_A\n",
      "Has been replaced by 0.315\n",
      "ID :3612.0\n",
      "7.13 is abnormal as value of  I_A\n",
      "Has been replaced by 0.335\n",
      "ID :3723.0\n",
      "7.21 is abnormal as value of  I_A\n",
      "Has been replaced by 0.355\n",
      "ID :5068.0\n",
      "1.02 is abnormal as value of  I_A\n",
      "Has been replaced by 0.39\n",
      "Int64Index([  14,  174,  175,  176,  498,  499,  527,  528,  529,  530,  531,\n",
      "             673,  859,  981, 1065, 1178, 1237, 1373, 1419, 1520, 1561, 1722,\n",
      "            2036, 2071, 2214, 2567, 2740, 2890, 3108, 3136, 3253, 3254, 3255,\n",
      "            3303, 3305, 3309, 3311, 3313, 3316, 3318, 3429, 5083, 6976],\n",
      "           dtype='int64')\n",
      "Int64Index([2567, 1419,   14,  527,  528,  529,  530,  531, 2071, 1561, 1178,\n",
      "             673, 3108, 2214, 1065,  174,  175,  176, 2740, 3253, 3254, 3255,\n",
      "            1722, 3136, 6976, 2890,  981, 1237,  859, 5083, 1373, 3429, 3303,\n",
      "            3305, 3309, 3311, 1520, 3313,  498,  499, 2036, 3316, 3318],\n",
      "           dtype='int64')\n",
      "        ID    V_A    V_B    V_C    I_A    I_B    I_C         y\n",
      "14      22  65382      7    107  7.190  2.840  2.750  5.440741\n",
      "174    198    493    664    669  0.420  0.250  0.340       NaN\n",
      "175    199    486    673    681  0.750  0.260  1.130  0.564661\n",
      "176    200    480    678    703  1.150  0.280  1.640       NaN\n",
      "498    542    264    681    679  2.620  0.490  3.090  1.777304\n",
      "499    543    290    293     15  6.810  6.780  6.800       NaN\n",
      "527    591     36  65402      0  6.680  6.720  3.560       NaN\n",
      "528    592     36  65402      0  1.560  6.720  3.560  0.426489\n",
      "529    593     37  65403      3  6.680  6.720  3.560       NaN\n",
      "530    594     37  65403      3  1.560  6.720  3.560       NaN\n",
      "531    595     37     39  65406  6.770  6.780  6.780       NaN\n",
      "673    737     37     37  65514  0.345  6.450  6.520       NaN\n",
      "859    948  65394      4     14  0.330  0.300  2.370       NaN\n",
      "981   1070  65477     41    692  2.620  5.320  3.150       NaN\n",
      "1065  1173  65408     22    250  7.040  6.275  2.750  7.753474\n",
      "1178  1286    310    307  65438  7.180  7.160  7.180  4.694385\n",
      "1237  1362  65386     23    244  7.000  5.350  2.790  7.806384\n",
      "1373  1519     77  65387     14  0.770  7.100  0.745  0.839478\n",
      "1419  1565  65420      8    260  6.930  5.120  0.830       NaN\n",
      "1520  1666    293    255  65460  6.870  6.800  6.810  4.521973\n",
      "1561  1717     36     40  65396  0.340  6.830  6.820       NaN\n",
      "1722  1894     74  65350     22  0.745  7.130  0.795  0.780917\n",
      "2036  2223    297  65446     18  7.060  7.060  3.010       NaN\n",
      "2071  2271    114  65353     14  1.105  7.280  1.120       NaN\n",
      "2214  2414    292    296  65470  7.020  7.000  6.990  4.299538\n",
      "2567  2797    305  65512     17  6.860  6.860  3.600  4.221675\n",
      "2740  2986  65515      0     89  6.890  1.660  0.210  4.196051\n",
      "2890  3152      3     33    260  6.670  7.580  2.780  7.650842\n",
      "3108  3393     34     38  65470  0.335  6.650  6.520       NaN\n",
      "3136  3421    290    300  65475  6.980  6.990  7.000       NaN\n",
      "3253  3538     36     37  65463  0.900  6.620  6.600  0.344703\n",
      "3254  3539     36     37  65463  1.440  6.620  6.600  0.209395\n",
      "3255  3540     36  65463     18  0.880  6.560  3.470  0.209395\n",
      "3303  3597  65419     10     63  0.315  0.335  2.700       NaN\n",
      "3305  3599  65419      5     64  6.970  0.340  3.350  0.268685\n",
      "3309  3603  65420      3     67  7.010  0.340  3.160  0.286363\n",
      "3311  3605  65420     11     67  7.010  0.340  3.360  0.278587\n",
      "3313  3607  65420     15     68  7.090  0.340  0.030  0.286621\n",
      "3316  3610     36  65423      7  6.940  7.060  0.370       NaN\n",
      "3318  3612     37  65422      5  0.335  7.050  0.370  0.192079\n",
      "3429  3723     39     42  65440  0.355  7.060  7.080  0.460215\n",
      "5083  5521     36  65410      0  1.660  6.720  0.360  0.426272\n",
      "6976  7437    807    831  65491  9.000  6.400  6.370       NaN\n",
      "ID :22.0\n",
      "65382.0 is abnormal as value of  V_A\n",
      "Has been replaced by 725.0\n",
      "ID :198.0\n",
      "493.0 is abnormal as value of  V_A\n",
      "Has been replaced by 512.0\n",
      "ID :199.0\n",
      "486.0 is abnormal as value of  V_A\n",
      "Has been replaced by 512.0\n",
      "ID :200.0\n",
      "480.0 is abnormal as value of  V_A\n",
      "Has been replaced by 512.0\n",
      "ID :542.0\n",
      "264.0 is abnormal as value of  V_A\n",
      "Has been replaced by 678.5\n",
      "ID :543.0\n",
      "290.0 is abnormal as value of  V_A\n",
      "Has been replaced by 678.5\n",
      "ID :591.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.0\n",
      "ID :592.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.0\n",
      "ID :593.0\n",
      "37.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.0\n",
      "ID :594.0\n",
      "37.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.0\n",
      "ID :595.0\n",
      "37.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.0\n",
      "ID :737.0\n",
      "37.0 is abnormal as value of  V_A\n",
      "Has been replaced by 650.0\n",
      "ID :948.0\n",
      "65394.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.5\n",
      "ID :1070.0\n",
      "65477.0 is abnormal as value of  V_A\n",
      "Has been replaced by 697.5\n",
      "ID :1173.0\n",
      "65408.0 is abnormal as value of  V_A\n",
      "Has been replaced by 707.0\n",
      "ID :1286.0\n",
      "310.0 is abnormal as value of  V_A\n",
      "Has been replaced by 715.5\n",
      "ID :1362.0\n",
      "65386.0 is abnormal as value of  V_A\n",
      "Has been replaced by 702.0\n",
      "ID :1519.0\n",
      "77.0 is abnormal as value of  V_A\n",
      "Has been replaced by 711.0\n",
      "ID :1565.0\n",
      "65420.0 is abnormal as value of  V_A\n",
      "Has been replaced by 697.5\n",
      "ID :1666.0\n",
      "293.0 is abnormal as value of  V_A\n",
      "Has been replaced by 681.5\n",
      "ID :1717.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 687.5\n",
      "ID :1894.0\n",
      "74.0 is abnormal as value of  V_A\n",
      "Has been replaced by 729.5\n",
      "ID :2223.0\n",
      "297.0 is abnormal as value of  V_A\n",
      "Has been replaced by 709.0\n",
      "ID :2271.0\n",
      "114.0 is abnormal as value of  V_A\n",
      "Has been replaced by 729.0\n",
      "ID :2414.0\n",
      "292.0 is abnormal as value of  V_A\n",
      "Has been replaced by 703.5\n",
      "ID :2797.0\n",
      "305.0 is abnormal as value of  V_A\n",
      "Has been replaced by 688.5\n",
      "ID :2986.0\n",
      "65515.0 is abnormal as value of  V_A\n",
      "Has been replaced by 691.5\n",
      "ID :3152.0\n",
      "3.0 is abnormal as value of  V_A\n",
      "Has been replaced by 673.0\n",
      "ID :3393.0\n",
      "34.0 is abnormal as value of  V_A\n",
      "Has been replaced by 659.0\n",
      "ID :3421.0\n",
      "290.0 is abnormal as value of  V_A\n",
      "Has been replaced by 700.5\n",
      "ID :3538.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 658.0\n",
      "ID :3539.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 658.0\n",
      "ID :3540.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 658.0\n",
      "ID :3597.0\n",
      "65419.0 is abnormal as value of  V_A\n",
      "Has been replaced by 700.0\n",
      "ID :3599.0\n",
      "65419.0 is abnormal as value of  V_A\n",
      "Has been replaced by 698.5\n",
      "ID :3603.0\n",
      "65420.0 is abnormal as value of  V_A\n",
      "Has been replaced by 696.0\n",
      "ID :3605.0\n",
      "65420.0 is abnormal as value of  V_A\n",
      "Has been replaced by 694.0\n",
      "ID :3607.0\n",
      "65420.0 is abnormal as value of  V_A\n",
      "Has been replaced by 695.0\n",
      "ID :3610.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 708.5\n",
      "ID :3612.0\n",
      "37.0 is abnormal as value of  V_A\n",
      "Has been replaced by 704.0\n",
      "ID :3723.0\n",
      "39.0 is abnormal as value of  V_A\n",
      "Has been replaced by 718.5\n",
      "ID :5521.0\n",
      "36.0 is abnormal as value of  V_A\n",
      "Has been replaced by 689.5\n",
      "ID :7437.0\n",
      "807.0 is abnormal as value of  V_A\n",
      "Has been replaced by 646.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   14,   499,   527,   528,   529,   530,   531,   673,   859,\n",
      "              981,  1065,  1066,  1067,  1178,  1237,  1326,  1373,  1419,\n",
      "             1520,  1561,  1722,  1950,  2036,  2071,  2214,  2364,  2454,\n",
      "             2567,  2629,  2670,  2890,  3108,  3136,  3253,  3254,  3255,\n",
      "             3303,  3305,  3309,  3311,  3313,  3316,  3317,  3318,  3429,\n",
      "             4068,  5083,  5488,  5565,  6976,  7281, 14216, 15833, 15971],\n",
      "           dtype='int64')\n",
      "Int64Index([ 2567, 14216,  1419,    14,   527,   528,   529,   530,   531,\n",
      "             2454,  2071,  1561,  1178,  1950,   673,  3108,  2214,  1065,\n",
      "             1066,  1067,  1326,  3253,  3254,  3255,  1722,  2364,  5565,\n",
      "             3136,  6976,  2629,  2890,   981,  1237, 15833,   859,  5083,\n",
      "             1373, 15971,  4068,  3429,  3303,  3305,  3309,  2670,  3311,\n",
      "             1520,  3313,  5488,   499,  2036,  3316,  3317,  3318,  7281],\n",
      "           dtype='int64')\n",
      "          ID    V_A    V_B    V_C    I_A    I_B    I_C         y\n",
      "14        22  725.0      7    107  7.190  2.840  2.750  5.440741\n",
      "499      543  678.5    293     15  6.810  6.780  6.800       NaN\n",
      "527      591  687.0  65402      0  6.680  6.720  3.560       NaN\n",
      "528      592  687.0  65402      0  1.560  6.720  3.560  0.426489\n",
      "529      593  687.0  65403      3  6.680  6.720  3.560       NaN\n",
      "530      594  687.0  65403      3  1.560  6.720  3.560       NaN\n",
      "531      595  687.0     39  65406  6.770  6.780  6.780       NaN\n",
      "673      737  650.0     37  65514  0.345  6.450  6.520       NaN\n",
      "859      948  687.5      4     14  0.330  0.300  2.370       NaN\n",
      "981     1070  697.5     41    692  2.620  5.320  3.150       NaN\n",
      "1065    1173  707.0     22    250  7.040  6.275  2.750  7.753474\n",
      "1066    1174  708.0     22    250  5.420  7.190  2.750       NaN\n",
      "1067    1175  553.0  65406     27  7.070  7.070  4.210       NaN\n",
      "1178    1286  715.5    307  65438  7.180  7.160  7.180  4.694385\n",
      "1237    1362  702.0     23    244  7.000  5.350  2.790  7.806384\n",
      "1326    1451  560.0  65454     27  6.840  6.820  3.120  7.996870\n",
      "1373    1519  711.0  65387     14  0.770  7.100  0.745  0.839478\n",
      "1419    1565  697.5      8    260  6.930  5.120  0.830       NaN\n",
      "1520    1666  681.5    255  65460  6.870  6.800  6.810  4.521973\n",
      "1561    1717  687.5     40  65396  0.340  6.830  6.820       NaN\n",
      "1722    1894  729.5  65350     22  0.745  7.130  0.795  0.780917\n",
      "1950    2137  563.0  65428     30  6.950  6.970  5.660       NaN\n",
      "2036    2223  709.0  65446     18  7.060  7.060  3.010       NaN\n",
      "2071    2271  729.0  65353     14  1.105  7.280  1.120       NaN\n",
      "2214    2414  703.5    296  65470  7.020  7.000  6.990  4.299538\n",
      "2364    2579  560.0  65481     39  6.820  6.800  1.850       NaN\n",
      "2454    2684  564.0  65438     26  6.860  6.860  0.700       NaN\n",
      "2567    2797  688.5  65512     17  6.860  6.860  3.600  4.221675\n",
      "2629    2875  555.0  65455     23  6.830  6.820  1.990  7.794517\n",
      "2670    2916  604.0  65508     30  6.730  6.720  4.970       NaN\n",
      "2890    3152  673.0     33    260  6.670  7.580  2.780  7.650842\n",
      "3108    3393  659.0     38  65470  0.335  6.650  6.520       NaN\n",
      "3136    3421  700.5    300  65475  6.980  6.990  7.000       NaN\n",
      "3253    3538  658.0     37  65463  0.900  6.620  6.600  0.344703\n",
      "3254    3539  658.0     37  65463  1.440  6.620  6.600  0.209395\n",
      "3255    3540  658.0  65463     18  0.880  6.560  3.470  0.209395\n",
      "3303    3597  700.0     10     63  0.315  0.335  2.700       NaN\n",
      "3305    3599  698.5      5     64  6.970  0.340  3.350  0.268685\n",
      "3309    3603  696.0      3     67  7.010  0.340  3.160  0.286363\n",
      "3311    3605  694.0     11     67  7.010  0.340  3.360  0.278587\n",
      "3313    3607  695.0     15     68  7.090  0.340  0.030  0.286621\n",
      "3316    3610  708.5  65423      7  6.940  7.060  0.370       NaN\n",
      "3317    3611  710.0  65423      7  0.330  1.940  0.370       NaN\n",
      "3318    3612  704.0  65422      5  0.335  7.050  0.370  0.192079\n",
      "3429    3723  718.5     42  65440  0.355  7.060  7.080  0.460215\n",
      "4068    4401  667.0      9     99  0.370  1.770  3.310  0.323052\n",
      "5083    5521  689.5  65410      0  1.660  6.720  0.360  0.426272\n",
      "5488    5939  715.0     15    137  2.890  3.540  2.690  2.858739\n",
      "5565    6016  667.0  65505     23  7.960  9.210  4.750       NaN\n",
      "6976    7437  646.5    831  65491  9.000  6.400  6.370       NaN\n",
      "7281    7742  689.0     32     30  0.340  1.380  0.490       NaN\n",
      "14216  14682  661.0     36    316  2.850  4.650  2.020       NaN\n",
      "15833  16299  638.0     18    589  5.490  6.610  3.190       NaN\n",
      "15971  16437  599.0    146     26  7.960  8.470  7.820       NaN\n",
      "ID :22.0\n",
      "7.0 is abnormal as value of  V_B\n",
      "Has been replaced by 724.5\n",
      "ID :543.0\n",
      "293.0 is abnormal as value of  V_B\n",
      "Has been replaced by 681.0\n",
      "ID :591.0\n",
      "65402.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :592.0\n",
      "65402.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :593.0\n",
      "65403.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :594.0\n",
      "65403.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :595.0\n",
      "39.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :737.0\n",
      "37.0 is abnormal as value of  V_B\n",
      "Has been replaced by 649.0\n",
      "ID :948.0\n",
      "4.0 is abnormal as value of  V_B\n",
      "Has been replaced by 687.0\n",
      "ID :1070.0\n",
      "41.0 is abnormal as value of  V_B\n",
      "Has been replaced by 694.0\n",
      "ID :1173.0\n",
      "22.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.5\n",
      "ID :1174.0\n",
      "22.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.5\n",
      "ID :1175.0\n",
      "65406.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.5\n",
      "ID :1286.0\n",
      "307.0 is abnormal as value of  V_B\n",
      "Has been replaced by 716.5\n",
      "ID :1362.0\n",
      "23.0 is abnormal as value of  V_B\n",
      "Has been replaced by 698.5\n",
      "ID :1451.0\n",
      "65454.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.5\n",
      "ID :1519.0\n",
      "65387.0 is abnormal as value of  V_B\n",
      "Has been replaced by 712.5\n",
      "ID :1565.0\n",
      "8.0 is abnormal as value of  V_B\n",
      "Has been replaced by 692.5\n",
      "ID :1666.0\n",
      "255.0 is abnormal as value of  V_B\n",
      "Has been replaced by 684.5\n",
      "ID :1717.0\n",
      "40.0 is abnormal as value of  V_B\n",
      "Has been replaced by 683.0\n",
      "ID :1894.0\n",
      "65350.0 is abnormal as value of  V_B\n",
      "Has been replaced by 713.5\n",
      "ID :2137.0\n",
      "65428.0 is abnormal as value of  V_B\n",
      "Has been replaced by 694.5\n",
      "ID :2223.0\n",
      "65446.0 is abnormal as value of  V_B\n",
      "Has been replaced by 707.0\n",
      "ID :2271.0\n",
      "65353.0 is abnormal as value of  V_B\n",
      "Has been replaced by 719.0\n",
      "ID :2414.0\n",
      "296.0 is abnormal as value of  V_B\n",
      "Has been replaced by 701.5\n",
      "ID :2579.0\n",
      "65481.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.0\n",
      "ID :2684.0\n",
      "65438.0 is abnormal as value of  V_B\n",
      "Has been replaced by 687.0\n",
      "ID :2797.0\n",
      "65512.0 is abnormal as value of  V_B\n",
      "Has been replaced by 687.0\n",
      "ID :2875.0\n",
      "65455.0 is abnormal as value of  V_B\n",
      "Has been replaced by 684.5\n",
      "ID :2916.0\n",
      "65508.0 is abnormal as value of  V_B\n",
      "Has been replaced by 675.0\n",
      "ID :3152.0\n",
      "33.0 is abnormal as value of  V_B\n",
      "Has been replaced by 668.5\n",
      "ID :3393.0\n",
      "38.0 is abnormal as value of  V_B\n",
      "Has been replaced by 664.5\n",
      "ID :3421.0\n",
      "300.0 is abnormal as value of  V_B\n",
      "Has been replaced by 699.0\n",
      "ID :3538.0\n",
      "37.0 is abnormal as value of  V_B\n",
      "Has been replaced by 661.5\n",
      "ID :3539.0\n",
      "37.0 is abnormal as value of  V_B\n",
      "Has been replaced by 661.5\n",
      "ID :3540.0\n",
      "65463.0 is abnormal as value of  V_B\n",
      "Has been replaced by 661.5\n",
      "ID :3597.0\n",
      "10.0 is abnormal as value of  V_B\n",
      "Has been replaced by 704.0\n",
      "ID :3599.0\n",
      "5.0 is abnormal as value of  V_B\n",
      "Has been replaced by 699.0\n",
      "ID :3603.0\n",
      "3.0 is abnormal as value of  V_B\n",
      "Has been replaced by 701.0\n",
      "ID :3605.0\n",
      "11.0 is abnormal as value of  V_B\n",
      "Has been replaced by 698.0\n",
      "ID :3607.0\n",
      "15.0 is abnormal as value of  V_B\n",
      "Has been replaced by 705.0\n",
      "ID :3610.0\n",
      "65423.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.0\n",
      "ID :3611.0\n",
      "65423.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.0\n",
      "ID :3612.0\n",
      "65422.0 is abnormal as value of  V_B\n",
      "Has been replaced by 703.0\n",
      "ID :3723.0\n",
      "42.0 is abnormal as value of  V_B\n",
      "Has been replaced by 698.0\n",
      "ID :4401.0\n",
      "9.0 is abnormal as value of  V_B\n",
      "Has been replaced by 668.0\n",
      "ID :5521.0\n",
      "65410.0 is abnormal as value of  V_B\n",
      "Has been replaced by 682.5\n",
      "ID :5939.0\n",
      "15.0 is abnormal as value of  V_B\n",
      "Has been replaced by 715.5\n",
      "ID :6016.0\n",
      "65505.0 is abnormal as value of  V_B\n",
      "Has been replaced by 666.0\n",
      "ID :7437.0\n",
      "831.0 is abnormal as value of  V_B\n",
      "Has been replaced by 645.5\n",
      "ID :7742.0\n",
      "32.0 is abnormal as value of  V_B\n",
      "Has been replaced by 681.0\n",
      "ID :14682.0\n",
      "36.0 is abnormal as value of  V_B\n",
      "Has been replaced by 657.0\n",
      "ID :16299.0\n",
      "18.0 is abnormal as value of  V_B\n",
      "Has been replaced by 633.5\n",
      "ID :16437.0\n",
      "146.0 is abnormal as value of  V_B\n",
      "Has been replaced by 597.5\n",
      "Int64Index([   14,   127,   468,   499,   529,   530,   531,   673,   859,\n",
      "             1065,  1066,  1067,  1178,  1237,  1326,  1373,  1419,  1520,\n",
      "             1561,  1722,  1950,  2036,  2071,  2214,  2364,  2454,  2567,\n",
      "             2629,  2670,  2740,  2887,  2890,  3108,  3136,  3253,  3254,\n",
      "             3255,  3303,  3304,  3305,  3309,  3310,  3311,  3312,  3313,\n",
      "             3316,  3317,  3318,  3429,  3967,  4068,  4301,  4857,  5488,\n",
      "             5565,  6976,  7281, 10456, 10457, 11366, 12348, 12723, 14216,\n",
      "            15971],\n",
      "           dtype='int64')\n",
      "Int64Index([ 2567, 14216,  1419,    14,   529,   530,   531,  2454,  2071,\n",
      "             1561,  1178,  1950,   673,  3108,  2214,  1065,  1066,  1067,\n",
      "             1326, 12723,  2740,  3253,  3254,  3255,  1722,  2364,  5565,\n",
      "            12348,  3136,  6976,  2629,  2887,  2890,  4301,   468,  1237,\n",
      "             3967, 10456, 10457,   859,  5488,  1373, 15971,  4068,  3429,\n",
      "            11366,  3303,  3304,  3305,  3309,  2670,  3310,  1520,  3311,\n",
      "             3312,   499,  2036,  3313,  3316,  3317,  3318,  4857,  7281,\n",
      "              127],\n",
      "           dtype='int64')\n",
      "          ID    V_A    V_B    V_C    I_A    I_B    I_C          y\n",
      "14        22  725.0  724.5    107  7.190  2.840  2.750   5.440741\n",
      "127      135  555.0  559.0  65498  6.850  6.770  6.790        NaN\n",
      "468      512  554.0  563.0     23  6.700  6.640  6.670   7.807098\n",
      "499      543  678.5  681.0     15  6.810  6.780  6.800        NaN\n",
      "529      593  687.0  682.0      3  6.680  6.720  3.560        NaN\n",
      "530      594  687.0  682.0      3  1.560  6.720  3.560        NaN\n",
      "531      595  687.0  682.0  65406  6.770  6.780  6.780        NaN\n",
      "673      737  650.0  649.0  65514  0.345  6.450  6.520        NaN\n",
      "859      948  687.5  687.0     14  0.330  0.300  2.370        NaN\n",
      "1065    1173  707.0  703.5    250  7.040  6.275  2.750   7.753474\n",
      "1066    1174  708.0  703.5    250  5.420  7.190  2.750        NaN\n",
      "1067    1175  553.0  703.5     27  7.070  7.070  4.210        NaN\n",
      "1178    1286  715.5  716.5  65438  7.180  7.160  7.180   4.694385\n",
      "1237    1362  702.0  698.5    244  7.000  5.350  2.790   7.806384\n",
      "1326    1451  560.0  682.5     27  6.840  6.820  3.120   7.996870\n",
      "1373    1519  711.0  712.5     14  0.770  7.100  0.745   0.839478\n",
      "1419    1565  697.5  692.5    260  6.930  5.120  0.830        NaN\n",
      "1520    1666  681.5  684.5  65460  6.870  6.800  6.810   4.521973\n",
      "1561    1717  687.5  683.0  65396  0.340  6.830  6.820        NaN\n",
      "1722    1894  729.5  713.5     22  0.745  7.130  0.795   0.780917\n",
      "1950    2137  563.0  694.5     30  6.950  6.970  5.660        NaN\n",
      "2036    2223  709.0  707.0     18  7.060  7.060  3.010        NaN\n",
      "2071    2271  729.0  719.0     14  1.105  7.280  1.120        NaN\n",
      "2214    2414  703.5  701.5  65470  7.020  7.000  6.990   4.299538\n",
      "2364    2579  560.0  682.0     39  6.820  6.800  1.850        NaN\n",
      "2454    2684  564.0  687.0     26  6.860  6.860  0.700        NaN\n",
      "2567    2797  688.5  687.0     17  6.860  6.860  3.600   4.221675\n",
      "2629    2875  555.0  684.5     23  6.830  6.820  1.990   7.794517\n",
      "2670    2916  604.0  675.0     30  6.730  6.720  4.970        NaN\n",
      "2740    2986  691.5    0.0     89  6.890  1.660  0.210   4.196051\n",
      "...      ...    ...    ...    ...    ...    ...    ...        ...\n",
      "3253    3538  658.0  661.5  65463  0.900  6.620  6.600   0.344703\n",
      "3254    3539  658.0  661.5  65463  1.440  6.620  6.600   0.209395\n",
      "3255    3540  658.0  661.5     18  0.880  6.560  3.470   0.209395\n",
      "3303    3597  700.0  704.0     63  0.315  0.335  2.700        NaN\n",
      "3304    3598  690.0  697.0     63  0.320  0.340  0.140        NaN\n",
      "3305    3599  698.5  699.0     64  6.970  0.340  3.350   0.268685\n",
      "3309    3603  696.0  701.0     67  7.010  0.340  3.160   0.286363\n",
      "3310    3604  697.0  693.0     67  0.320  0.340  0.600        NaN\n",
      "3311    3605  694.0  698.0     67  7.010  0.340  3.360   0.278587\n",
      "3312    3606  691.0  703.0     67  0.330  0.340  0.800   0.286621\n",
      "3313    3607  695.0  705.0     68  7.090  0.340  0.030   0.286621\n",
      "3316    3610  708.5  703.0      7  6.940  7.060  0.370        NaN\n",
      "3317    3611  710.0  703.0      7  0.330  1.940  0.370        NaN\n",
      "3318    3612  704.0  703.0      5  0.335  7.050  0.370   0.192079\n",
      "3429    3723  718.5  698.0  65440  0.355  7.060  7.080   0.460215\n",
      "3967    4300  665.0  668.0     31  0.330  0.320  0.700   0.325498\n",
      "4068    4401  667.0  668.0     99  0.370  1.770  3.310   0.323052\n",
      "4301    4651  543.0  612.0    479  0.050  0.060  0.050   0.032366\n",
      "4857    5264  679.0  678.0     10  0.310  0.330  0.020   0.203511\n",
      "5488    5939  715.0  715.5    137  2.890  3.540  2.690   2.858739\n",
      "5565    6016  667.0  666.0     23  7.960  9.210  4.750        NaN\n",
      "6976    7437  646.5  645.5  65491  9.000  6.400  6.370        NaN\n",
      "7281    7742  689.0  681.0     30  0.340  1.380  0.490        NaN\n",
      "10456  10920  643.0  653.0     27  0.330  0.330  0.740        NaN\n",
      "10457  10921  646.0  638.0     27  0.340  0.340  0.590   0.590406\n",
      "11366  11832  622.0  618.0    146  7.900  7.880  8.760  10.814965\n",
      "12348  12814  671.0  675.0     14  2.880  3.110  3.930   3.439684\n",
      "12723  13189  622.0  618.0     10  2.920  2.880  3.450        NaN\n",
      "14216  14682  661.0  657.0    316  2.850  4.650  2.020        NaN\n",
      "15971  16437  599.0  597.5     26  7.960  8.470  7.820        NaN\n",
      "\n",
      "[64 rows x 8 columns]\n",
      "ID :22.0\n",
      "107.0 is abnormal as value of  V_C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has been replaced by 722.5\n",
      "ID :135.0\n",
      "65498.0 is abnormal as value of  V_C\n",
      "Has been replaced by 677.0\n",
      "ID :512.0\n",
      "23.0 is abnormal as value of  V_C\n",
      "Has been replaced by 665.0\n",
      "ID :543.0\n",
      "15.0 is abnormal as value of  V_C\n",
      "Has been replaced by 679.5\n",
      "ID :593.0\n",
      "3.0 is abnormal as value of  V_C\n",
      "Has been replaced by 338.5\n",
      "ID :594.0\n",
      "3.0 is abnormal as value of  V_C\n",
      "Has been replaced by 338.5\n",
      "ID :595.0\n",
      "65406.0 is abnormal as value of  V_C\n",
      "Has been replaced by 338.5\n",
      "ID :737.0\n",
      "65514.0 is abnormal as value of  V_C\n",
      "Has been replaced by 645.0\n",
      "ID :948.0\n",
      "14.0 is abnormal as value of  V_C\n",
      "Has been replaced by 697.0\n",
      "ID :1173.0\n",
      "250.0 is abnormal as value of  V_C\n",
      "Has been replaced by 706.5\n",
      "ID :1174.0\n",
      "250.0 is abnormal as value of  V_C\n",
      "Has been replaced by 706.5\n",
      "ID :1175.0\n",
      "27.0 is abnormal as value of  V_C\n",
      "Has been replaced by 706.5\n",
      "ID :1286.0\n",
      "65438.0 is abnormal as value of  V_C\n",
      "Has been replaced by 716.5\n",
      "ID :1362.0\n",
      "244.0 is abnormal as value of  V_C\n",
      "Has been replaced by 700.5\n",
      "ID :1451.0\n",
      "27.0 is abnormal as value of  V_C\n",
      "Has been replaced by 682.0\n",
      "ID :1519.0\n",
      "14.0 is abnormal as value of  V_C\n",
      "Has been replaced by 703.5\n",
      "ID :1565.0\n",
      "260.0 is abnormal as value of  V_C\n",
      "Has been replaced by 695.5\n",
      "ID :1666.0\n",
      "65460.0 is abnormal as value of  V_C\n",
      "Has been replaced by 684.0\n",
      "ID :1717.0\n",
      "65396.0 is abnormal as value of  V_C\n",
      "Has been replaced by 680.5\n",
      "ID :1894.0\n",
      "22.0 is abnormal as value of  V_C\n",
      "Has been replaced by 717.5\n",
      "ID :2137.0\n",
      "30.0 is abnormal as value of  V_C\n",
      "Has been replaced by 695.5\n",
      "ID :2223.0\n",
      "18.0 is abnormal as value of  V_C\n",
      "Has been replaced by 705.5\n",
      "ID :2271.0\n",
      "14.0 is abnormal as value of  V_C\n",
      "Has been replaced by 726.0\n",
      "ID :2414.0\n",
      "65470.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :2579.0\n",
      "39.0 is abnormal as value of  V_C\n",
      "Has been replaced by 683.0\n",
      "ID :2684.0\n",
      "26.0 is abnormal as value of  V_C\n",
      "Has been replaced by 685.5\n",
      "ID :2797.0\n",
      "17.0 is abnormal as value of  V_C\n",
      "Has been replaced by 686.5\n",
      "ID :2875.0\n",
      "23.0 is abnormal as value of  V_C\n",
      "Has been replaced by 683.0\n",
      "ID :2916.0\n",
      "30.0 is abnormal as value of  V_C\n",
      "Has been replaced by 676.0\n",
      "ID :2986.0\n",
      "89.0 is abnormal as value of  V_C\n",
      "Has been replaced by 686.5\n",
      "ID :3149.0\n",
      "5.0 is abnormal as value of  V_C\n",
      "Has been replaced by 668.5\n",
      "ID :3152.0\n",
      "260.0 is abnormal as value of  V_C\n",
      "Has been replaced by 669.5\n",
      "ID :3393.0\n",
      "65470.0 is abnormal as value of  V_C\n",
      "Has been replaced by 655.0\n",
      "ID :3421.0\n",
      "65475.0 is abnormal as value of  V_C\n",
      "Has been replaced by 701.5\n",
      "ID :3538.0\n",
      "65463.0 is abnormal as value of  V_C\n",
      "Has been replaced by 658.0\n",
      "ID :3539.0\n",
      "65463.0 is abnormal as value of  V_C\n",
      "Has been replaced by 658.0\n",
      "ID :3540.0\n",
      "18.0 is abnormal as value of  V_C\n",
      "Has been replaced by 658.0\n",
      "ID :3597.0\n",
      "63.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.0\n",
      "ID :3598.0\n",
      "63.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.0\n",
      "ID :3599.0\n",
      "64.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.0\n",
      "ID :3603.0\n",
      "67.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :3604.0\n",
      "67.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :3605.0\n",
      "67.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :3606.0\n",
      "67.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :3607.0\n",
      "68.0 is abnormal as value of  V_C\n",
      "Has been replaced by 698.5\n",
      "ID :3610.0\n",
      "7.0 is abnormal as value of  V_C\n",
      "Has been replaced by 693.0\n",
      "ID :3611.0\n",
      "7.0 is abnormal as value of  V_C\n",
      "Has been replaced by 693.0\n",
      "ID :3612.0\n",
      "5.0 is abnormal as value of  V_C\n",
      "Has been replaced by 693.0\n",
      "ID :3723.0\n",
      "65440.0 is abnormal as value of  V_C\n",
      "Has been replaced by 704.5\n",
      "ID :4300.0\n",
      "31.0 is abnormal as value of  V_C\n",
      "Has been replaced by 663.0\n",
      "ID :4401.0\n",
      "99.0 is abnormal as value of  V_C\n",
      "Has been replaced by 664.5\n",
      "ID :4651.0\n",
      "479.0 is abnormal as value of  V_C\n",
      "Has been replaced by 0.0\n",
      "ID :5264.0\n",
      "10.0 is abnormal as value of  V_C\n",
      "Has been replaced by 674.0\n",
      "ID :5939.0\n",
      "137.0 is abnormal as value of  V_C\n",
      "Has been replaced by 714.5\n",
      "ID :6016.0\n",
      "23.0 is abnormal as value of  V_C\n",
      "Has been replaced by 663.0\n",
      "ID :7437.0\n",
      "65491.0 is abnormal as value of  V_C\n",
      "Has been replaced by 644.0\n",
      "ID :7742.0\n",
      "30.0 is abnormal as value of  V_C\n",
      "Has been replaced by 665.5\n",
      "ID :10920.0\n",
      "27.0 is abnormal as value of  V_C\n",
      "Has been replaced by 628.0\n",
      "ID :10921.0\n",
      "27.0 is abnormal as value of  V_C\n",
      "Has been replaced by 628.0\n",
      "ID :11832.0\n",
      "146.0 is abnormal as value of  V_C\n",
      "Has been replaced by 616.5\n",
      "ID :12814.0\n",
      "14.0 is abnormal as value of  V_C\n",
      "Has been replaced by 671.0\n",
      "ID :13189.0\n",
      "10.0 is abnormal as value of  V_C\n",
      "Has been replaced by 619.0\n",
      "ID :14682.0\n",
      "316.0 is abnormal as value of  V_C\n",
      "Has been replaced by 655.0\n",
      "ID :16437.0\n",
      "26.0 is abnormal as value of  V_C\n",
      "Has been replaced by 594.5\n"
     ]
    }
   ],
   "source": [
    "#计算偏差率的辅助列\n",
    "# for c in ['I_A','I_B','I_C','V_A','V_B','V_C']:\n",
    "for c in ['I_A','I_B','I_C']:\n",
    "    df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "    df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']\n",
    "    df[c+'_cor'] = df[c]\n",
    "    \n",
    "    #out of range\n",
    "    oor_index = df[df[c]>20].index\n",
    "    print(oor_index)\n",
    "    \n",
    "#     outlier_index = df[df[c+'_exception_ratio']>1.6].index\n",
    "#     print(outlier_index)\n",
    "    \n",
    "#     ab_index = pd.Int64Index(set(list(oor_index)+list(outlier_index)))\n",
    "    ab_index = pd.Int64Index(set(list(oor_index)))\n",
    "    print(ab_index)\n",
    "    \n",
    "    ab_data = df.loc[ab_index].sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    print(ab_data[['ID', 'V_A', 'V_B', 'V_C', 'I_A', 'I_B', 'I_C', 'y']])\n",
    "    \n",
    "    # 上下记录均值替代异常值\n",
    "    for idx, line in ab_data.iterrows():\n",
    "        ID = line['ID']\n",
    "        value = line[c]\n",
    "        \n",
    "        index = df[df['ID'] == ID].index\n",
    "            \n",
    "        before_offset = 1\n",
    "        while (idx - before_offset)in ab_index:\n",
    "            before_offset += 1\n",
    "\n",
    "        after_offset = 1\n",
    "        while (idx + after_offset) in ab_index:\n",
    "            after_offset += 1\n",
    "    \n",
    "        print('ID :' + str(ID))\n",
    "        print(value, 'is abnormal as value of ',c)\n",
    "        replace_value = (df.loc[index - before_offset, c].values + df.loc[index + after_offset, c].values) / 2\n",
    "        df.loc[index, c+'_cor'] = replace_value[0]\n",
    "        print('Has been replaced by '+str(replace_value[0]))\n",
    "    \n",
    "    df[c] = df[c+'_cor']\n",
    "    df.drop(columns=[c+'_cor',c+'_exception_ratio',c+'_avg_sequence'],axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "# Corrections for presumed outlier points for I_A \n",
    "    \n",
    "for c in ['I_A']:\n",
    "    df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "    df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']\n",
    "    df[c+'_cor'] = df[c]\n",
    "    \n",
    "#     #out of range\n",
    "#     oor_index = df[df[c]>20].index\n",
    "#     print(oor_index)\n",
    "    \n",
    "    outlier_index = df[df[c+'_exception_ratio']>4].index\n",
    "    print('outlier_index :\\n',outlier_index)\n",
    "    \n",
    "#     ab_index = pd.Int64Index(set(list(oor_index)+list(outlier_index)))\n",
    "    ab_index = pd.Int64Index(set(list(outlier_index)))\n",
    "    print(ab_index)\n",
    "    \n",
    "    ab_data = df.loc[ab_index].sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    print(ab_data[['ID', 'V_A', 'V_B', 'V_C', 'I_A', 'I_B', 'I_C', 'y']])\n",
    "    \n",
    "    # 上下记录均值替代异常值\n",
    "    for idx, line in ab_data.iterrows():\n",
    "        ID = line['ID']\n",
    "        value = line[c]\n",
    "        \n",
    "        index = df[df['ID'] == ID].index\n",
    "            \n",
    "        before_offset = 1\n",
    "        while (idx - before_offset)in ab_index:\n",
    "            before_offset += 1\n",
    "\n",
    "        after_offset = 1\n",
    "        while (idx + after_offset) in ab_index:\n",
    "            after_offset += 1\n",
    "    \n",
    "        print('ID :' + str(ID))\n",
    "        print(value, 'is abnormal as value of ',c)\n",
    "        replace_value = (df.loc[index - before_offset, c].values + df.loc[index + after_offset, c].values) / 2\n",
    "        df.loc[index, c+'_cor'] = replace_value[0]\n",
    "        print('Has been replaced by '+str(replace_value[0]))\n",
    "    \n",
    "    df[c] = df[c+'_cor']\n",
    "    df.drop(columns=[c+'_cor',c+'_exception_ratio',c+'_avg_sequence'],axis=1,inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "for c in ['V_A','V_B','V_C']:\n",
    "    df[c+'_avg_sequence'] = np.nanmean([df[c].shift(i) for i in rolling_mask_eight],axis=0)\n",
    "    df[c+'_exception_ratio'] = np.abs(df[c]-df[c+'_avg_sequence'])/df[c+'_avg_sequence']\n",
    "    df[c+'_cor'] = df[c]\n",
    "    \n",
    "    #out of range\n",
    "    oor_index = df[(df[c]>800)|((df[c]<500)&(df[c]!=0))].index\n",
    "    print(oor_index)\n",
    "    \n",
    "#     outlier_index = df[df[c+'_exception_ratio']>1.6].index\n",
    "#     print(outlier_index)\n",
    "    \n",
    "#     ab_index = pd.Int64Index(set(list(oor_index)+list(outlier_index)))\n",
    "    ab_index = pd.Int64Index(set(list(oor_index)))\n",
    "    print(ab_index)\n",
    "    \n",
    "    ab_data = df.loc[ab_index].sort_values(by='ID', ascending=True)\n",
    "    \n",
    "    print(ab_data[['ID', 'V_A', 'V_B', 'V_C', 'I_A', 'I_B', 'I_C', 'y']])\n",
    "    \n",
    "    # 上下记录均值替代异常值\n",
    "    for idx, line in ab_data.iterrows():\n",
    "        ID = line['ID']\n",
    "        value = line[c]\n",
    "        \n",
    "        index = df[df['ID'] == ID].index\n",
    "            \n",
    "        before_offset = 1\n",
    "        while (idx - before_offset)in ab_index:\n",
    "            before_offset += 1\n",
    "\n",
    "        after_offset = 1\n",
    "        while (idx + after_offset) in ab_index:\n",
    "            after_offset += 1\n",
    "    \n",
    "        print('ID :' + str(ID))\n",
    "        print(value, 'is abnormal as value of ',c)\n",
    "        replace_value = (df.loc[index - before_offset, c].values + df.loc[index + after_offset, c].values) / 2\n",
    "        df.loc[index, c+'_cor'] = replace_value[0]\n",
    "        print('Has been replaced by '+str(replace_value[0]))\n",
    "    \n",
    "    df[c] = df[c+'_cor']\n",
    "    df.drop(columns=[c+'_cor',c+'_exception_ratio',c+'_avg_sequence'],axis=1,inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# Correct Power Features        \n",
    "df['P_A']=df['I_A']*df['V_A']\n",
    "df['P_B']=df['I_B']*df['V_B']\n",
    "df['P_C']=df['I_C']*df['V_C']\n",
    "\n",
    "df['P_avg']=1/3*(df['P_A']+df['P_B']+df['P_C'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.drop(columns=['I_A','I_B','I_C','V_A','V_B','V_C','P_A','P_B','P_C','P_avg'],axis=1,inplace=True)\n",
    "# df.drop(columns=['I_A_avg_sequence','I_A_exception_ratio','I_B_avg_sequence','I_B_exception_ratio','I_C_avg_sequence','I_C_exception_ratio','V_B_avg_sequence','V_B_exception_ratio','V_A_avg_sequence','V_A_exception_ratio','V_C_avg_sequence','V_C_exception_ratio',],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.增加前后有效发电量均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = all_data.copy()\n",
    "\n",
    "\n",
    "\n",
    "#前二后二\n",
    "next_one = []\n",
    "prev_one = []\n",
    "next_id = []\n",
    "prev_id = []\n",
    "\n",
    "second_next_one = []\n",
    "second_prev_one = []\n",
    "\n",
    "df_len = df.shape[0]\n",
    "\n",
    "i_y =df.columns.get_loc(\"y\")\n",
    "\n",
    "def get_prev_nn_index(cur_i):\n",
    "    prev_i = cur_i-1\n",
    "    while(prev_i>=0 and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i-=1\n",
    "    return prev_i\n",
    "\n",
    "def get_next_nn_index(cur_i):\n",
    "    prev_i = cur_i+1\n",
    "    while(prev_i<df_len and pd.isnull(df.iat[prev_i,i_y])):\n",
    "        prev_i+=1\n",
    "    return prev_i\n",
    "\n",
    "for i in range(df_len):\n",
    "    f_pre_i=get_prev_nn_index(i)\n",
    "    if(f_pre_i)<0:\n",
    "        prev_one.append(np.nan)\n",
    "        prev_id.append(0)\n",
    "    else:\n",
    "        prev_one.append(df.iat[f_pre_i,i_y])\n",
    "        prev_id.append(f_pre_i)\n",
    "        \n",
    "    s_pre_i=get_prev_nn_index(f_pre_i)\n",
    "    if (s_pre_i)<0:\n",
    "        second_prev_one.append(np.nan)\n",
    "    else:\n",
    "        second_prev_one.append(df.iat[s_pre_i,i_y])\n",
    "    \n",
    "    f_next_i=get_next_nn_index(i)\n",
    "    if(f_next_i<df_len):\n",
    "        next_one.append(df.iat[f_next_i,i_y])\n",
    "        next_id.append(f_next_i)\n",
    "    else:\n",
    "        next_one.append(np.nan)\n",
    "        next_id.append(df_len)\n",
    "    \n",
    "    s_next_i=get_next_nn_index(f_next_i)\n",
    "    if(s_next_i<df_len):\n",
    "        second_next_one.append(df.iat[s_next_i,i_y])\n",
    "    else:\n",
    "        second_next_one.append(np.nan)\n",
    "        \n",
    "\n",
    "df['next_value'] = next_one\n",
    "df['prev_value'] = prev_one\n",
    "df['avg_value'] = np.nanmean([df['next_value'], df['prev_value']],axis=0)\n",
    "\n",
    "df.drop(['next_value','prev_value'],1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.增加前后功率均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg(df):\n",
    "    array = np.array(df[\"P_avg\"])\n",
    "    newarray=[]\n",
    "    num = 0\n",
    "    for i in np.arange(len(array)):\n",
    "        for j in np.arange(10):\n",
    "            if i<10:\n",
    "                num = (array[j-1]+array[j-2]+array[j-3])/3\n",
    "            if i>=10:\n",
    "                num = (array[i-1]+array[i-2]+array[i-3]+array[i-5]+array[i-6]+array[i-7]+array[i-8]+array[i-9])/9\n",
    "        newarray.append(num)\n",
    "    df[\"old_SoCalledSF_P_avg\"] = newarray\n",
    "    return df\n",
    "\n",
    "df = add_avg(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.训练集测试集数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.去除训练集的重复样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 8409)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拆分数据\n",
    "\n",
    "train_data = df[df['is_train']==1]\n",
    "test_data = df[df['is_train']==0]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备提交结果\n",
    "\n",
    "df_result = pd.DataFrame()\n",
    "df_result['ID'] = list(test_data['ID'])\n",
    "special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_drop_base=['I_A','I_B','I_C','V_A','V_B','V_C','P_A','P_B','P_C','P_avg']\n",
    "# l_drop =[]\n",
    "# for c in l_drop_base:\n",
    "#     l_drop.append(c+'_cor')\n",
    "# print(l_drop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重前训练集条数:9000\n",
      "去重后训练集条数:8918\n"
     ]
    }
   ],
   "source": [
    "print('去重前训练集条数:' +str(train_data.shape[0]))\n",
    "# train_data = train_data.drop_duplicates(train_data.columns.drop(['ID','avg_value','old_SoCalledSF_P_avg']), keep='first')\n",
    "# train_data = train_data.drop_duplicates(train_data.columns.drop(['ID','avg_value','old_SoCalledSF_P_avg']+l_drop), keep='first')\n",
    "train_data = train_data.drop_duplicates(train_data.columns.drop(['ID','avg_value','old_SoCalledSF_P_avg']), keep='first')\n",
    "print('去重后训练集条数:' +str(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.使训练集样本分布更合理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def improve_train_test_data(train_data, test_data, poly=False, select=False):\n",
    "#     Y = train_data['y']\n",
    "#     X = train_data.drop(['y','ID','is_train'], axis=1)\n",
    "#     test_data = test_data.drop(['y','ID','is_train'], axis=1)\n",
    "    \n",
    "#     polynm = None\n",
    "#     if poly:\n",
    "#         from sklearn.preprocessing import PolynomialFeatures\n",
    "#         polynm = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "#         X = polynm.fit_transform(X)\n",
    "#         test_data = polynm.transform(test_data)\n",
    "        \n",
    "#     X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=123)\n",
    "    \n",
    "#     sm = None\n",
    "#     if select:\n",
    "#         from sklearn.feature_selection import SelectFromModel\n",
    "#         sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "#         X_train = sm.fit_transform(X_train, Y_train)\n",
    "#         X_val = sm.transform(X_val)\n",
    "#         test_data = sm.transform(test_data)\n",
    "        \n",
    "#     train_X = np.concatenate([X_train, X_val])\n",
    "#     train_Y = np.concatenate([Y_train, Y_val])\n",
    "\n",
    "# #     sm = None\n",
    "# #     if select:\n",
    "# #         from sklearn.feature_selection import SelectFromModel\n",
    "# #         sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "# #         X = sm.fit_transform(X, Y)\n",
    "# #         test_data = sm.transform(test_data)\n",
    "    \n",
    "# #     train_X = X\n",
    "# #     train_Y = Y\n",
    "#     test_X = test_data\n",
    "        \n",
    "#     return train_X, train_Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_train_test_data(train_data, test_data, poly=False, select=False):\n",
    "    Y = train_data[['y']]\n",
    "    X = train_data.drop(['y','is_train','I_B','I_C'], axis=1)\n",
    "    test_X = test_data.drop(['y','is_train','I_B','I_C'], axis=1)\n",
    "        \n",
    "    return X, Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=True)\n",
    "# X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data)\n",
    "\n",
    "# train_X = np.concatenate([X_train, X_test])\n",
    "# train_Y = np.concatenate([y_train, y_test])\n",
    "\n",
    "\n",
    "\n",
    "# test_X = sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X, train_Y, test_X = improve_train_test_data(train_data, test_data, poly=True, select=True)\n",
    "train_X, train_Y, test_X = improve_train_test_data(train_data, test_data, poly=False, select=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义打分函数,  SCORE = 1/(1+RMSE)\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# def cal_score(mse):\n",
    "#     return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "# 定义交叉验证函数  \n",
    "def cross_validate(models, X, Y, test_X, cv=10):\n",
    "    model_name, mse_avg, score_avg, rmse_r, score_r = [], [], [], [], []\n",
    "    test_preds_all_folds = []\n",
    "    for i, model in enumerate(models):\n",
    "        #获取模型名\n",
    "        name = str(i + 1) + '.' + str(model) \n",
    "#         print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        print(name)\n",
    "#         model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        model_name.append(name.split('(')[0])\n",
    "        #计算metric\n",
    "        \n",
    "        skf = KFold(10, shuffle=True, random_state=2018)\n",
    "        Y_pred = np.zeros(X.shape[0])\n",
    "        for train_idx,val_idx in skf.split(X):\n",
    "            print('train_idx: \\n',str(len(train_idx)),train_idx)\n",
    "            print('val_idx: \\n',str(len(val_idx)),val_idx)\n",
    "            X_train, X_val, Y_train, Y_val = X.iloc[train_idx], X.iloc[val_idx], Y.iloc[train_idx], Y.iloc[val_idx]\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred[val_idx] = model.predict(X_val)\n",
    "            print('Y_pred: \\n',Y_pred)\n",
    "            test_preds_all_folds.append(model.predict(test_X))\n",
    "        rmse_c = np.sqrt(mean_squared_error(Y, Y_pred))\n",
    "        score_c = 1/(1+rmse_c)\n",
    "        rmse_r.append(rmse_c)\n",
    "        score_r.append(score_c)\n",
    "        \n",
    "        test_preds = np.mean(test_preds_all_folds, axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         folds=strat_k_fold.split(X,Y)\n",
    "        #apply shuffling to cross_val_score\n",
    "#         strat_k_fold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=0)\n",
    "#         nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "#         nmse = cross_val_score(model, X, Y, cv=cv, scoring='neg_mean_squared_error')\n",
    "#         avg_mse = np.average(-nmse)\n",
    "#         mse_avg.append(avg_mse)\n",
    "#         #计算分数\n",
    "#         scores = cal_score(-nmse)\n",
    "#         avg_score = np.average(scores)    \n",
    "#         score_avg.append(avg_score)\n",
    "#         print('MSE:', -nmse)\n",
    "#         print('Score:', scores)\n",
    "#         print('Average MSE:', avg_mse, ' - Score:', avg_score, ' - Score_c:', score_c, '\\n')\n",
    "        print('Score:', score_c, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "#     res['Avg MSE'] = mse_avg\n",
    "#     res['Avg Score'] = score_avg\n",
    "    res['Score Customize'] = score_r\n",
    "    res['RMSE Customize'] = rmse_r\n",
    "    return res, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基学习器\n",
    "\n",
    "xgbt1 = xgb.XGBRegressor(n_estimators=950, max_depth=3, max_features='sqrt', random_state=321, n_jobs=8)\n",
    "xgbt2 = xgb.XGBRegressor(n_estimators=1000, max_depth=3, max_features='sqrt', random_state=456, n_jobs=8)\n",
    "xgbt3 = xgb.XGBRegressor(n_estimators=1100, max_depth=3, max_features='sqrt', random_state=789)\n",
    "# n_estimators=1000  max_depth=5  'sqrt'  GradientBoostingRegressor 最佳参数 ,learning_rate=0.08\n",
    "gbdt1 = GradientBoostingRegressor(n_estimators=800, max_depth=4, max_features='log2', random_state=123,learning_rate=0.08)\n",
    "gbdt2 = GradientBoostingRegressor(n_estimators=900, max_depth=4, max_features='log2', random_state=456,learning_rate=0.08)\n",
    "gbdt3 = GradientBoostingRegressor(n_estimators=1000, max_depth=5, max_features='log2', random_state=789,learning_rate=0.08)\n",
    "# n_estimators=700, max_features='auto', random_state=2, n_jobs=8,max_depth=10\n",
    "forest1 = RandomForestRegressor(n_estimators=800, max_features='sqrt', random_state=7, n_jobs=8)\n",
    "forest2 = RandomForestRegressor(n_estimators=900, max_features='log2', random_state=9, n_jobs=8)\n",
    "forest3 = RandomForestRegressor(n_estimators=900, max_features='sqrt', random_state=11) \n",
    "\n",
    "lgb1 = LGBMRegressor(n_estimators=900, max_depth=5, random_state=5, n_jobs=8) \n",
    "lgb2 = LGBMRegressor(n_estimators=850, max_depth=4, random_state=7, n_jobs=8)\n",
    "lgb3 = LGBMRegressor(n_estimators=720, max_depth=4, random_state=9)\n",
    "# lgb3 = LGBMRegressor(n_estimators=1000, max_depth=5, random_state=9, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.028317\tvalid's score: 0.855962\n",
      "Early stopping, best iteration is:\n",
      "[716]\tvalid's l2: 0.0269006\tvalid's score: 0.859096\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0170063\tvalid's score: 0.884636\n",
      "Early stopping, best iteration is:\n",
      "[530]\tvalid's l2: 0.0168919\tvalid's score: 0.88498\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid's l2: 0.0196453\tvalid's score: 0.877069\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid's l2: 0.0143643\tvalid's score: 0.892976\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.100913\tvalid's score: 0.758917\n",
      "Early stopping, best iteration is:\n",
      "[500]\tvalid's l2: 0.100913\tvalid's score: 0.758917\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0177179\tvalid's score: 0.882528\n",
      "[1000]\tvalid's l2: 0.0167863\tvalid's score: 0.885299\n",
      "Early stopping, best iteration is:\n",
      "[1117]\tvalid's l2: 0.0166689\tvalid's score: 0.885655\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalid's l2: 0.0215595\tvalid's score: 0.871968\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0219086\tvalid's score: 0.871068\n",
      "[1000]\tvalid's l2: 0.0200636\tvalid's score: 0.875928\n",
      "[1500]\tvalid's l2: 0.0196959\tvalid's score: 0.87693\n",
      "Early stopping, best iteration is:\n",
      "[1618]\tvalid's l2: 0.0196703\tvalid's score: 0.877\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0144012\tvalid's score: 0.892853\n",
      "[1000]\tvalid's l2: 0.0139775\tvalid's score: 0.894273\n",
      "Early stopping, best iteration is:\n",
      "[986]\tvalid's l2: 0.013932\tvalid's score: 0.894427\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0162382\tvalid's score: 0.886974\n",
      "[1000]\tvalid's l2: 0.0154013\tvalid's score: 0.889599\n",
      "Early stopping, best iteration is:\n",
      "[921]\tvalid's l2: 0.0153431\tvalid's score: 0.889785\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0174276\tvalid's score: 0.883382\n",
      "[1000]\tvalid's l2: 0.0157779\tvalid's score: 0.888407\n",
      "[1500]\tvalid's l2: 0.0155225\tvalid's score: 0.889213\n",
      "Early stopping, best iteration is:\n",
      "[1420]\tvalid's l2: 0.0154833\tvalid's score: 0.889338\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0349555\tvalid's score: 0.842486\n",
      "[1000]\tvalid's l2: 0.028846\tvalid's score: 0.854817\n",
      "[1500]\tvalid's l2: 0.0272959\tvalid's score: 0.858211\n",
      "[2000]\tvalid's l2: 0.026924\tvalid's score: 0.859044\n",
      "Early stopping, best iteration is:\n",
      "[1954]\tvalid's l2: 0.0268993\tvalid's score: 0.859099\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0224656\tvalid's score: 0.869652\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid's l2: 0.0220942\tvalid's score: 0.870594\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.021651\tvalid's score: 0.871731\n",
      "[1000]\tvalid's l2: 0.0198211\tvalid's score: 0.876587\n",
      "[1500]\tvalid's l2: 0.0193725\tvalid's score: 0.877821\n",
      "Early stopping, best iteration is:\n",
      "[1467]\tvalid's l2: 0.019348\tvalid's score: 0.877888\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's l2: 0.0140822\tvalid's score: 0.89392\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[156]\tvalid's l2: 0.0125473\tvalid's score: 0.899269\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid's l2: 0.0250467\tvalid's score: 0.863363\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.105217\tvalid's score: 0.755075\n",
      "[1000]\tvalid's l2: 0.104496\tvalid's score: 0.75571\n",
      "Early stopping, best iteration is:\n",
      "[927]\tvalid's l2: 0.10441\tvalid's score: 0.755786\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.021356\tvalid's score: 0.872496\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid's l2: 0.0210151\tvalid's score: 0.873388\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.01209\tvalid's score: 0.900938\n",
      "Early stopping, best iteration is:\n",
      "[733]\tvalid's l2: 0.0119442\tvalid's score: 0.901478\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[309]\tvalid's l2: 0.0134986\tvalid's score: 0.89591\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's l2: 0.0161031\tvalid's score: 0.887392\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid's l2: 0.0276213\tvalid's score: 0.857488\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0990206\tvalid's score: 0.760644\n",
      "[1000]\tvalid's l2: 0.0971745\tvalid's score: 0.762353\n",
      "Early stopping, best iteration is:\n",
      "[946]\tvalid's l2: 0.0971273\tvalid's score: 0.762397\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0201504\tvalid's score: 0.875693\n",
      "Early stopping, best iteration is:\n",
      "[434]\tvalid's l2: 0.0200408\tvalid's score: 0.87599\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0254953\tvalid's score: 0.862312\n",
      "Early stopping, best iteration is:\n",
      "[850]\tvalid's l2: 0.0251429\tvalid's score: 0.863137\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0178453\tvalid's score: 0.882156\n",
      "Early stopping, best iteration is:\n",
      "[442]\tvalid's l2: 0.0177521\tvalid's score: 0.882428\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[218]\tvalid's l2: 0.0146697\tvalid's score: 0.891966\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0222747\tvalid's score: 0.870135\n",
      "[1000]\tvalid's l2: 0.0184929\tvalid's score: 0.880291\n",
      "[1500]\tvalid's l2: 0.0175334\tvalid's score: 0.883069\n",
      "Early stopping, best iteration is:\n",
      "[1746]\tvalid's l2: 0.0173241\tvalid's score: 0.883688\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.019358\tvalid's score: 0.87786\n",
      "Early stopping, best iteration is:\n",
      "[447]\tvalid's l2: 0.0191166\tvalid's score: 0.878532\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0133358\tvalid's score: 0.896474\n",
      "Early stopping, best iteration is:\n",
      "[548]\tvalid's l2: 0.0132297\tvalid's score: 0.896845\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0160694\tvalid's score: 0.887497\n",
      "Early stopping, best iteration is:\n",
      "[839]\tvalid's l2: 0.0158476\tvalid's score: 0.888188\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0312288\tvalid's score: 0.849822\n",
      "[1000]\tvalid's l2: 0.0284942\tvalid's score: 0.855577\n",
      "[1500]\tvalid's l2: 0.0279167\tvalid's score: 0.856837\n",
      "Early stopping, best iteration is:\n",
      "[1478]\tvalid's l2: 0.0278834\tvalid's score: 0.85691\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0206064\tvalid's score: 0.87447\n",
      "Early stopping, best iteration is:\n",
      "[446]\tvalid's l2: 0.0204552\tvalid's score: 0.874874\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0187404\tvalid's score: 0.879588\n",
      "Early stopping, best iteration is:\n",
      "[659]\tvalid's l2: 0.0185601\tvalid's score: 0.880099\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid's l2: 0.0188663\tvalid's score: 0.879233\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid's l2: 0.0223444\tvalid's score: 0.869958\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid's l2: 0.0978129\tvalid's score: 0.761759\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0180312\tvalid's score: 0.881616\n",
      "[1000]\tvalid's l2: 0.0170108\tvalid's score: 0.884623\n",
      "Early stopping, best iteration is:\n",
      "[1263]\tvalid's l2: 0.0167736\tvalid's score: 0.885337\n",
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid's l2: 0.0220507\tvalid's score: 0.870705\n",
      "Early stopping, best iteration is:\n",
      "[773]\tvalid's l2: 0.0217678\tvalid's score: 0.87143\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0181409\tvalid's score: 0.881299\n",
      "Early stopping, best iteration is:\n",
      "[858]\tvalid's l2: 0.0175545\tvalid's score: 0.883007\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0147592\tvalid's score: 0.891673\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid's l2: 0.0146343\tvalid's score: 0.892083\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0192833\tvalid's score: 0.878068\n",
      "Early stopping, best iteration is:\n",
      "[612]\tvalid's l2: 0.0189874\tvalid's score: 0.878893\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[327]\tvalid's l2: 0.0222433\tvalid's score: 0.870215\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid's l2: 0.022125\tvalid's score: 0.870515\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.025068\tvalid's score: 0.863313\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid's l2: 0.0244605\tvalid's score: 0.864754\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\tvalid's l2: 0.0993972\tvalid's score: 0.760298\n",
      "[1000]\tvalid's l2: 0.0979368\tvalid's score: 0.761644\n",
      "Early stopping, best iteration is:\n",
      "[1061]\tvalid's l2: 0.0978462\tvalid's score: 0.761728\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's l2: 0.0144476\tvalid's score: 0.892699\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid's l2: 0.0243805\tvalid's score: 0.864945\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalid's l2: 0.0176548\tvalid's score: 0.882713\n",
      "local cv: 0.8628893604241332\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "import lightgbm\n",
    "\n",
    "def my_val(preds, train_data):\n",
    "    label = train_data.get_label()\n",
    "    return 'score', 1/(1+np.sqrt(mean_squared_error(preds, label))), True\n",
    "def my_obj(preds, train_data):\n",
    "    labels = train_deata.get_label()\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.08,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "# 同样的条件下，此参数设置达到local cv: 0.8575832021441638\n",
    "\n",
    "# train = train_data.copy()\n",
    "# test = test_X.copy()\n",
    "\n",
    "\n",
    "test_predicts = []\n",
    "val_preds = []\n",
    "\n",
    "# log_test_predicts = []\n",
    "# log_val_predicts = []\n",
    "for idx, seed in enumerate([1,2,3,4,5]):\n",
    "# for idx, seed in enumerate([1]):\n",
    "    kf = KFold(10, shuffle=True, random_state=seed)\n",
    "    \n",
    "    val_preds.append(np.zeros(train_X.shape[0]))\n",
    "    for n_fold, (tra_idx, val_idx) in enumerate(kf.split(train_X)):\n",
    "#         tra = train.iloc[tra_idx]\n",
    "#         val = train.iloc[val_idx]\n",
    "#         tst = test.copy()\n",
    "\n",
    "#         predictor = [c for c in tra.columns.tolist() if c not in['y','I_B','I_C','is_train']]\n",
    "\n",
    "        train_set = lightgbm.Dataset(\n",
    "            train_X.iloc[tra_idx],\n",
    "            train_Y.iloc[tra_idx]['y']\n",
    "        )\n",
    "\n",
    "        validation_set = lightgbm.Dataset(\n",
    "            train_X.iloc[val_idx],\n",
    "            train_Y.iloc[val_idx]['y']\n",
    "        )\n",
    "\n",
    "        model = lightgbm.train(params, train_set, num_boost_round=5000,\n",
    "                              valid_sets= [validation_set],\n",
    "                              valid_names=['valid'],\n",
    "                              early_stopping_rounds=100,\n",
    "                               feval=my_val,\n",
    "                              verbose_eval=500)\n",
    "\n",
    "        val_preds[idx][val_idx] = model.predict(train_X.iloc[val_idx])\n",
    "        test_predicts.append(model.predict(test_X))\n",
    "\n",
    "print('local cv:',1/(1+np.sqrt(mean_squared_error(train_Y['y'],np.mean(val_preds,axis=0)))))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(train_y, predict_y ):\n",
    "    print('local cv:',1/(1+np.sqrt(mean_squared_error(train_y, predict_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练得到的模型预测Test_X\n",
    "wyh_train = np.mean(val_preds,axis=0)\n",
    "wyh_predict = np.mean(test_predicts,axis=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = [\n",
    "    xgbt1, gbdt1, forest1, lgb1,\n",
    "    xgbt2, gbdt2, forest2, lgb2,\n",
    "    xgbt3, gbdt3, forest3, lgb3\n",
    "]\n",
    "\n",
    "regrs_light = [\n",
    "    lgb3, xgbt3, gbdt3, forest3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(models=regrs_light, X = train_X, Y = train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(models=regrs_light, X = train_X, Y = train_Y['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacker(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    \n",
    "    # Train_X: 原始训练集输入矩阵, Train_Y: 原始训练集输出矩阵, Test_X: 原始测试集输入矩阵\n",
    "    def fit_predict(self, Train_X, Train_Y, Test_X):\n",
    "        Train_X = np.array(Train_X)\n",
    "        Train_Y = np.array(Train_Y)\n",
    "        Test_X = np.array(Test_X)\n",
    "\n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, random_state=2018).split(Train_X, Train_Y))       \n",
    "        \n",
    "        # 以基学习器预测结果为特征的 stacker训练数据 与 stacker预测数据\n",
    "        # 原始训练集预测结果容器\n",
    "        S_train = np.zeros((Train_X.shape[0], len(self.base_models)))\n",
    "        # 原始测试集预测结果容器\n",
    "        S_predict = np.zeros((Test_X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        \n",
    "        model_name, score = [], []\n",
    "        \n",
    "        for n_model, regr in enumerate(self.base_models):\n",
    "            name = str(n_model + 1) + '.' + str(regr)\n",
    "#             print(n_model + 1, 'Base model:', str(regr).split('(')[0])\n",
    "            model_name.append(name.split('(')[0])\n",
    "            print(name)\n",
    "            S_predict_i = np.zeros((Test_X.shape[0], self.n_splits))\n",
    "            \n",
    "            for n_fold, (train_idx, test_idx) in enumerate(folds):\n",
    "                # 将X分为训练集与测试集\n",
    "                X_train_fold, Y_train_fold, X_test_fold, Y_test_fold = Train_X[train_idx], Train_Y[train_idx], Train_X[test_idx], Train_Y[test_idx]\n",
    "                print ('Fit fold', (n_fold+1), '...')\n",
    "                regr.fit(X_train_fold, Y_train_fold)\n",
    "                Y_pred = regr.predict(X_test_fold)\n",
    "                # 每折训练得到的模型根据原始训练集中的测试折的输入矩阵预测\n",
    "                S_train[test_idx, n_model] = Y_pred\n",
    "                # 每折训练得到的模型根据原始测试集输入矩阵预测\n",
    "                S_predict_i[:, n_fold] = regr.predict(Test_X)\n",
    "            \n",
    "            S_predict[:, n_model] = S_predict_i.mean(axis=1)\n",
    "            score_i = 1/(1+np.sqrt(mean_squared_error(Train_Y, S_train[:, n_model])))\n",
    "            score.append(score_i)\n",
    "            \n",
    "\n",
    "#         nmse_score = cross_val_score(self.stacker, S_train, Train_Y, cv=5, scoring='neg_mean_squared_error')\n",
    "#         print('CV MSE:', -nmse_score)\n",
    "#         print('Stacker AVG MSE:', -nmse_score.mean(), 'Stacker AVG Score:', np.mean(np.divide(1, 1 + np.sqrt(-nmse_score))))\n",
    "\n",
    "        \n",
    "#         self.stacker.fit(S_train, Train_Y)\n",
    "#         res = self.stacker.predict(S_predict)\n",
    "#         return res, S_train, S_predict\n",
    "        res_table = pd.DataFrame()\n",
    "        res_table['Model'] = model_name\n",
    "        res_table['Score'] = score\n",
    "\n",
    "        return S_train, S_predict, res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "       learning_rate=0.1, max_depth=4, min_child_samples=20,\n",
      "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=720,\n",
      "       n_jobs=-1, num_leaves=31, objective=None, random_state=9,\n",
      "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "       subsample_for_bin=200000, subsample_freq=0)\n",
      "Fit fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, max_features='sqrt', min_child_weight=1, missing=None,\n",
      "       n_estimators=1100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=789, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1)\n",
      "Fit fold 1 ...\n",
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "Fit fold 6 ...\n",
      "Fit fold 7 ...\n",
      "Fit fold 8 ...\n",
      "Fit fold 9 ...\n",
      "Fit fold 10 ...\n",
      "3.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.08, loss='ls', max_depth=5,\n",
      "             max_features='log2', max_leaf_nodes=None,\n",
      "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "             presort='auto', random_state=789, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Fit fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 2 ...\n",
      "Fit fold 3 ...\n",
      "Fit fold 4 ...\n",
      "Fit fold 5 ...\n",
      "Fit fold 6 ...\n",
      "Fit fold 7 ...\n",
      "Fit fold 8 ...\n",
      "Fit fold 9 ...\n",
      "Fit fold 10 ...\n",
      "4.RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=900, n_jobs=1,\n",
      "           oob_score=False, random_state=11, verbose=0, warm_start=False)\n",
      "Fit fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit fold 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_train:\n",
      " [[ 1.48810949  1.49961329  1.47679289  1.5959027 ]\n",
      " [ 1.72025262  1.67167699  1.71888866  1.74700498]\n",
      " [ 2.05450113  2.10437346  1.97131897  2.0349647 ]\n",
      " ...\n",
      " [10.34185434 10.3502779  10.26892987 10.34740259]\n",
      " [ 9.56665643  9.59838009  9.67723723  9.63561366]\n",
      " [ 9.07690899  9.18895817  9.15880149  9.31350001]] \n",
      " S_predict:\n",
      " [[0.53162361 0.41583386 0.52710884 0.39572983]\n",
      " [1.3197746  1.33778083 1.30854553 1.40115571]\n",
      " [2.13909506 2.15013087 2.13960081 2.19128228]\n",
      " ...\n",
      " [9.93801402 9.91859198 9.92932764 9.90987767]\n",
      " [9.87606076 9.90655937 9.849855   9.78472577]\n",
      " [9.06363481 9.10178289 9.07155724 9.11881196]]\n"
     ]
    }
   ],
   "source": [
    "# Stack with 4 models\n",
    "\n",
    "stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "stacker = Stacker(10, stacking_model, regrs_light)\n",
    "S_train, S_predict, table = stacker.fit_predict(train_X, train_Y, test_X)\n",
    "\n",
    "print('S_train:\\n',S_train,'\\n','S_predict:\\n',S_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.856319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.859571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.858816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.851396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model     Score\n",
       "0              1.LGBMRegressor  0.856319\n",
       "1               2.XGBRegressor  0.859571\n",
       "2  3.GradientBoostingRegressor  0.858816\n",
       "3      4.RandomForestRegressor  0.851396"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGBM</th>\n",
       "      <th>GBM</th>\n",
       "      <th>RF</th>\n",
       "      <th>WYH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.488109</td>\n",
       "      <td>1.499613</td>\n",
       "      <td>1.476793</td>\n",
       "      <td>1.595903</td>\n",
       "      <td>1.545617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.720253</td>\n",
       "      <td>1.671677</td>\n",
       "      <td>1.718889</td>\n",
       "      <td>1.747005</td>\n",
       "      <td>1.721858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.054501</td>\n",
       "      <td>2.104373</td>\n",
       "      <td>1.971319</td>\n",
       "      <td>2.034965</td>\n",
       "      <td>1.962870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.356387</td>\n",
       "      <td>2.266027</td>\n",
       "      <td>2.345125</td>\n",
       "      <td>2.422188</td>\n",
       "      <td>2.405345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.517721</td>\n",
       "      <td>2.495417</td>\n",
       "      <td>2.507623</td>\n",
       "      <td>2.631366</td>\n",
       "      <td>2.549975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LGBM      XGBM       GBM        RF       WYH\n",
       "0  1.488109  1.499613  1.476793  1.595903  1.545617\n",
       "1  1.720253  1.671677  1.718889  1.747005  1.721858\n",
       "2  2.054501  2.104373  1.971319  2.034965  1.962870\n",
       "3  2.356387  2.266027  2.345125  2.422188  2.405345\n",
       "4  2.517721  2.495417  2.507623  2.631366  2.549975"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_train=pd.DataFrame()\n",
    "stack_train['LGBM']=S_train[:,0]\n",
    "stack_train['XGBM']=S_train[:,1]\n",
    "stack_train['GBM']=S_train[:,2]\n",
    "stack_train['RF']=S_train[:,3]\n",
    "stack_train['WYH']=wyh_train\n",
    "stack_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGBM</th>\n",
       "      <th>XGBM</th>\n",
       "      <th>GBM</th>\n",
       "      <th>RF</th>\n",
       "      <th>WYH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.531624</td>\n",
       "      <td>0.415834</td>\n",
       "      <td>0.527109</td>\n",
       "      <td>0.395730</td>\n",
       "      <td>0.566327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.319775</td>\n",
       "      <td>1.337781</td>\n",
       "      <td>1.308546</td>\n",
       "      <td>1.401156</td>\n",
       "      <td>1.321474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.139095</td>\n",
       "      <td>2.150131</td>\n",
       "      <td>2.139601</td>\n",
       "      <td>2.191282</td>\n",
       "      <td>2.130599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.458713</td>\n",
       "      <td>3.428675</td>\n",
       "      <td>3.378112</td>\n",
       "      <td>3.401181</td>\n",
       "      <td>3.423788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.687951</td>\n",
       "      <td>3.690193</td>\n",
       "      <td>3.642646</td>\n",
       "      <td>3.645653</td>\n",
       "      <td>3.663321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LGBM      XGBM       GBM        RF       WYH\n",
       "0  0.531624  0.415834  0.527109  0.395730  0.566327\n",
       "1  1.319775  1.337781  1.308546  1.401156  1.321474\n",
       "2  2.139095  2.150131  2.139601  2.191282  2.130599\n",
       "3  3.458713  3.428675  3.378112  3.401181  3.423788\n",
       "4  3.687951  3.690193  3.642646  3.645653  3.663321"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_predict=pd.DataFrame()\n",
    "stack_predict['LGBM']=S_predict[:,0]\n",
    "stack_predict['XGBM']=S_predict[:,1]\n",
    "stack_predict['GBM']=S_predict[:,2]\n",
    "stack_predict['RF']=S_predict[:,3]\n",
    "stack_predict['WYH']=wyh_predict\n",
    "stack_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4591.000000\n",
       "mean        3.690791\n",
       "std         2.740833\n",
       "min         0.000234\n",
       "25%         1.366254\n",
       "50%         3.111472\n",
       "75%         5.661994\n",
       "max        11.968650\n",
       "Name: WYH_bias, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare = stack_train.copy()\n",
    "model_compare['y'] = train_Y['y']\n",
    "\n",
    "for c in ['LGBM','XGBM','GBM','RF','WYH']:\n",
    "    model_compare[c+'_bias'] = np.abs(model_compare[c]-model_compare['y'])\n",
    "\n",
    "\n",
    "model_compare['WYH_bias'].hist(bins=40)\n",
    "model_compare['WYH_bias'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f53e9a9828>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare['WYH_LGBM']=model_compare['WYH_bias']-model_compare['LGBM_bias']\n",
    "model_compare['WYH_LGBM'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f53ea2fa20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare['WYH_XGBM']=model_compare['WYH_bias']-model_compare['XGBM_bias']\n",
    "model_compare['WYH_XGBM'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f53eb16940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare['WYH_GBM']=model_compare['WYH_bias']-model_compare['GBM_bias']\n",
    "model_compare['WYH_GBM'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f53ed34cc0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_compare['WYH_RF']=model_compare['WYH_bias']-model_compare['RF_bias']\n",
    "model_compare['WYH_RF'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.01, gamma=0.01,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "train_idx: \n",
      " 8026 [   0    1    2 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   3   13   16   18   20   25   32   37   53   54   55   62   65   90\n",
      "   97  107  121  131  141  144  145  152  154  159  184  220  239  252\n",
      "  268  273  291  296  297  302  303  311  317  329  339  350  353  366\n",
      "  374  376  395  397  398  424  431  434  436  440  457  478  486  491\n",
      "  506  508  520  524  549  551  566  587  594  603  630  644  647  649\n",
      "  658  671  674  680  686  700  703  714  730  740  778  779  781  784\n",
      "  785  786  798  860  870  893  899  902  944  989 1001 1014 1020 1043\n",
      " 1064 1071 1088 1089 1093 1111 1118 1134 1145 1146 1152 1162 1166 1171\n",
      " 1245 1253 1254 1265 1284 1287 1294 1300 1322 1334 1340 1348 1357 1391\n",
      " 1398 1399 1416 1442 1449 1480 1481 1499 1502 1505 1509 1516 1541 1547\n",
      " 1552 1553 1555 1559 1560 1566 1568 1569 1572 1631 1633 1638 1647 1660\n",
      " 1661 1668 1678 1680 1685 1692 1704 1716 1719 1723 1730 1743 1744 1754\n",
      " 1780 1781 1791 1792 1813 1826 1828 1837 1840 1845 1851 1852 1857 1860\n",
      " 1868 1870 1876 1893 1922 1929 1931 1950 1951 1958 1968 1986 1995 2001\n",
      " 2040 2043 2046 2047 2048 2051 2055 2056 2071 2082 2083 2096 2104 2145\n",
      " 2161 2164 2172 2183 2197 2201 2207 2229 2232 2255 2261 2279 2290 2301\n",
      " 2309 2310 2322 2332 2337 2338 2346 2349 2354 2362 2384 2398 2403 2424\n",
      " 2425 2432 2452 2468 2497 2498 2510 2513 2528 2547 2550 2554 2560 2574\n",
      " 2575 2582 2589 2600 2603 2645 2647 2662 2675 2680 2685 2689 2691 2699\n",
      " 2711 2715 2758 2786 2790 2800 2811 2823 2827 2840 2846 2862 2878 2885\n",
      " 2894 2899 2909 2911 2921 2938 2939 2940 2941 2949 2971 2981 2985 2990\n",
      " 2997 3015 3037 3050 3067 3069 3074 3098 3107 3111 3118 3128 3137 3147\n",
      " 3150 3153 3158 3160 3175 3178 3184 3198 3244 3248 3257 3265 3285 3287\n",
      " 3297 3302 3312 3370 3372 3376 3381 3386 3399 3410 3411 3418 3449 3451\n",
      " 3490 3512 3516 3517 3520 3528 3536 3538 3559 3562 3564 3570 3614 3618\n",
      " 3626 3629 3639 3642 3648 3666 3668 3676 3678 3697 3711 3727 3744 3747\n",
      " 3756 3761 3762 3776 3782 3799 3802 3810 3822 3830 3833 3853 3855 3859\n",
      " 3864 3869 3872 3890 3899 3913 3926 3927 3942 3951 3954 3968 3978 3983\n",
      " 3985 3995 3997 4006 4007 4020 4035 4047 4065 4071 4085 4102 4105 4106\n",
      " 4125 4134 4135 4144 4151 4155 4158 4181 4184 4185 4188 4190 4192 4193\n",
      " 4196 4200 4203 4206 4209 4212 4215 4232 4246 4253 4254 4260 4273 4275\n",
      " 4283 4294 4334 4341 4381 4395 4396 4404 4405 4411 4415 4436 4437 4450\n",
      " 4465 4469 4475 4476 4500 4511 4512 4519 4520 4530 4538 4546 4553 4556\n",
      " 4562 4569 4570 4571 4579 4587 4590 4604 4620 4631 4655 4673 4681 4685\n",
      " 4701 4715 4721 4733 4742 4745 4752 4760 4789 4799 4805 4807 4815 4838\n",
      " 4841 4849 4851 4865 4866 4873 4874 4881 4887 4895 4899 4903 4913 4914\n",
      " 4915 4942 4959 4980 4989 4993 5008 5017 5022 5034 5035 5048 5069 5072\n",
      " 5093 5095 5102 5108 5111 5124 5146 5190 5202 5206 5210 5220 5223 5231\n",
      " 5233 5239 5241 5250 5252 5262 5267 5268 5271 5285 5306 5311 5316 5321\n",
      " 5340 5343 5356 5360 5417 5419 5424 5439 5446 5450 5466 5487 5513 5519\n",
      " 5527 5531 5536 5542 5549 5570 5576 5592 5614 5639 5641 5643 5649 5653\n",
      " 5658 5665 5674 5688 5698 5699 5706 5720 5743 5751 5757 5790 5791 5809\n",
      " 5820 5830 5850 5866 5876 5878 5881 5886 5891 5913 5918 5924 5926 5930\n",
      " 5933 5939 5942 5946 5968 5994 6009 6028 6034 6046 6062 6073 6074 6079\n",
      " 6082 6085 6103 6104 6111 6116 6122 6127 6145 6148 6151 6154 6195 6207\n",
      " 6214 6229 6231 6237 6238 6258 6260 6262 6265 6266 6274 6291 6296 6319\n",
      " 6349 6361 6370 6401 6405 6416 6417 6421 6436 6443 6449 6459 6462 6477\n",
      " 6501 6525 6550 6568 6595 6603 6609 6617 6618 6623 6639 6658 6674 6697\n",
      " 6700 6752 6773 6775 6776 6795 6799 6825 6834 6841 6855 6874 6876 6889\n",
      " 6909 6910 6914 6930 6931 6938 6982 6991 7011 7015 7017 7020 7021 7022\n",
      " 7029 7034 7037 7043 7052 7062 7114 7134 7143 7146 7153 7168 7175 7176\n",
      " 7178 7181 7188 7213 7214 7216 7225 7260 7287 7291 7293 7299 7302 7303\n",
      " 7304 7305 7308 7333 7357 7367 7400 7401 7406 7422 7434 7435 7444 7445\n",
      " 7450 7451 7460 7478 7491 7495 7498 7509 7511 7524 7534 7543 7544 7548\n",
      " 7575 7579 7587 7588 7601 7613 7617 7627 7646 7658 7683 7697 7699 7715\n",
      " 7718 7719 7725 7734 7759 7768 7769 7793 7822 7833 7839 7847 7848 7855\n",
      " 7868 7871 7881 7882 7887 7888 7893 7914 7928 7932 7934 7969 7971 8000\n",
      " 8009 8018 8024 8041 8051 8052 8057 8069 8072 8099 8109 8111 8118 8125\n",
      " 8132 8136 8142 8154 8156 8160 8171 8174 8175 8202 8205 8217 8240 8245\n",
      " 8254 8257 8259 8260 8266 8277 8281 8288 8289 8290 8314 8351 8353 8355\n",
      " 8359 8360 8361 8386 8390 8395 8396 8397 8413 8420 8433 8438 8444 8461\n",
      " 8472 8483 8484 8549 8563 8566 8574 8582 8589 8594 8601 8604 8608 8611\n",
      " 8619 8635 8638 8651 8659 8660 8666 8691 8734 8737 8747 8750 8757 8765\n",
      " 8767 8774 8791 8805 8815 8822 8841 8846 8878 8903]\n",
      "Y_pred: \n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      "train_idx: \n",
      " 8026 [   0    1    2 ... 8914 8915 8916]\n",
      "val_idx: \n",
      " 892 [  10   12   34   35   41   61   75   79   82   86   89   91   96   98\n",
      "   99  101  110  136  143  148  171  175  176  181  189  213  234  235\n",
      "  243  245  274  282  285  290  294  310  319  343  356  357  361  373\n",
      "  377  386  411  416  443  446  481  485  496  497  498  499  514  517\n",
      "  521  552  568  585  597  610  612  616  633  636  650  652  656  659\n",
      "  695  701  748  758  773  774  787  790  794  811  812  816  820  832\n",
      "  833  838  843  851  853  856  857  871  873  883  887  904  926  929\n",
      "  930  943  994 1015 1016 1017 1018 1022 1024 1030 1031 1033 1034 1044\n",
      " 1045 1048 1049 1058 1076 1084 1086 1103 1113 1116 1150 1164 1174 1176\n",
      " 1181 1182 1183 1187 1190 1196 1204 1220 1221 1223 1233 1237 1238 1275\n",
      " 1291 1299 1302 1319 1326 1329 1341 1343 1353 1354 1361 1364 1381 1387\n",
      " 1388 1393 1394 1407 1417 1424 1437 1440 1445 1452 1472 1476 1478 1492\n",
      " 1511 1513 1519 1525 1530 1570 1582 1598 1627 1648 1653 1663 1664 1674\n",
      " 1684 1695 1696 1697 1705 1711 1735 1741 1745 1763 1777 1821 1830 1831\n",
      " 1872 1890 1894 1901 1924 1935 1956 1966 1972 1989 2004 2005 2012 2028\n",
      " 2037 2038 2052 2060 2080 2084 2101 2102 2110 2112 2115 2117 2128 2131\n",
      " 2150 2153 2167 2189 2195 2203 2204 2208 2213 2220 2221 2224 2230 2268\n",
      " 2271 2274 2293 2297 2307 2323 2343 2369 2370 2371 2408 2414 2417 2429\n",
      " 2431 2433 2442 2445 2446 2449 2451 2465 2466 2477 2487 2490 2496 2523\n",
      " 2526 2532 2548 2558 2561 2562 2571 2573 2578 2595 2601 2616 2618 2644\n",
      " 2650 2654 2671 2681 2684 2687 2708 2710 2712 2717 2718 2722 2726 2735\n",
      " 2743 2761 2773 2776 2778 2780 2783 2784 2799 2802 2807 2813 2818 2828\n",
      " 2834 2836 2860 2863 2893 2897 2905 2915 2918 2925 2935 2998 2999 3002\n",
      " 3005 3020 3042 3045 3076 3088 3105 3119 3124 3127 3129 3146 3156 3162\n",
      " 3181 3205 3212 3218 3228 3230 3233 3251 3262 3267 3278 3300 3345 3366\n",
      " 3380 3389 3420 3423 3438 3440 3450 3458 3465 3470 3505 3509 3511 3519\n",
      " 3521 3523 3526 3535 3547 3548 3565 3569 3577 3579 3581 3586 3604 3619\n",
      " 3627 3636 3643 3650 3664 3673 3683 3689 3710 3713 3728 3749 3751 3765\n",
      " 3767 3769 3783 3784 3788 3795 3797 3811 3812 3829 3832 3842 3868 3874\n",
      " 3892 3898 3907 3933 3947 3949 3960 3992 4003 4025 4026 4052 4059 4086\n",
      " 4088 4099 4127 4139 4169 4170 4171 4189 4220 4233 4248 4249 4280 4282\n",
      " 4284 4287 4292 4298 4309 4333 4345 4346 4351 4353 4365 4368 4378 4384\n",
      " 4397 4400 4401 4416 4452 4467 4468 4483 4501 4504 4514 4517 4525 4540\n",
      " 4543 4547 4582 4594 4595 4606 4609 4635 4658 4659 4660 4693 4743 4769\n",
      " 4774 4781 4803 4812 4832 4834 4835 4869 4879 4880 4889 4894 4900 4902\n",
      " 4906 4917 4929 4941 4951 4956 4962 4963 4964 4983 4987 5009 5018 5023\n",
      " 5024 5030 5050 5052 5100 5103 5109 5117 5118 5119 5120 5122 5148 5152\n",
      " 5158 5181 5182 5188 5195 5204 5227 5229 5230 5232 5237 5238 5251 5284\n",
      " 5290 5296 5303 5304 5341 5350 5385 5392 5393 5394 5403 5427 5428 5436\n",
      " 5471 5472 5478 5482 5484 5497 5501 5502 5508 5514 5524 5545 5556 5563\n",
      " 5573 5577 5579 5584 5586 5593 5600 5617 5624 5635 5659 5671 5685 5697\n",
      " 5704 5707 5712 5714 5729 5736 5739 5747 5748 5752 5753 5760 5765 5770\n",
      " 5796 5816 5825 5845 5848 5872 5905 5948 5953 5972 5974 5976 5987 6003\n",
      " 6014 6021 6026 6037 6050 6059 6060 6076 6080 6094 6096 6126 6128 6136\n",
      " 6143 6150 6164 6170 6178 6180 6183 6192 6201 6202 6205 6218 6222 6232\n",
      " 6233 6236 6243 6247 6261 6263 6264 6275 6292 6300 6307 6331 6340 6350\n",
      " 6365 6373 6385 6386 6393 6395 6413 6414 6423 6431 6453 6493 6518 6531\n",
      " 6544 6555 6561 6562 6566 6588 6615 6630 6631 6646 6656 6664 6669 6671\n",
      " 6695 6698 6702 6704 6710 6719 6723 6727 6742 6746 6759 6787 6802 6810\n",
      " 6814 6839 6850 6857 6858 6866 6877 6894 6895 6907 6945 6971 6977 6983\n",
      " 6986 7030 7033 7050 7054 7055 7061 7069 7073 7088 7090 7108 7118 7119\n",
      " 7133 7142 7154 7166 7170 7179 7180 7184 7186 7190 7191 7234 7238 7239\n",
      " 7248 7251 7283 7309 7313 7327 7354 7359 7360 7363 7376 7381 7384 7389\n",
      " 7399 7411 7414 7447 7467 7468 7469 7474 7475 7503 7504 7518 7527 7532\n",
      " 7536 7539 7540 7552 7560 7566 7572 7577 7578 7584 7592 7614 7620 7621\n",
      " 7622 7625 7628 7631 7650 7654 7659 7664 7681 7689 7691 7700 7701 7704\n",
      " 7712 7723 7736 7744 7762 7785 7786 7788 7790 7815 7818 7832 7835 7837\n",
      " 7858 7886 7922 7929 7940 7951 7972 7979 7985 7988 8002 8021 8067 8073\n",
      " 8079 8084 8086 8092 8106 8110 8137 8141 8148 8158 8170 8180 8182 8193\n",
      " 8204 8213 8232 8235 8236 8244 8273 8313 8316 8326 8366 8372 8373 8374\n",
      " 8375 8399 8404 8414 8415 8426 8434 8436 8463 8480 8498 8502 8505 8506\n",
      " 8512 8513 8524 8525 8526 8558 8560 8562 8568 8570 8581 8595 8653 8680\n",
      " 8696 8703 8713 8718 8727 8730 8756 8781 8783 8799 8801 8827 8843 8849\n",
      " 8860 8875 8886 8887 8889 8890 8896 8897 8911 8917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: \n",
      " [0.         0.         0.         ... 0.         0.         9.16150331]\n",
      "train_idx: \n",
      " 8026 [   1    2    3 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   0   24   56   64   71   73   83   87  112  122  124  128  133  140\n",
      "  146  151  163  164  174  182  197  202  215  216  219  230  248  272\n",
      "  276  295  298  306  325  326  346  349  354  359  360  364  369  379\n",
      "  406  414  418  422  426  439  461  474  479  500  502  503  522  527\n",
      "  534  541  553  563  573  575  576  582  595  601  611  614  631  643\n",
      "  648  655  660  663  672  676  705  706  709  712  719  720  722  728\n",
      "  733  737  750  752  756  757  761  768  795  813  821  846  854  855\n",
      "  867  875  878  885  896  916  928  937  948  952  956  969  993  995\n",
      "  999 1011 1038 1065 1078 1087 1099 1101 1106 1117 1121 1131 1144 1154\n",
      " 1155 1163 1170 1173 1203 1214 1247 1251 1258 1286 1298 1304 1313 1315\n",
      " 1328 1332 1362 1366 1367 1376 1402 1406 1408 1411 1423 1425 1434 1435\n",
      " 1436 1466 1468 1487 1489 1503 1504 1514 1521 1531 1538 1539 1543 1548\n",
      " 1587 1616 1630 1641 1645 1649 1673 1686 1707 1722 1727 1739 1748 1751\n",
      " 1771 1774 1775 1783 1793 1802 1805 1812 1816 1824 1867 1871 1874 1877\n",
      " 1881 1884 1919 1927 1933 1937 1938 1941 1953 1961 1971 1991 1992 1994\n",
      " 2010 2020 2024 2053 2067 2105 2107 2113 2123 2124 2125 2137 2147 2156\n",
      " 2169 2170 2191 2210 2215 2227 2242 2250 2260 2267 2285 2289 2308 2311\n",
      " 2319 2320 2335 2336 2356 2364 2368 2392 2393 2404 2413 2439 2440 2456\n",
      " 2460 2483 2491 2495 2503 2514 2525 2533 2546 2552 2557 2569 2576 2599\n",
      " 2637 2652 2657 2665 2692 2705 2720 2730 2747 2749 2756 2759 2779 2782\n",
      " 2830 2831 2844 2855 2867 2904 2928 2937 2944 2963 2973 2984 2996 3007\n",
      " 3016 3017 3029 3041 3047 3053 3058 3064 3085 3086 3094 3106 3125 3133\n",
      " 3151 3152 3157 3161 3180 3194 3197 3199 3209 3210 3215 3227 3238 3243\n",
      " 3255 3258 3273 3275 3284 3289 3296 3315 3324 3325 3331 3340 3350 3352\n",
      " 3362 3400 3406 3414 3416 3426 3434 3437 3443 3452 3454 3460 3467 3469\n",
      " 3471 3474 3476 3478 3479 3481 3492 3500 3513 3514 3539 3544 3550 3553\n",
      " 3571 3576 3589 3592 3608 3621 3624 3637 3644 3654 3665 3680 3682 3695\n",
      " 3705 3707 3709 3716 3726 3752 3755 3758 3773 3780 3785 3796 3815 3824\n",
      " 3827 3845 3848 3849 3857 3891 3894 3905 3910 3914 3924 3925 3928 3939\n",
      " 3943 3944 3953 3955 3959 3972 3984 3987 3998 4000 4002 4010 4016 4043\n",
      " 4072 4081 4084 4091 4111 4119 4149 4174 4199 4201 4229 4231 4240 4266\n",
      " 4269 4274 4277 4290 4299 4303 4317 4322 4326 4359 4373 4376 4379 4387\n",
      " 4407 4410 4441 4453 4454 4457 4474 4481 4494 4498 4542 4559 4586 4624\n",
      " 4625 4641 4642 4645 4663 4677 4682 4704 4712 4713 4719 4727 4741 4759\n",
      " 4765 4793 4797 4811 4829 4839 4844 4846 4853 4855 4860 4862 4863 4872\n",
      " 4875 4882 4883 4884 4891 4892 4911 4920 4924 4950 4952 4992 5004 5031\n",
      " 5046 5047 5070 5077 5078 5079 5088 5098 5107 5112 5125 5133 5135 5137\n",
      " 5150 5153 5160 5172 5177 5200 5212 5225 5255 5259 5275 5276 5292 5294\n",
      " 5295 5297 5309 5322 5337 5351 5362 5365 5378 5386 5448 5481 5491 5492\n",
      " 5503 5511 5534 5544 5547 5566 5567 5578 5582 5585 5611 5613 5652 5655\n",
      " 5663 5677 5689 5691 5700 5715 5728 5731 5734 5737 5744 5766 5813 5853\n",
      " 5854 5863 5867 5868 5871 5880 5883 5885 5906 5912 5923 5938 5940 5944\n",
      " 5955 5960 5978 5979 5985 5995 5998 6000 6001 6013 6023 6033 6036 6039\n",
      " 6069 6077 6086 6092 6100 6117 6152 6165 6173 6176 6179 6204 6235 6240\n",
      " 6246 6248 6256 6279 6284 6290 6323 6335 6368 6369 6381 6383 6389 6399\n",
      " 6428 6440 6472 6494 6513 6524 6530 6532 6534 6543 6560 6563 6565 6572\n",
      " 6581 6586 6593 6605 6606 6610 6611 6636 6655 6660 6663 6668 6670 6673\n",
      " 6675 6676 6677 6683 6685 6720 6738 6741 6745 6754 6779 6781 6796 6797\n",
      " 6801 6807 6809 6824 6829 6852 6853 6861 6870 6886 6891 6899 6904 6915\n",
      " 6927 6928 6936 6948 6973 6987 7025 7046 7065 7083 7102 7103 7107 7115\n",
      " 7124 7127 7141 7150 7157 7163 7164 7206 7209 7217 7219 7226 7255 7256\n",
      " 7262 7266 7277 7317 7320 7339 7353 7362 7380 7385 7393 7394 7402 7412\n",
      " 7425 7426 7428 7442 7449 7463 7485 7489 7493 7501 7505 7515 7516 7525\n",
      " 7526 7542 7549 7564 7568 7576 7590 7591 7594 7603 7623 7644 7655 7656\n",
      " 7665 7668 7669 7671 7672 7676 7678 7685 7690 7694 7705 7706 7713 7735\n",
      " 7745 7751 7757 7765 7773 7780 7782 7787 7798 7804 7812 7814 7846 7849\n",
      " 7859 7867 7877 7879 7883 7890 7896 7908 7916 7930 7935 7939 7942 7944\n",
      " 7949 7975 7977 7982 7991 8026 8034 8046 8048 8055 8058 8059 8064 8083\n",
      " 8091 8096 8115 8147 8149 8161 8176 8185 8200 8206 8212 8215 8222 8243\n",
      " 8256 8275 8276 8284 8287 8293 8300 8301 8310 8315 8320 8329 8332 8337\n",
      " 8338 8340 8341 8343 8405 8427 8446 8460 8465 8470 8482 8494 8500 8501\n",
      " 8523 8540 8557 8572 8598 8599 8615 8636 8652 8655 8663 8667 8668 8671\n",
      " 8677 8682 8687 8695 8697 8698 8699 8707 8711 8716 8753 8772 8773 8824\n",
      " 8831 8837 8842 8858 8881 8891 8892 8900 8901 8912]\n",
      "Y_pred: \n",
      " [1.50167958 0.         0.         ... 0.         0.         9.16150331]\n",
      "train_idx: \n",
      " 8026 [   0    2    3 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   1    5   11   30   31   33   42   45   49   50   66  120  130  149\n",
      "  209  210  217  218  221  231  232  236  237  251  258  283  287  288\n",
      "  292  300  307  308  338  340  378  390  394  405  430  445  451  463\n",
      "  476  480  504  509  512  530  536  539  542  544  546  548  550  567\n",
      "  578  584  588  590  596  602  607  608  609  618  619  620  621  624\n",
      "  628  637  651  664  670  673  684  685  688  693  694  704  710  727\n",
      "  732  734  762  772  780  809  824  825  834  848  864  865  876  892\n",
      "  895  897  908  911  919  949  954  955  958  971  976  980  983  984\n",
      " 1021 1029 1046 1052 1054 1055 1056 1062 1083 1094 1098 1102 1104 1123\n",
      " 1128 1138 1140 1165 1172 1177 1205 1207 1210 1231 1232 1243 1255 1276\n",
      " 1281 1305 1316 1317 1324 1327 1336 1339 1349 1350 1358 1359 1363 1378\n",
      " 1379 1384 1404 1454 1455 1458 1491 1497 1528 1529 1571 1573 1585 1593\n",
      " 1595 1619 1628 1635 1656 1675 1700 1713 1736 1756 1760 1761 1772 1778\n",
      " 1801 1808 1820 1823 1827 1834 1835 1847 1853 1854 1861 1875 1887 1889\n",
      " 1896 1899 1900 1917 1939 1942 1945 1949 1955 1977 1978 1984 1997 2011\n",
      " 2014 2016 2031 2034 2068 2076 2097 2122 2126 2136 2157 2173 2178 2185\n",
      " 2190 2194 2206 2209 2216 2225 2238 2244 2245 2252 2253 2254 2256 2262\n",
      " 2277 2278 2280 2294 2300 2312 2324 2345 2352 2355 2361 2363 2367 2374\n",
      " 2383 2386 2387 2390 2396 2401 2405 2407 2438 2454 2505 2512 2520 2527\n",
      " 2541 2551 2559 2563 2567 2570 2592 2623 2625 2638 2676 2686 2688 2702\n",
      " 2706 2709 2713 2716 2721 2752 2754 2760 2769 2777 2797 2798 2805 2812\n",
      " 2833 2843 2874 2876 2883 2902 2906 2913 2932 2934 2942 2951 2956 2968\n",
      " 2976 2989 2991 3012 3025 3027 3056 3071 3099 3113 3122 3123 3126 3130\n",
      " 3148 3164 3174 3179 3188 3201 3203 3211 3220 3240 3242 3246 3253 3261\n",
      " 3270 3276 3301 3314 3316 3318 3320 3343 3353 3357 3373 3374 3375 3385\n",
      " 3396 3401 3407 3409 3422 3431 3433 3435 3446 3456 3462 3485 3491 3494\n",
      " 3502 3529 3530 3533 3551 3561 3583 3590 3593 3595 3598 3603 3607 3634\n",
      " 3670 3671 3717 3723 3725 3729 3742 3754 3766 3787 3805 3806 3839 3840\n",
      " 3847 3860 3885 3888 3901 3915 3916 3917 3918 3929 3936 3945 3964 3988\n",
      " 3991 3999 4051 4054 4060 4068 4078 4104 4107 4112 4116 4118 4143 4160\n",
      " 4166 4168 4204 4214 4234 4239 4241 4293 4302 4312 4344 4356 4357 4388\n",
      " 4412 4418 4422 4429 4438 4440 4445 4459 4470 4485 4487 4496 4506 4507\n",
      " 4510 4523 4532 4534 4541 4549 4557 4558 4584 4591 4596 4600 4619 4634\n",
      " 4639 4640 4668 4669 4690 4694 4697 4702 4714 4716 4722 4740 4747 4751\n",
      " 4755 4756 4770 4776 4788 4802 4808 4820 4822 4858 4876 4890 4919 4923\n",
      " 4932 4934 4935 4936 4945 4949 4972 4984 4990 4995 5000 5001 5029 5040\n",
      " 5041 5049 5056 5058 5062 5064 5065 5081 5087 5099 5113 5127 5128 5154\n",
      " 5156 5161 5167 5168 5208 5222 5248 5277 5289 5299 5310 5336 5364 5389\n",
      " 5415 5423 5425 5435 5443 5457 5461 5464 5476 5485 5494 5496 5509 5512\n",
      " 5528 5529 5532 5539 5554 5569 5571 5591 5595 5608 5615 5619 5622 5626\n",
      " 5633 5637 5654 5660 5666 5667 5673 5676 5686 5701 5708 5724 5738 5756\n",
      " 5761 5764 5775 5776 5780 5782 5801 5805 5810 5811 5812 5814 5832 5833\n",
      " 5837 5847 5849 5855 5857 5870 5873 5904 5928 5934 5935 5945 5954 5983\n",
      " 5989 5997 6002 6010 6038 6047 6056 6066 6081 6091 6099 6102 6114 6121\n",
      " 6129 6131 6137 6140 6158 6172 6210 6216 6219 6251 6267 6278 6288 6294\n",
      " 6305 6306 6320 6333 6346 6364 6366 6378 6380 6398 6404 6407 6433 6434\n",
      " 6452 6463 6465 6471 6492 6504 6519 6537 6538 6551 6556 6569 6582 6589\n",
      " 6616 6624 6626 6634 6642 6647 6657 6662 6679 6703 6740 6748 6765 6768\n",
      " 6803 6808 6817 6831 6837 6862 6864 6873 6902 6906 6911 6912 6921 6922\n",
      " 6929 6942 6947 6956 6958 6959 6961 6966 6976 6979 6993 6994 7006 7018\n",
      " 7019 7026 7056 7064 7067 7078 7086 7092 7099 7110 7111 7125 7130 7151\n",
      " 7161 7167 7169 7185 7193 7200 7208 7233 7237 7245 7246 7259 7269 7272\n",
      " 7282 7318 7319 7321 7325 7345 7372 7377 7386 7387 7391 7397 7404 7416\n",
      " 7418 7436 7476 7482 7484 7497 7500 7506 7517 7555 7559 7565 7570 7596\n",
      " 7610 7611 7615 7661 7667 7674 7677 7688 7692 7711 7739 7742 7755 7800\n",
      " 7802 7807 7808 7821 7825 7830 7831 7836 7840 7844 7856 7870 7885 7897\n",
      " 7903 7909 7927 7946 7952 7953 7954 7961 7976 7978 7980 7986 7990 8001\n",
      " 8017 8029 8030 8042 8043 8061 8065 8093 8097 8119 8133 8134 8135 8153\n",
      " 8162 8163 8167 8201 8208 8220 8223 8233 8247 8264 8269 8286 8291 8292\n",
      " 8299 8304 8306 8322 8323 8349 8371 8376 8421 8429 8440 8450 8464 8469\n",
      " 8473 8476 8481 8496 8509 8511 8536 8537 8546 8564 8569 8585 8597 8609\n",
      " 8614 8629 8632 8640 8642 8644 8656 8658 8673 8675 8678 8684 8686 8700\n",
      " 8702 8720 8725 8744 8749 8755 8764 8778 8792 8795 8828 8830 8833 8840\n",
      " 8850 8853 8862 8867 8880 8898 8904 8905 8908 8913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: \n",
      " [1.50167958 1.70580025 0.         ... 0.         0.         9.16150331]\n",
      "train_idx: \n",
      " 8026 [   0    1    2 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   9   15   28   38   63   68   85  119  139  150  161  165  168  170\n",
      "  179  186  188  191  194  195  196  198  203  206  224  254  255  266\n",
      "  267  314  330  362  370  375  382  384  403  441  450  465  472  482\n",
      "  484  489  490  505  507  513  518  532  535  538  570  574  577  580\n",
      "  581  583  592  598  600  604  615  622  638  639  654  657  665  675\n",
      "  677  682  698  724  731  743  745  760  764  796  810  818  828  872\n",
      "  877  881  888  891  906  913  921  922  924  941  946  961  964  986\n",
      " 1000 1012 1026 1032 1042 1057 1059 1061 1067 1082 1085 1109 1110 1139\n",
      " 1153 1193 1200 1208 1215 1217 1224 1234 1241 1248 1261 1262 1263 1270\n",
      " 1293 1296 1308 1311 1318 1346 1370 1382 1386 1428 1439 1450 1459 1464\n",
      " 1486 1490 1506 1517 1526 1544 1565 1578 1584 1591 1602 1608 1611 1617\n",
      " 1624 1626 1639 1642 1650 1666 1679 1688 1691 1703 1708 1728 1750 1768\n",
      " 1782 1784 1794 1797 1798 1809 1810 1839 1859 1869 1873 1895 1903 1904\n",
      " 1907 1923 1925 1928 1930 1940 1946 1948 1952 1965 1981 1982 1987 1993\n",
      " 1998 2007 2027 2035 2039 2059 2063 2072 2073 2088 2091 2100 2106 2114\n",
      " 2119 2120 2138 2139 2146 2159 2171 2174 2177 2187 2199 2202 2217 2218\n",
      " 2236 2251 2266 2269 2281 2313 2330 2357 2360 2365 2388 2415 2423 2435\n",
      " 2443 2455 2470 2473 2474 2486 2494 2506 2517 2530 2534 2542 2564 2565\n",
      " 2581 2584 2587 2591 2609 2614 2615 2624 2627 2633 2634 2642 2653 2659\n",
      " 2669 2683 2700 2707 2719 2744 2745 2753 2762 2763 2775 2804 2822 2829\n",
      " 2857 2869 2870 2880 2881 2888 2910 2917 2933 2954 2964 2975 3004 3018\n",
      " 3021 3022 3024 3034 3049 3059 3072 3081 3087 3089 3091 3093 3108 3109\n",
      " 3117 3142 3144 3149 3163 3166 3167 3168 3182 3193 3217 3229 3231 3235\n",
      " 3249 3259 3272 3280 3292 3294 3303 3305 3327 3329 3334 3336 3344 3346\n",
      " 3355 3364 3378 3382 3393 3404 3405 3417 3427 3439 3459 3463 3468 3473\n",
      " 3525 3537 3540 3542 3552 3557 3582 3585 3596 3597 3613 3617 3623 3645\n",
      " 3656 3687 3706 3731 3734 3738 3739 3741 3770 3772 3777 3781 3789 3790\n",
      " 3792 3793 3800 3808 3820 3825 3826 3828 3850 3858 3865 3875 3893 3900\n",
      " 3922 3937 3950 3963 3980 3989 3994 3996 4004 4013 4030 4037 4044 4058\n",
      " 4063 4069 4073 4075 4079 4080 4092 4093 4101 4113 4133 4142 4150 4156\n",
      " 4165 4167 4179 4211 4216 4222 4224 4228 4236 4245 4247 4258 4259 4261\n",
      " 4262 4267 4272 4289 4295 4301 4307 4310 4318 4330 4332 4336 4342 4343\n",
      " 4350 4352 4354 4375 4380 4403 4406 4409 4414 4421 4424 4428 4435 4448\n",
      " 4464 4509 4515 4527 4535 4548 4552 4554 4577 4580 4583 4588 4601 4611\n",
      " 4612 4615 4629 4644 4649 4650 4661 4664 4667 4670 4671 4680 4687 4692\n",
      " 4699 4705 4711 4720 4736 4748 4749 4757 4767 4772 4775 4778 4779 4785\n",
      " 4787 4790 4801 4804 4809 4823 4843 4848 4856 4885 4898 4939 4955 4958\n",
      " 4961 4966 4969 4971 4981 4985 5002 5007 5016 5019 5025 5026 5043 5051\n",
      " 5054 5055 5066 5071 5074 5076 5083 5089 5096 5104 5116 5123 5126 5139\n",
      " 5159 5162 5163 5173 5174 5186 5191 5196 5207 5219 5221 5226 5234 5240\n",
      " 5257 5260 5263 5265 5266 5280 5293 5307 5308 5318 5333 5334 5366 5367\n",
      " 5374 5390 5402 5406 5414 5422 5437 5458 5468 5480 5504 5507 5521 5526\n",
      " 5541 5553 5580 5583 5597 5625 5648 5678 5703 5711 5740 5742 5772 5784\n",
      " 5786 5787 5794 5804 5829 5831 5834 5839 5846 5858 5860 5874 5877 5887\n",
      " 5888 5895 5901 5921 5941 5981 5992 5993 6007 6008 6051 6083 6088 6113\n",
      " 6115 6119 6133 6134 6138 6142 6146 6149 6174 6203 6241 6244 6249 6253\n",
      " 6277 6280 6313 6325 6327 6332 6336 6341 6345 6351 6352 6355 6359 6371\n",
      " 6372 6394 6410 6419 6426 6429 6430 6445 6464 6466 6489 6495 6500 6509\n",
      " 6516 6540 6579 6607 6625 6627 6628 6635 6648 6651 6654 6661 6678 6680\n",
      " 6690 6701 6706 6712 6728 6731 6749 6756 6757 6760 6761 6770 6780 6784\n",
      " 6785 6816 6819 6827 6830 6836 6844 6859 6860 6865 6867 6868 6875 6887\n",
      " 6896 6903 6916 6925 6946 6974 6988 7008 7009 7028 7038 7042 7048 7079\n",
      " 7080 7084 7089 7093 7097 7100 7104 7113 7116 7147 7148 7172 7205 7207\n",
      " 7230 7249 7252 7261 7271 7274 7281 7285 7286 7290 7298 7315 7335 7336\n",
      " 7350 7351 7361 7373 7395 7413 7419 7423 7427 7432 7441 7461 7486 7488\n",
      " 7499 7502 7521 7528 7531 7551 7562 7569 7581 7586 7598 7602 7606 7608\n",
      " 7618 7619 7634 7636 7647 7653 7679 7686 7695 7698 7703 7716 7717 7732\n",
      " 7740 7743 7749 7753 7760 7761 7777 7778 7781 7801 7805 7823 7850 7863\n",
      " 7889 7898 7911 7925 7967 7992 8005 8023 8025 8027 8028 8049 8050 8056\n",
      " 8066 8082 8114 8116 8120 8124 8128 8146 8186 8189 8191 8195 8209 8218\n",
      " 8239 8246 8250 8282 8285 8303 8324 8330 8331 8362 8369 8382 8384 8392\n",
      " 8398 8417 8468 8488 8499 8529 8531 8533 8535 8539 8547 8573 8578 8579\n",
      " 8617 8621 8637 8641 8662 8669 8674 8692 8704 8710 8715 8738 8742 8752\n",
      " 8766 8777 8787 8794 8835 8844 8871 8885 8895 8910]\n",
      "Y_pred: \n",
      " [1.50167958 1.70580025 0.         ... 0.         0.         9.16150331]\n",
      "train_idx: \n",
      " 8026 [   0    1    2 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   4   19   27   36   39   48   60   67   84   92   94  109  115  116\n",
      "  118  123  129  132  135  138  155  157  166  177  178  190  200  223\n",
      "  277  284  304  305  320  322  324  328  333  345  351  365  368  391\n",
      "  396  401  407  420  421  456  519  531  556  564  571  572  605  606\n",
      "  613  632  640  661  681  687  718  759  775  782  801  806  808  815\n",
      "  819  831  840  842  852  882  900  901  909  912  931  934  936  951\n",
      "  953  959 1004 1019 1051 1073 1079 1092 1119 1137 1157 1158 1179 1184\n",
      " 1188 1199 1202 1209 1228 1240 1246 1249 1250 1259 1266 1267 1277 1278\n",
      " 1285 1289 1290 1292 1297 1303 1307 1335 1351 1360 1365 1374 1396 1409\n",
      " 1412 1413 1418 1426 1431 1432 1433 1451 1463 1484 1485 1495 1510 1512\n",
      " 1536 1551 1562 1567 1576 1579 1588 1592 1596 1597 1601 1604 1620 1621\n",
      " 1625 1634 1646 1676 1682 1693 1720 1729 1731 1740 1757 1758 1773 1776\n",
      " 1786 1790 1803 1804 1811 1829 1833 1858 1862 1864 1878 1879 1911 1913\n",
      " 1916 1920 1934 1960 1964 1973 1990 2006 2013 2018 2019 2025 2026 2032\n",
      " 2045 2050 2062 2065 2066 2070 2081 2093 2094 2103 2108 2109 2111 2129\n",
      " 2133 2158 2160 2165 2176 2184 2193 2200 2205 2228 2284 2292 2306 2315\n",
      " 2321 2326 2340 2380 2399 2412 2426 2427 2430 2434 2441 2479 2481 2499\n",
      " 2500 2501 2524 2537 2544 2545 2553 2580 2590 2594 2596 2598 2606 2607\n",
      " 2612 2617 2620 2622 2630 2635 2640 2641 2660 2694 2714 2723 2724 2725\n",
      " 2738 2748 2788 2789 2791 2815 2817 2853 2875 2889 2896 2903 2908 2916\n",
      " 2945 2961 2962 2972 2979 2980 2988 2993 3000 3008 3013 3032 3036 3040\n",
      " 3057 3062 3077 3078 3082 3110 3132 3141 3155 3172 3177 3190 3202 3204\n",
      " 3237 3254 3277 3281 3283 3321 3323 3330 3339 3347 3348 3360 3387 3388\n",
      " 3391 3402 3432 3441 3444 3455 3486 3488 3499 3501 3506 3545 3554 3560\n",
      " 3574 3580 3600 3602 3609 3612 3620 3622 3625 3633 3646 3651 3661 3674\n",
      " 3675 3690 3696 3700 3701 3704 3708 3714 3718 3720 3736 3737 3743 3753\n",
      " 3759 3764 3775 3814 3837 3852 3873 3876 3889 3902 3903 3920 3967 3970\n",
      " 3974 3982 4005 4008 4012 4023 4024 4027 4028 4036 4045 4048 4061 4070\n",
      " 4074 4082 4087 4114 4122 4123 4124 4131 4132 4138 4148 4152 4177 4187\n",
      " 4208 4230 4238 4271 4286 4308 4314 4337 4374 4377 4386 4389 4391 4417\n",
      " 4420 4425 4444 4447 4449 4451 4461 4462 4463 4466 4472 4477 4480 4482\n",
      " 4486 4537 4563 4585 4614 4623 4628 4633 4636 4643 4646 4662 4684 4723\n",
      " 4724 4725 4732 4738 4750 4754 4758 4761 4764 4784 4806 4825 4827 4845\n",
      " 4847 4850 4864 4886 4896 4916 4918 4938 4944 4960 4967 4968 4970 4973\n",
      " 4977 4988 4991 4996 5003 5006 5014 5015 5021 5032 5039 5053 5080 5082\n",
      " 5085 5141 5142 5143 5165 5203 5224 5228 5243 5244 5245 5269 5281 5287\n",
      " 5300 5315 5320 5327 5332 5346 5379 5388 5391 5397 5400 5408 5418 5438\n",
      " 5440 5444 5452 5454 5467 5469 5470 5488 5490 5498 5500 5522 5538 5557\n",
      " 5560 5587 5590 5609 5612 5616 5634 5645 5650 5656 5684 5695 5696 5716\n",
      " 5722 5746 5750 5755 5768 5774 5806 5815 5819 5827 5835 5836 5844 5893\n",
      " 5898 5900 5907 5911 5915 5927 5951 5957 5962 5964 5984 5986 5996 6017\n",
      " 6018 6024 6027 6030 6040 6041 6042 6052 6057 6078 6090 6107 6108 6109\n",
      " 6110 6125 6132 6139 6147 6163 6167 6187 6188 6191 6199 6209 6211 6217\n",
      " 6220 6228 6230 6259 6268 6276 6282 6297 6301 6304 6314 6317 6322 6338\n",
      " 6339 6347 6354 6358 6367 6377 6379 6382 6390 6406 6409 6411 6412 6424\n",
      " 6437 6448 6450 6455 6470 6480 6481 6482 6483 6485 6486 6497 6507 6512\n",
      " 6523 6533 6535 6552 6558 6559 6599 6600 6601 6602 6604 6622 6638 6643\n",
      " 6645 6686 6693 6696 6711 6713 6714 6718 6739 6747 6750 6755 6758 6762\n",
      " 6769 6771 6783 6788 6791 6820 6823 6826 6843 6845 6846 6848 6872 6879\n",
      " 6885 6888 6890 6905 6919 6941 6949 6953 6955 6962 6980 6996 6997 7000\n",
      " 7013 7014 7027 7044 7051 7058 7063 7075 7085 7112 7138 7165 7171 7174\n",
      " 7195 7196 7197 7201 7236 7240 7265 7268 7270 7284 7312 7322 7326 7344\n",
      " 7347 7364 7374 7446 7465 7483 7492 7494 7507 7541 7546 7567 7573 7583\n",
      " 7589 7599 7612 7630 7633 7637 7639 7643 7648 7651 7693 7708 7710 7737\n",
      " 7738 7750 7752 7754 7758 7767 7770 7783 7810 7819 7820 7843 7853 7854\n",
      " 7869 7874 7891 7895 7899 7902 7907 7910 7921 7923 7937 7943 7947 7973\n",
      " 7981 7984 7987 7993 7994 7995 7999 8019 8022 8031 8033 8036 8053 8063\n",
      " 8076 8077 8087 8090 8102 8107 8129 8130 8138 8139 8140 8159 8165 8172\n",
      " 8179 8194 8199 8219 8228 8270 8274 8280 8283 8318 8335 8347 8367 8379\n",
      " 8381 8383 8388 8401 8403 8411 8432 8435 8447 8455 8456 8471 8474 8489\n",
      " 8490 8508 8510 8518 8521 8522 8541 8552 8555 8559 8565 8583 8587 8591\n",
      " 8596 8607 8610 8620 8625 8628 8643 8646 8649 8654 8661 8664 8683 8708\n",
      " 8722 8728 8729 8739 8746 8748 8785 8789 8798 8806 8807 8817 8819 8826\n",
      " 8834 8836 8838 8859 8861 8866 8870 8888 8899 8906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: \n",
      " [1.50167958 1.70580025 0.         ... 0.         0.         9.16150331]\n",
      "train_idx: \n",
      " 8026 [   0    1    2 ... 8914 8915 8917]\n",
      "val_idx: \n",
      " 892 [   8   23   40   44   46   47   51   59   70   74   80   93  102  104\n",
      "  113  114  127  134  167  192  225  246  247  250  264  265  278  289\n",
      "  312  321  323  336  337  341  347  363  389  400  402  419  425  442\n",
      "  444  455  462  466  467  469  477  483  487  492  495  501  510  523\n",
      "  537  545  554  557  579  586  591  623  625  627  635  642  645  668\n",
      "  707  708  711  717  723  736  754  763  766  767  777  783  789  792\n",
      "  803  804  807  822  835  836  837  841  844  845  858  861  866  884\n",
      "  898  918  923  925  935  938  957  966  967  973  974  979  985  987\n",
      "  997 1007 1009 1027 1028 1041 1047 1068 1077 1081 1090 1107 1125 1127\n",
      " 1136 1141 1149 1151 1168 1180 1191 1206 1218 1219 1225 1226 1264 1271\n",
      " 1280 1283 1301 1310 1338 1342 1371 1373 1390 1392 1395 1401 1405 1421\n",
      " 1441 1446 1448 1453 1461 1462 1465 1467 1475 1498 1501 1507 1508 1515\n",
      " 1518 1527 1533 1535 1542 1546 1556 1577 1580 1599 1600 1606 1610 1612\n",
      " 1623 1629 1636 1637 1651 1655 1657 1662 1665 1669 1724 1725 1732 1742\n",
      " 1747 1749 1762 1766 1767 1787 1788 1795 1799 1818 1825 1841 1855 1888\n",
      " 1898 1902 1936 1944 1947 1988 2017 2023 2029 2033 2042 2061 2086 2087\n",
      " 2090 2127 2134 2144 2152 2155 2168 2188 2192 2219 2222 2235 2248 2259\n",
      " 2270 2276 2291 2296 2298 2314 2316 2317 2325 2347 2348 2372 2373 2375\n",
      " 2377 2378 2382 2394 2395 2400 2402 2437 2457 2458 2462 2471 2476 2478\n",
      " 2485 2492 2507 2508 2521 2531 2536 2538 2556 2568 2579 2586 2605 2619\n",
      " 2621 2628 2629 2639 2656 2664 2668 2678 2690 2695 2701 2704 2729 2741\n",
      " 2746 2750 2751 2755 2757 2768 2772 2795 2796 2819 2832 2835 2837 2838\n",
      " 2842 2854 2856 2858 2859 2865 2868 2873 2898 2912 2919 2922 2923 2936\n",
      " 2947 2952 2957 2959 2960 2974 2987 2992 3001 3003 3048 3051 3054 3055\n",
      " 3073 3096 3100 3112 3120 3121 3139 3143 3169 3173 3183 3186 3208 3219\n",
      " 3222 3247 3256 3263 3295 3307 3308 3317 3328 3338 3349 3363 3371 3377\n",
      " 3379 3412 3419 3424 3472 3480 3483 3484 3495 3522 3531 3532 3546 3555\n",
      " 3563 3572 3578 3584 3587 3591 3601 3606 3615 3631 3635 3652 3684 3721\n",
      " 3722 3733 3735 3768 3809 3831 3835 3836 3838 3851 3861 3867 3870 3878\n",
      " 3879 3880 3881 3887 3904 3908 3919 3930 3940 3952 3957 3961 3969 3971\n",
      " 4014 4017 4031 4032 4041 4056 4064 4083 4094 4097 4103 4109 4129 4140\n",
      " 4153 4157 4161 4164 4195 4202 4217 4227 4237 4244 4251 4256 4263 4304\n",
      " 4319 4325 4327 4347 4362 4364 4383 4385 4393 4419 4430 4443 4458 4488\n",
      " 4490 4491 4502 4528 4533 4536 4568 4572 4573 4574 4576 4578 4581 4598\n",
      " 4602 4605 4610 4626 4630 4652 4679 4708 4710 4728 4730 4737 4768 4791\n",
      " 4795 4813 4818 4819 4824 4831 4833 4837 4842 4852 4857 4910 4921 4931\n",
      " 4943 4946 4976 4979 4982 4999 5013 5028 5063 5073 5090 5092 5094 5101\n",
      " 5105 5110 5129 5134 5149 5166 5176 5184 5189 5197 5201 5211 5216 5235\n",
      " 5236 5242 5246 5247 5274 5278 5282 5312 5314 5339 5347 5352 5359 5361\n",
      " 5363 5368 5371 5377 5383 5384 5399 5409 5412 5421 5431 5433 5449 5456\n",
      " 5483 5505 5506 5515 5516 5551 5559 5562 5575 5581 5589 5598 5607 5610\n",
      " 5620 5644 5646 5662 5664 5672 5675 5680 5681 5682 5692 5694 5717 5725\n",
      " 5732 5733 5741 5749 5759 5767 5777 5778 5792 5799 5808 5822 5859 5862\n",
      " 5879 5897 5902 5909 5914 5917 5937 5943 5963 5967 5973 5982 6004 6012\n",
      " 6022 6035 6044 6048 6053 6054 6058 6070 6089 6105 6106 6112 6123 6130\n",
      " 6160 6169 6185 6200 6212 6221 6227 6242 6254 6255 6257 6273 6289 6295\n",
      " 6298 6309 6312 6315 6321 6334 6344 6356 6375 6388 6397 6425 6442 6460\n",
      " 6467 6503 6510 6511 6527 6528 6542 6546 6547 6554 6564 6573 6578 6580\n",
      " 6591 6608 6613 6619 6621 6637 6641 6652 6667 6672 6688 6694 6699 6724\n",
      " 6725 6732 6733 6744 6766 6804 6811 6815 6828 6835 6838 6840 6869 6898\n",
      " 6901 6913 6920 6954 6960 6967 6970 6972 7036 7066 7074 7081 7098 7120\n",
      " 7121 7128 7135 7136 7145 7156 7158 7160 7192 7215 7222 7280 7288 7289\n",
      " 7292 7297 7310 7314 7337 7348 7355 7365 7369 7378 7383 7421 7429 7430\n",
      " 7443 7454 7457 7470 7496 7508 7537 7550 7553 7554 7558 7604 7607 7616\n",
      " 7624 7629 7638 7642 7663 7670 7687 7728 7741 7746 7748 7766 7774 7796\n",
      " 7799 7806 7811 7813 7828 7838 7860 7862 7876 7884 7906 7913 7919 7920\n",
      " 7926 7941 7955 7958 7968 7970 7996 8006 8007 8015 8038 8039 8040 8044\n",
      " 8060 8062 8078 8080 8089 8150 8166 8169 8173 8177 8178 8181 8190 8207\n",
      " 8226 8234 8252 8271 8279 8294 8305 8312 8319 8321 8325 8334 8350 8352\n",
      " 8356 8357 8358 8368 8377 8385 8387 8406 8407 8419 8424 8430 8448 8453\n",
      " 8477 8479 8493 8528 8532 8542 8545 8576 8577 8592 8606 8612 8616 8622\n",
      " 8624 8633 8650 8685 8694 8706 8709 8712 8732 8740 8745 8759 8760 8762\n",
      " 8770 8775 8780 8786 8797 8804 8810 8813 8823 8825 8839 8845 8848 8852\n",
      " 8854 8856 8865 8869 8874 8877 8879 8884 8893 8916]\n",
      "Y_pred: \n",
      " [1.50167958 1.70580025 0.         ... 0.         9.64450956 9.16150331]\n",
      "train_idx: \n",
      " 8026 [   0    1    3 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 892 [   2    6   14   17   21   43   69   81   88   95  105  117  156  158\n",
      "  160  169  172  183  187  199  207  212  233  241  259  260  261  271\n",
      "  275  281  286  293  299  301  309  315  334  342  352  372  388  399\n",
      "  428  435  438  459  511  528  540  543  555  561  589  634  646  678\n",
      "  690  692  696  697  721  729  746  751  765  797  802  817  823  850\n",
      "  869  874  889  894  905  915  920  927  945  965  975  990  991 1010\n",
      " 1023 1025 1035 1036 1037 1066 1074 1097 1114 1126 1132 1133 1143 1148\n",
      " 1167 1169 1189 1192 1195 1198 1201 1212 1227 1230 1252 1256 1260 1268\n",
      " 1269 1314 1323 1325 1331 1333 1337 1345 1355 1372 1380 1403 1410 1420\n",
      " 1427 1443 1447 1456 1457 1460 1470 1471 1473 1479 1482 1488 1493 1522\n",
      " 1534 1557 1558 1561 1575 1590 1607 1632 1654 1671 1689 1694 1698 1715\n",
      " 1717 1718 1721 1733 1746 1755 1764 1785 1814 1836 1842 1849 1865 1866\n",
      " 1882 1905 1906 1915 1957 1959 1962 1980 2003 2009 2057 2079 2089 2099\n",
      " 2118 2140 2151 2166 2196 2212 2223 2231 2241 2247 2249 2273 2286 2287\n",
      " 2302 2328 2333 2334 2339 2341 2344 2350 2358 2376 2397 2406 2410 2411\n",
      " 2421 2436 2447 2448 2453 2461 2472 2475 2484 2488 2504 2509 2515 2519\n",
      " 2535 2539 2543 2572 2577 2610 2613 2631 2646 2651 2658 2663 2670 2672\n",
      " 2677 2728 2733 2734 2736 2739 2740 2742 2764 2766 2787 2801 2806 2810\n",
      " 2814 2824 2845 2847 2849 2850 2851 2861 2864 2866 2872 2887 2890 2891\n",
      " 2892 2900 2914 2926 2929 2943 2948 2955 2969 2978 2986 2995 3006 3014\n",
      " 3030 3035 3046 3060 3065 3097 3104 3114 3115 3116 3135 3138 3187 3191\n",
      " 3196 3206 3214 3223 3225 3234 3250 3252 3266 3269 3271 3279 3286 3291\n",
      " 3298 3311 3332 3337 3354 3356 3358 3359 3367 3368 3384 3392 3394 3397\n",
      " 3408 3413 3421 3447 3448 3457 3466 3487 3498 3503 3504 3507 3510 3518\n",
      " 3543 3558 3566 3588 3610 3611 3628 3632 3641 3653 3655 3667 3669 3677\n",
      " 3679 3685 3686 3699 3719 3746 3748 3750 3786 3794 3798 3804 3807 3816\n",
      " 3817 3819 3843 3846 3863 3866 3886 3921 3934 3973 3975 3977 4001 4009\n",
      " 4019 4021 4022 4029 4034 4038 4049 4053 4062 4089 4095 4096 4110 4115\n",
      " 4117 4120 4128 4136 4137 4147 4163 4182 4183 4186 4194 4197 4207 4213\n",
      " 4218 4221 4252 4264 4265 4276 4279 4296 4297 4306 4313 4316 4320 4331\n",
      " 4339 4340 4348 4360 4361 4366 4371 4392 4394 4398 4399 4408 4413 4427\n",
      " 4431 4432 4433 4434 4439 4442 4455 4456 4471 4479 4492 4495 4497 4508\n",
      " 4513 4529 4531 4539 4544 4564 4575 4592 4593 4613 4616 4618 4632 4638\n",
      " 4647 4648 4651 4666 4674 4678 4688 4695 4703 4706 4707 4726 4735 4744\n",
      " 4771 4773 4777 4782 4783 4800 4816 4836 4840 4861 4870 4878 4893 4904\n",
      " 4908 4912 4922 4933 4937 4947 4954 4974 4994 5005 5012 5033 5044 5045\n",
      " 5114 5115 5121 5132 5138 5147 5157 5164 5169 5170 5193 5198 5205 5209\n",
      " 5214 5217 5218 5249 5253 5264 5273 5301 5345 5358 5376 5380 5387 5401\n",
      " 5426 5429 5432 5441 5442 5447 5463 5465 5473 5493 5495 5530 5546 5548\n",
      " 5564 5568 5572 5574 5599 5601 5603 5604 5623 5630 5631 5632 5638 5640\n",
      " 5642 5669 5670 5687 5702 5709 5710 5713 5718 5726 5754 5758 5773 5779\n",
      " 5789 5797 5803 5838 5840 5842 5851 5856 5865 5889 5890 5899 5903 5910\n",
      " 5932 5947 5952 5961 5975 5980 5988 5990 6005 6015 6032 6043 6064 6065\n",
      " 6072 6087 6093 6095 6124 6144 6153 6155 6156 6157 6159 6181 6184 6193\n",
      " 6196 6208 6215 6224 6239 6250 6269 6283 6302 6303 6316 6329 6342 6343\n",
      " 6360 6362 6376 6384 6387 6391 6402 6420 6427 6432 6444 6461 6469 6484\n",
      " 6487 6490 6506 6508 6514 6515 6536 6541 6553 6570 6584 6585 6592 6594\n",
      " 6596 6632 6649 6650 6659 6665 6682 6684 6705 6709 6716 6743 6767 6774\n",
      " 6786 6790 6805 6812 6854 6871 6881 6882 6883 6884 6893 6900 6917 6933\n",
      " 6937 6940 6944 6963 6964 6978 6985 6990 7002 7003 7023 7031 7041 7047\n",
      " 7053 7070 7071 7106 7122 7129 7139 7140 7144 7173 7189 7198 7203 7218\n",
      " 7221 7223 7224 7228 7229 7232 7235 7241 7244 7250 7254 7257 7263 7264\n",
      " 7276 7294 7296 7300 7301 7307 7330 7331 7342 7352 7358 7382 7392 7431\n",
      " 7437 7438 7452 7456 7458 7466 7473 7490 7512 7522 7535 7547 7556 7561\n",
      " 7595 7597 7600 7609 7632 7640 7641 7645 7657 7702 7720 7722 7724 7727\n",
      " 7733 7772 7775 7779 7784 7794 7816 7834 7842 7851 7857 7865 7872 7873\n",
      " 7875 7905 7915 7918 7924 7933 7936 7956 7957 7964 7966 7974 7983 7998\n",
      " 8010 8012 8014 8020 8032 8045 8047 8054 8088 8094 8100 8101 8105 8108\n",
      " 8127 8143 8145 8152 8157 8188 8192 8196 8211 8225 8229 8238 8241 8251\n",
      " 8268 8295 8296 8308 8333 8342 8345 8346 8348 8391 8400 8412 8416 8418\n",
      " 8423 8428 8431 8437 8441 8443 8445 8451 8452 8454 8457 8486 8491 8497\n",
      " 8516 8517 8519 8534 8544 8554 8556 8567 8580 8584 8593 8618 8626 8627\n",
      " 8631 8639 8647 8648 8670 8676 8688 8690 8701 8721 8726 8731 8741 8754\n",
      " 8758 8769 8802 8808 8820 8855 8868 8873 8902 8907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: \n",
      " [1.50167958 1.70580025 1.98029753 ... 0.         9.64450956 9.16150331]\n",
      "train_idx: \n",
      " 8027 [   0    1    2 ... 8915 8916 8917]\n",
      "val_idx: \n",
      " 891 [   7   29   57   58   76   78  103  106  108  125  126  137  147  162\n",
      "  173  185  193  205  208  226  228  240  244  253  263  270  279  318\n",
      "  327  331  335  344  348  358  367  371  380  381  387  392  404  408\n",
      "  409  410  413  415  427  429  433  447  453  464  468  471  475  488\n",
      "  526  533  547  559  562  593  599  629  662  679  683  689  691  702\n",
      "  713  716  726  738  741  742  744  747  753  755  770  771  776  788\n",
      "  791  799  800  805  814  849  859  863  886  910  914  939  942  968\n",
      "  970  978  981  998 1002 1003 1008 1013 1039 1040 1053 1060 1069 1070\n",
      " 1072 1075 1080 1105 1108 1112 1115 1120 1159 1175 1185 1186 1194 1213\n",
      " 1216 1222 1244 1272 1274 1279 1295 1312 1320 1321 1330 1347 1369 1375\n",
      " 1377 1385 1400 1419 1422 1430 1444 1469 1474 1477 1494 1500 1520 1532\n",
      " 1537 1583 1586 1594 1603 1605 1609 1614 1618 1640 1644 1659 1670 1681\n",
      " 1687 1699 1706 1710 1734 1738 1752 1759 1779 1789 1815 1817 1838 1844\n",
      " 1846 1848 1850 1863 1886 1891 1897 1910 1912 1921 1926 1932 1943 1963\n",
      " 1967 1970 1974 1983 1996 1999 2000 2008 2015 2021 2030 2036 2044 2049\n",
      " 2069 2074 2077 2078 2085 2092 2098 2121 2141 2143 2163 2175 2181 2214\n",
      " 2226 2234 2246 2257 2263 2265 2275 2295 2299 2304 2305 2318 2327 2329\n",
      " 2331 2359 2391 2409 2416 2418 2420 2422 2428 2450 2464 2482 2493 2516\n",
      " 2522 2529 2555 2583 2585 2593 2608 2636 2648 2649 2682 2693 2696 2697\n",
      " 2698 2703 2765 2767 2781 2792 2803 2808 2821 2839 2871 2879 2886 2895\n",
      " 2901 2907 2924 2927 2953 2965 2966 2967 2982 2994 3026 3028 3044 3052\n",
      " 3061 3079 3083 3092 3095 3101 3103 3131 3136 3140 3159 3165 3170 3176\n",
      " 3185 3200 3207 3216 3224 3236 3241 3260 3274 3306 3313 3319 3322 3333\n",
      " 3342 3365 3369 3390 3398 3403 3415 3425 3428 3429 3430 3436 3442 3453\n",
      " 3477 3482 3489 3496 3497 3508 3549 3567 3568 3573 3575 3594 3599 3605\n",
      " 3630 3647 3657 3658 3659 3660 3662 3663 3672 3688 3691 3694 3698 3712\n",
      " 3732 3745 3763 3774 3778 3791 3803 3813 3844 3856 3877 3882 3883 3906\n",
      " 3911 3931 3932 3935 3938 3941 3948 3958 3979 4011 4015 4018 4040 4055\n",
      " 4076 4077 4141 4146 4154 4175 4178 4180 4223 4242 4243 4255 4281 4288\n",
      " 4291 4311 4315 4323 4324 4328 4335 4338 4349 4355 4358 4367 4370 4382\n",
      " 4460 4473 4478 4484 4505 4516 4521 4522 4560 4567 4589 4597 4599 4603\n",
      " 4621 4622 4653 4654 4656 4672 4675 4676 4686 4691 4696 4698 4700 4717\n",
      " 4718 4729 4753 4762 4798 4810 4817 4821 4826 4828 4830 4859 4867 4868\n",
      " 4877 4888 4897 4901 4909 4925 4926 4927 4928 4940 4953 4965 5011 5027\n",
      " 5036 5059 5061 5075 5086 5091 5097 5130 5131 5136 5144 5155 5171 5175\n",
      " 5180 5199 5215 5261 5283 5286 5298 5302 5305 5313 5317 5319 5324 5325\n",
      " 5326 5331 5342 5349 5353 5354 5357 5369 5370 5372 5375 5382 5395 5398\n",
      " 5410 5411 5413 5430 5434 5451 5455 5459 5477 5517 5520 5523 5533 5540\n",
      " 5552 5555 5558 5561 5565 5596 5602 5605 5606 5621 5628 5629 5636 5647\n",
      " 5651 5661 5668 5679 5683 5690 5705 5721 5727 5735 5763 5771 5793 5795\n",
      " 5798 5802 5807 5817 5818 5823 5826 5841 5864 5884 5892 5919 5922 5925\n",
      " 5931 5936 5959 5965 5966 5970 5977 5991 5999 6006 6011 6025 6031 6061\n",
      " 6063 6068 6084 6120 6135 6168 6171 6177 6186 6189 6190 6194 6197 6198\n",
      " 6223 6234 6245 6271 6286 6287 6293 6310 6318 6324 6328 6330 6348 6357\n",
      " 6363 6374 6396 6403 6422 6439 6441 6454 6456 6457 6473 6474 6479 6488\n",
      " 6491 6498 6499 6529 6539 6545 6557 6571 6575 6577 6583 6597 6598 6612\n",
      " 6614 6633 6640 6653 6687 6689 6691 6707 6715 6717 6721 6726 6729 6735\n",
      " 6736 6737 6753 6763 6764 6794 6800 6813 6818 6842 6851 6918 6923 6932\n",
      " 6935 6943 6950 6951 6957 6968 6975 6981 6989 6995 6998 6999 7004 7007\n",
      " 7012 7016 7035 7040 7060 7068 7072 7077 7082 7094 7095 7105 7117 7123\n",
      " 7132 7137 7149 7152 7177 7183 7187 7194 7199 7210 7231 7242 7247 7253\n",
      " 7267 7273 7275 7278 7306 7311 7328 7332 7334 7341 7343 7349 7375 7379\n",
      " 7388 7390 7396 7408 7409 7410 7415 7420 7433 7439 7459 7464 7471 7472\n",
      " 7479 7480 7487 7510 7519 7523 7529 7530 7545 7563 7571 7574 7582 7585\n",
      " 7605 7626 7652 7662 7666 7680 7707 7714 7721 7726 7729 7731 7756 7771\n",
      " 7776 7789 7792 7795 7803 7809 7817 7824 7827 7829 7841 7852 7892 7894\n",
      " 7901 7904 7931 7938 7945 7950 7997 8003 8008 8035 8037 8068 8074 8081\n",
      " 8085 8098 8112 8113 8121 8126 8131 8144 8151 8168 8184 8187 8197 8198\n",
      " 8203 8221 8224 8227 8230 8231 8242 8248 8249 8255 8261 8262 8263 8265\n",
      " 8267 8272 8278 8297 8298 8302 8307 8309 8328 8336 8364 8370 8389 8394\n",
      " 8402 8408 8410 8422 8425 8439 8449 8459 8466 8467 8475 8478 8485 8507\n",
      " 8514 8520 8530 8538 8575 8590 8605 8613 8623 8665 8672 8679 8681 8693\n",
      " 8705 8719 8723 8735 8736 8743 8761 8771 8782 8793 8800 8812 8814 8829\n",
      " 8847 8857 8863 8864 8876 8883 8894 8909 8914]\n",
      "Y_pred: \n",
      " [1.50167958 1.70580025 1.98029753 ... 0.         9.64450956 9.16150331]\n",
      "train_idx: \n",
      " 8027 [   0    1    2 ... 8914 8916 8917]\n",
      "val_idx: \n",
      " 891 [  22   26   52   72   77  100  111  142  153  180  201  204  211  214\n",
      "  222  227  229  238  242  249  256  257  262  269  280  313  316  332\n",
      "  355  383  385  393  412  417  423  432  437  448  449  452  454  458\n",
      "  460  470  473  493  494  515  516  525  529  558  560  565  569  617\n",
      "  626  641  653  666  667  669  699  715  725  735  739  749  769  793\n",
      "  826  827  829  830  839  847  862  868  879  880  890  903  907  917\n",
      "  932  933  940  947  950  960  962  963  972  977  982  988  992  996\n",
      " 1005 1006 1050 1063 1091 1095 1096 1100 1122 1124 1129 1130 1135 1142\n",
      " 1147 1156 1160 1161 1178 1197 1211 1229 1235 1236 1239 1242 1257 1273\n",
      " 1282 1288 1306 1309 1344 1352 1356 1368 1383 1389 1397 1414 1415 1429\n",
      " 1438 1483 1496 1523 1524 1540 1545 1549 1550 1554 1563 1564 1574 1581\n",
      " 1589 1613 1615 1622 1643 1652 1658 1667 1672 1677 1683 1690 1701 1702\n",
      " 1709 1712 1714 1726 1737 1753 1765 1769 1770 1796 1800 1806 1807 1819\n",
      " 1822 1832 1843 1856 1880 1883 1885 1892 1908 1909 1914 1918 1954 1969\n",
      " 1975 1976 1979 1985 2002 2022 2041 2054 2058 2064 2075 2095 2116 2130\n",
      " 2132 2135 2142 2148 2149 2154 2162 2179 2180 2182 2186 2198 2211 2233\n",
      " 2237 2239 2240 2243 2258 2264 2272 2282 2283 2288 2303 2342 2351 2353\n",
      " 2366 2379 2381 2385 2389 2419 2444 2459 2463 2467 2469 2480 2489 2502\n",
      " 2511 2518 2540 2549 2566 2588 2597 2602 2604 2611 2626 2632 2643 2655\n",
      " 2661 2666 2667 2673 2674 2679 2727 2731 2732 2737 2770 2771 2774 2785\n",
      " 2793 2794 2809 2816 2820 2825 2826 2841 2848 2852 2877 2882 2884 2920\n",
      " 2930 2931 2946 2950 2958 2970 2977 2983 3009 3010 3011 3019 3023 3031\n",
      " 3033 3038 3039 3043 3063 3066 3068 3070 3075 3080 3084 3090 3102 3134\n",
      " 3145 3154 3171 3189 3192 3195 3213 3221 3226 3232 3239 3245 3264 3268\n",
      " 3282 3288 3290 3293 3299 3304 3309 3310 3326 3335 3341 3351 3361 3383\n",
      " 3395 3445 3461 3464 3475 3493 3515 3524 3527 3534 3541 3556 3616 3638\n",
      " 3640 3649 3681 3692 3693 3702 3703 3715 3724 3730 3740 3757 3760 3771\n",
      " 3779 3801 3818 3821 3823 3834 3841 3854 3862 3871 3884 3895 3896 3897\n",
      " 3909 3912 3923 3946 3956 3962 3965 3966 3976 3981 3986 3990 3993 4033\n",
      " 4039 4042 4046 4050 4057 4066 4067 4090 4098 4100 4108 4121 4126 4130\n",
      " 4145 4159 4162 4172 4173 4176 4191 4198 4205 4210 4219 4225 4226 4235\n",
      " 4250 4257 4268 4270 4278 4285 4300 4305 4321 4329 4363 4369 4372 4390\n",
      " 4402 4423 4426 4446 4489 4493 4499 4503 4518 4524 4526 4545 4550 4551\n",
      " 4555 4561 4565 4566 4607 4608 4617 4627 4637 4657 4665 4683 4689 4709\n",
      " 4731 4734 4739 4746 4763 4766 4780 4786 4792 4794 4796 4814 4854 4871\n",
      " 4905 4907 4930 4948 4957 4975 4978 4986 4997 4998 5010 5020 5037 5038\n",
      " 5042 5057 5060 5067 5068 5084 5106 5140 5145 5151 5178 5179 5183 5185\n",
      " 5187 5192 5194 5213 5254 5256 5258 5270 5272 5279 5288 5291 5323 5328\n",
      " 5329 5330 5335 5338 5344 5348 5355 5373 5381 5396 5404 5405 5407 5416\n",
      " 5420 5445 5453 5460 5462 5474 5475 5479 5486 5489 5499 5510 5518 5525\n",
      " 5535 5537 5543 5550 5588 5594 5618 5627 5657 5693 5719 5723 5730 5745\n",
      " 5762 5769 5781 5783 5785 5788 5800 5821 5824 5828 5843 5852 5861 5869\n",
      " 5875 5882 5894 5896 5908 5916 5920 5929 5949 5950 5956 5958 5969 5971\n",
      " 6016 6019 6020 6029 6045 6049 6055 6067 6071 6075 6097 6098 6101 6118\n",
      " 6141 6161 6162 6166 6175 6182 6206 6213 6225 6226 6252 6270 6272 6281\n",
      " 6285 6299 6308 6311 6326 6337 6353 6392 6400 6408 6415 6418 6435 6438\n",
      " 6446 6447 6451 6458 6468 6475 6476 6478 6496 6502 6505 6517 6520 6521\n",
      " 6522 6526 6548 6549 6567 6574 6576 6587 6590 6620 6629 6644 6666 6681\n",
      " 6692 6708 6722 6730 6734 6751 6772 6777 6778 6782 6789 6792 6793 6798\n",
      " 6806 6821 6822 6832 6833 6847 6849 6856 6863 6878 6880 6892 6897 6908\n",
      " 6924 6926 6934 6939 6952 6965 6969 6984 6992 7001 7005 7010 7024 7032\n",
      " 7039 7045 7049 7057 7059 7076 7087 7091 7096 7101 7109 7126 7131 7155\n",
      " 7159 7162 7182 7202 7204 7211 7212 7220 7227 7243 7258 7279 7295 7316\n",
      " 7323 7324 7329 7338 7340 7346 7356 7366 7368 7370 7371 7398 7403 7405\n",
      " 7407 7417 7424 7440 7448 7453 7455 7462 7477 7481 7513 7514 7520 7533\n",
      " 7538 7557 7580 7593 7635 7649 7660 7673 7675 7682 7684 7696 7709 7730\n",
      " 7747 7763 7764 7791 7797 7826 7845 7861 7864 7866 7878 7880 7900 7912\n",
      " 7917 7948 7959 7960 7962 7963 7965 7989 8004 8011 8013 8016 8070 8071\n",
      " 8075 8095 8103 8104 8117 8122 8123 8155 8164 8183 8210 8214 8216 8237\n",
      " 8253 8258 8311 8317 8327 8339 8344 8354 8363 8365 8378 8380 8393 8409\n",
      " 8442 8458 8462 8487 8492 8495 8503 8504 8515 8527 8543 8548 8550 8551\n",
      " 8553 8561 8571 8586 8588 8600 8602 8603 8630 8634 8645 8657 8689 8714\n",
      " 8717 8724 8733 8751 8763 8768 8776 8779 8784 8788 8790 8796 8803 8809\n",
      " 8811 8816 8818 8821 8832 8851 8872 8882 8915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: \n",
      " [ 1.50167958  1.70580025  1.98029753 ... 10.36393331  9.64450956\n",
      "  9.16150331]\n",
      "Score: 0.8660330781947748 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "res,stack_test_preds = cross_validate(models = [svr], X=stack_train, Y=train_Y['y'], test_X = stack_predict)\n",
    "\n",
    "stacking_res = res['Score Customize'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Stack with 4 models\n",
    "\n",
    "# stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "# stacker = Stacker(5, stacking_model, regrs_light)\n",
    "# pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack with 12 models\n",
    "\n",
    "# stacking_model = SVR(C=100, gamma=0.01, epsilon=0.01)\n",
    "# stacker = Stacker(5, stacking_model, regrs)\n",
    "# pred_stack, S_train_data, S_predict_data = stacker.fit_predict(train_X, train_Y, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_result['score'] = pred_stack\n",
    "\n",
    "# index = df_result[df_result['ID'].isin(special_missing_ID)].index\n",
    "# df_result.loc[index, 'score'] = 0.379993053\n",
    "\n",
    "# df_result.to_csv('../result/081703_08785.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[4,'Model'] = '5.WYH'\n",
    "\n",
    "table.loc[4,'Score'] = 1/(1+np.sqrt(mean_squared_error(train_Y['y'],np.mean(val_preds,axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[5,'Model'] = '6.Stacking'\n",
    "\n",
    "table.loc[5,'Score'] = stacking_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.LGBMRegressor</td>\n",
       "      <td>0.856319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.XGBRegressor</td>\n",
       "      <td>0.859571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.GradientBoostingRegressor</td>\n",
       "      <td>0.858816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.RandomForestRegressor</td>\n",
       "      <td>0.851396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.WYH</td>\n",
       "      <td>0.862889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.Stacking</td>\n",
       "      <td>0.866033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model     Score\n",
       "0              1.LGBMRegressor  0.856319\n",
       "1               2.XGBRegressor  0.859571\n",
       "2  3.GradientBoostingRegressor  0.858816\n",
       "3      4.RandomForestRegressor  0.851396\n",
       "4                        5.WYH  0.862889\n",
       "5                   6.Stacking  0.866033"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f53edf9e48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_compare = stack_predict.copy()\n",
    "pred_compare['Stack'] = stack_test_preds\n",
    "pred_compare['Stack_WYH'] = pred_compare['Stack'] - pred_compare['WYH']\n",
    "pred_compare['Stack_WYH'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['y_pred']=pred_compare['Stack'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_A</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.346507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.346338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4867</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.339321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.559406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8026</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8832</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9255</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9660</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10272</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10453</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10653</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.649965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.663342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.654635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.663229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15042</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15678</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.653181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15882</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.668977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16459</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16876</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17061</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       I_A    y_pred\n",
       "0      0.0  0.547199\n",
       "855    0.0  0.324794\n",
       "1543   0.0  0.346507\n",
       "1714   0.0  0.347556\n",
       "2583   0.0  0.335367\n",
       "2930   0.0  0.337186\n",
       "4123   0.0  0.344451\n",
       "4300   0.0  0.346338\n",
       "4462   0.0  0.342035\n",
       "4632   0.0  0.417639\n",
       "4633   0.0  0.417488\n",
       "4674   0.0  0.340842\n",
       "4675   0.0  0.341515\n",
       "4867   0.0  0.339321\n",
       "4870   0.0  0.342316\n",
       "5072   0.0  0.338869\n",
       "5463   0.0  0.338207\n",
       "5665   0.0  0.352843\n",
       "6071   0.0  0.433857\n",
       "6074   0.0  0.479625\n",
       "7684   0.0  0.357058\n",
       "7783   0.0  0.559406\n",
       "7848   0.0  0.362492\n",
       "8026   0.0  0.492144\n",
       "8624   0.0  0.548159\n",
       "8832   0.0  0.616582\n",
       "9041   0.0  0.425709\n",
       "9255   0.0  0.582689\n",
       "9452   0.0  0.535812\n",
       "9660   0.0  0.468153\n",
       "9871   0.0  0.635724\n",
       "10272  0.0  0.450514\n",
       "10453  0.0  0.428606\n",
       "10653  0.0  0.569232\n",
       "11497  0.0  0.649965\n",
       "13364  0.0  0.663342\n",
       "13566  0.0  0.654635\n",
       "13777  0.0  0.664648\n",
       "13991  0.0  0.495675\n",
       "14833  0.0  0.663229\n",
       "15042  0.0  0.538505\n",
       "15678  0.0  0.653181\n",
       "15882  0.0  0.668977\n",
       "16459  0.0  0.574492\n",
       "16876  0.0  0.487870\n",
       "17061  0.0  0.429386"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[test_X['I_A']==0.00][['I_A','y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.547199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1.304727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>2.128413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>3.415849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>3.666793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21</td>\n",
       "      <td>4.217450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>4.350903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>4.787781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>4.971161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28</td>\n",
       "      <td>5.287567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29</td>\n",
       "      <td>5.522665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31</td>\n",
       "      <td>5.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>5.890293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33</td>\n",
       "      <td>6.113539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>36</td>\n",
       "      <td>6.430901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37</td>\n",
       "      <td>6.657782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>38</td>\n",
       "      <td>6.797333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40</td>\n",
       "      <td>7.062540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>41</td>\n",
       "      <td>7.112132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>46</td>\n",
       "      <td>7.525681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>48</td>\n",
       "      <td>7.749364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50</td>\n",
       "      <td>7.957594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>51</td>\n",
       "      <td>8.061181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>53</td>\n",
       "      <td>8.197114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>54</td>\n",
       "      <td>8.266435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>55</td>\n",
       "      <td>8.340520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>56</td>\n",
       "      <td>8.399741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>59</td>\n",
       "      <td>8.739110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>62</td>\n",
       "      <td>8.891712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>69</td>\n",
       "      <td>9.268059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17350</th>\n",
       "      <td>17818</td>\n",
       "      <td>10.033677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17352</th>\n",
       "      <td>17820</td>\n",
       "      <td>9.910595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17354</th>\n",
       "      <td>17822</td>\n",
       "      <td>10.172295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17356</th>\n",
       "      <td>17824</td>\n",
       "      <td>10.314861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17357</th>\n",
       "      <td>17825</td>\n",
       "      <td>10.215808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17364</th>\n",
       "      <td>17832</td>\n",
       "      <td>10.273371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17365</th>\n",
       "      <td>17833</td>\n",
       "      <td>10.358510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17367</th>\n",
       "      <td>17835</td>\n",
       "      <td>10.278656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>17838</td>\n",
       "      <td>10.393794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17371</th>\n",
       "      <td>17839</td>\n",
       "      <td>10.435576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17372</th>\n",
       "      <td>17840</td>\n",
       "      <td>10.534883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17376</th>\n",
       "      <td>17844</td>\n",
       "      <td>10.350460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>17845</td>\n",
       "      <td>10.444779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17382</th>\n",
       "      <td>17850</td>\n",
       "      <td>8.275655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17384</th>\n",
       "      <td>17852</td>\n",
       "      <td>7.784492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17385</th>\n",
       "      <td>17853</td>\n",
       "      <td>8.950961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17387</th>\n",
       "      <td>17855</td>\n",
       "      <td>8.506043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17388</th>\n",
       "      <td>17856</td>\n",
       "      <td>7.553587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17389</th>\n",
       "      <td>17857</td>\n",
       "      <td>8.159491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17391</th>\n",
       "      <td>17859</td>\n",
       "      <td>9.778748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17393</th>\n",
       "      <td>17861</td>\n",
       "      <td>10.271628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395</th>\n",
       "      <td>17863</td>\n",
       "      <td>10.050328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17396</th>\n",
       "      <td>17864</td>\n",
       "      <td>8.858809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17397</th>\n",
       "      <td>17865</td>\n",
       "      <td>9.896217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17400</th>\n",
       "      <td>17868</td>\n",
       "      <td>8.356943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>17869</td>\n",
       "      <td>8.254763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17402</th>\n",
       "      <td>17870</td>\n",
       "      <td>10.103292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17403</th>\n",
       "      <td>17871</td>\n",
       "      <td>9.941879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17404</th>\n",
       "      <td>17872</td>\n",
       "      <td>9.869179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17407</th>\n",
       "      <td>17875</td>\n",
       "      <td>9.044364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID     y_pred\n",
       "0          1   0.547199\n",
       "1          9   1.304727\n",
       "5         13   2.128413\n",
       "9         17   3.415849\n",
       "10        18   3.666793\n",
       "13        21   4.217450\n",
       "15        23   4.350903\n",
       "17        25   4.787781\n",
       "18        26   4.971161\n",
       "20        28   5.287567\n",
       "21        29   5.522665\n",
       "23        31   5.731200\n",
       "24        32   5.890293\n",
       "25        33   6.113539\n",
       "28        36   6.430901\n",
       "29        37   6.657782\n",
       "30        38   6.797333\n",
       "32        40   7.062540\n",
       "33        41   7.112132\n",
       "38        46   7.525681\n",
       "40        48   7.749364\n",
       "42        50   7.957594\n",
       "43        51   8.061181\n",
       "45        53   8.197114\n",
       "46        54   8.266435\n",
       "47        55   8.340520\n",
       "48        56   8.399741\n",
       "51        59   8.739110\n",
       "54        62   8.891712\n",
       "61        69   9.268059\n",
       "...      ...        ...\n",
       "17350  17818  10.033677\n",
       "17352  17820   9.910595\n",
       "17354  17822  10.172295\n",
       "17356  17824  10.314861\n",
       "17357  17825  10.215808\n",
       "17364  17832  10.273371\n",
       "17365  17833  10.358510\n",
       "17367  17835  10.278656\n",
       "17370  17838  10.393794\n",
       "17371  17839  10.435576\n",
       "17372  17840  10.534883\n",
       "17376  17844  10.350460\n",
       "17377  17845  10.444779\n",
       "17382  17850   8.275655\n",
       "17384  17852   7.784492\n",
       "17385  17853   8.950961\n",
       "17387  17855   8.506043\n",
       "17388  17856   7.553587\n",
       "17389  17857   8.159491\n",
       "17391  17859   9.778748\n",
       "17393  17861  10.271628\n",
       "17395  17863  10.050328\n",
       "17396  17864   8.858809\n",
       "17397  17865   9.896217\n",
       "17400  17868   8.356943\n",
       "17401  17869   8.254763\n",
       "17402  17870  10.103292\n",
       "17403  17871   9.941879\n",
       "17404  17872   9.869179\n",
       "17407  17875   9.044364\n",
       "\n",
       "[8409 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = test_X[['ID','y_pred']]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ans[ans['ID'].isin(special_missing_ID)].index\n",
    "ans.loc[index, 'y_pred'] = 0.379993053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8409.000000\n",
       "mean        0.000324\n",
       "std         0.032488\n",
       "min        -0.479929\n",
       "25%        -0.010521\n",
       "50%        -0.000009\n",
       "75%         0.010723\n",
       "max         0.775440\n",
       "Name: final_WYH, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_compare['final'] = ans['y_pred'].tolist()\n",
    "pred_compare['final_WYH'] = pred_compare['final']-pred_compare['WYH']\n",
    "pred_compare['final_WYH'].hist(bins=40)\n",
    "pred_compare['final_WYH'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[['ID','y_pred']].to_csv('../result/082001_08660.csv',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
